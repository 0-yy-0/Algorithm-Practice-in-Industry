# ECIR2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Large Language Models are Zero-Shot Rankers for Recommender Systems](https://doi.org/10.1007/978-3-031-56060-6_24)|Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, Wayne Xin Zhao||Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank.|最近，大型语言模型(LLM)(例如 GPT-4)展示了令人印象深刻的通用任务解决能力，包括接近推荐任务的潜力。沿着这条研究路线，这项工作旨在调查作为推荐系统排名模型的 LLM 的能力。为了进行实证研究，我们首先将推荐问题形式化为一个条件排序任务，将序贯交互历史作为条件，并将候选生成模型检索到的项目作为候选项。我们采用了一种特定的提示方法来解决 LLM 的排序问题: 我们仔细设计了提示模板，包括顺序交互历史，候选项，和排序指令。我们对推荐系统中两个广泛使用的数据集进行了广泛的实验，得出了在推荐系统中使用 LLM 的几个关键发现。我们证明了 LLM 具有良好的零拍排序能力，甚至比传统的推荐模型更有竞争力或更好的候选人由多个候选生成器检索。我们还证明 LLM 很难感知历史交互的次序，并且可能受到位置偏差等偏见的影响，而这些问题可以通过特别设计的激励和自举策略得到缓解。复制这项工作的代码可在 https://github.com/rucaibox/llmrank 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+are+Zero-Shot+Rankers+for+Recommender+Systems)|3|
|[Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents](https://doi.org/10.1007/978-3-031-56060-6_15)|Nishchal Prasad, Mohand Boughanem, Taoufiq Dkaki||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Large+Language+Models+and+Hierarchical+Frameworks+for+Classification+of+Large+Unstructured+Legal+Documents)|1|
|[Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification - Extended Abstract](https://doi.org/10.1007/978-3-031-56072-9_1)|Janek Bevendorff, Xavier Bonet Casals, Berta Chulvi, Daryna Dementieva, Ashaf Elnagar, Dayne Freitag, Maik Fröbe, Damir Korencic, Maximilian Mayerl, Animesh Mukherjee, Alexander Panchenko, Martin Potthast, Francisco Rangel, Paolo Rosso, Alisa Smirnova, Efstathios Stamatatos, Benno Stein, Mariona Taulé, Dmitry Ustalov, Matti Wiegmann, Eva Zangerle||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overview+of+PAN+2024:+Multi-author+Writing+Style+Analysis,+Multilingual+Text+Detoxification,+Oppositional+Thinking+Analysis,+and+Generative+AI+Authorship+Verification+-+Extended+Abstract)|1|
|[Incorporating Query Recommendation for Improving In-Car Conversational Search](https://doi.org/10.1007/978-3-031-56069-9_36)|Md. Rashad Al Hasan Rony, Soumya Ranjan Sahoo, Abbas Goher Khan, Ken E. Friedl, Viju Sudhi, Christian Süß||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incorporating+Query+Recommendation+for+Improving+In-Car+Conversational+Search)|0|
|[ChatGPT Goes Shopping: LLMs Can Predict Relevance in eCommerce Search](https://doi.org/10.1007/978-3-031-56066-8_1)|Beatriz Soviero, Daniel Kuhn, Alexandre Salle, Viviane Pereira Moreira||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ChatGPT+Goes+Shopping:+LLMs+Can+Predict+Relevance+in+eCommerce+Search)|0|
|[Lottery4CVR: Neuron-Connection Level Sharing for Multi-task Learning in Video Conversion Rate Prediction](https://doi.org/10.1007/978-3-031-56069-9_31)|Xuanji Xiao, Jimmy Chen, Yuzhen Liu, Xing Yao, Pei Liu, Chaosheng Fan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lottery4CVR:+Neuron-Connection+Level+Sharing+for+Multi-task+Learning+in+Video+Conversion+Rate+Prediction)|0|
|[Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search](https://doi.org/10.1007/978-3-031-56060-6_3)|Kathryn E. Kirchoff, James Wellnitz, Joshua E. Hochuli, Travis Maxfield, Konstantin I. Popov, Shawn M. Gomez, Alexander Tropsha|Department of Pharmacology, UNC Chapel Hill.; Department of Computer Science, UNC Chapel Hill.; Eshelman School of Pharmacy, UNC Chapel Hill.|Nearest neighbor-based similarity searching is a common task in chemistry, with notable use cases in drug discovery. Yet, some of the most commonly used approaches for this task still leverage a brute-force approach. In practice this can be computationally costly and overly time-consuming, due in part to the sheer size of modern chemical databases. Previous computational advancements for this task have generally relied on improvements to hardware or dataset-specific tricks that lack generalizability. Approaches that leverage lower-complexity searching algorithms remain relatively underexplored. However, many of these algorithms are approximate solutions and/or struggle with typical high-dimensional chemical embeddings. Here we evaluate whether a combination of low-dimensional chemical embeddings and a k-d tree data structure can achieve fast nearest neighbor queries while maintaining performance on standard chemical similarity search benchmarks. We examine different dimensionality reductions of standard chemical embeddings as well as a learned, structurally-aware embedding-SmallSA-for this task. With this framework, searches on over one billion chemicals execute in less than a second on a single CPU core, five orders of magnitude faster than the brute-force approach. We also demonstrate that SmallSA achieves competitive performance on chemical similarity benchmarks.|基于最近邻的相似性搜索是化学中的一个常见任务，在药物发现中有着显著的应用案例。然而，这项任务中一些最常用的方法仍然使用蛮力方法。在实践中，由于现代化学品数据库的庞大规模，这可能会造成计算成本高昂和过度耗时。此任务之前的计算改进通常依赖于对缺乏普遍性的硬件或数据集特定技巧的改进。利用低复杂度搜索算法的方法仍然相对缺乏探索。然而，许多这些算法是近似解决方案和/或与典型的高维化学嵌入斗争。在这里，我们评估是否结合低维化学嵌入和 k-d 树数据结构可以实现快速最近邻查询，同时保持标准化学相似性搜索基准的性能。我们考察了不同维度的标准化学嵌入降低以及一个学习，结构意识的嵌入-SmallSA-为这项任务。有了这个框架，超过十亿种化学物质的搜索在不到一秒钟的时间内在一个 CPU 核心上执行，比蛮力搜索数量级快5倍。我们亦证明 SmallSA 在化学相似性基准方面取得具竞争力的表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Utilizing+Low-Dimensional+Molecular+Embeddings+for+Rapid+Chemical+Similarity+Search)|0|
|[Evaluating the Impact of Content Deletion on Tabular Data Similarity and Retrieval Using Contextual Word Embeddings](https://doi.org/10.1007/978-3-031-56060-6_28)|Alberto Berenguer, David Tomás, JoseNorberto Mazón||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+the+Impact+of+Content+Deletion+on+Tabular+Data+Similarity+and+Retrieval+Using+Contextual+Word+Embeddings)|0|
|[RIGHT: Retrieval-Augmented Generation for Mainstream Hashtag Recommendation](https://doi.org/10.1007/978-3-031-56027-9_3)|RunZe Fan, Yixing Fan, Jiangui Chen, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng||Automatic mainstream hashtag recommendation aims to accurately provide users
with concise and popular topical hashtags before publication. Generally,
mainstream hashtag recommendation faces challenges in the comprehensive
difficulty of newly posted tweets in response to new topics, and the accurate
identification of mainstream hashtags beyond semantic correctness. However,
previous retrieval-based methods based on a fixed predefined mainstream hashtag
list excel in producing mainstream hashtags, but fail to understand the
constant flow of up-to-date information. Conversely, generation-based methods
demonstrate a superior ability to comprehend newly posted tweets, but their
capacity is constrained to identifying mainstream hashtags without additional
features. Inspired by the recent success of the retrieval-augmented technique,
in this work, we attempt to adopt this framework to combine the advantages of
both approaches. Meantime, with the help of the generator component, we could
rethink how to further improve the quality of the retriever component at a low
cost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag
Recommender (RIGHT), which consists of three components: 1) a retriever seeks
relevant hashtags from the entire tweet-hashtags set; 2) a selector enhances
mainstream identification by introducing global signals; and 3) a generator
incorporates input tweets and selected hashtags to directly generate the
desired hashtags. The experimental results show that our method achieves
significant improvements over state-of-the-art baselines. Moreover, RIGHT can
be easily integrated into large language models, improving the performance of
ChatGPT by more than 10%.|自动主流话题标签推荐的目的是准确地为用户提供简洁和流行的话题标签发布前。一般来说，主流话题标签推荐面临的挑战包括: 新发布的推文在回应新话题方面的综合难度，以及在语义正确性之外对主流话题标签的准确识别。然而，以往基于固定预定义主流标签列表的检索方法在生成主流标签方面表现出色，但不能理解不断更新的信息流。相反，基于生成的方法展示了理解新发布的 tweet 的优越能力，但它们的能力仅限于识别主流标签，而没有其他特性。受近年来检索增强技术的成功启发，本文尝试采用这一框架将两种方法的优点结合起来。同时，借助于发生器组件，我们可以重新思考如何以较低的成本进一步提高检索器组件的质量。因此，我们提出了 RetrIeval 增强的生成主流 HashTag 推荐器(RIGHT) ，它由三个组成部分组成: 1)检索器从整个 tweet-HashTag 集中寻找相关的 HashTag; 2)选择器通过引入全局信号增强主流识别; 3)生成器结合输入 tweet 和选定的 HashTag 直接生成所需的 HashTag。实验结果表明，我们的方法取得了显着的改进，在最先进的基线。此外，可以很容易地将权限集成到大型语言模型中，使 ChatGPT 的性能提高10% 以上。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RIGHT:+Retrieval-Augmented+Generation+for+Mainstream+Hashtag+Recommendation)|0|
|[Exploring the Nexus Between Retrievability and Query Generation Strategies](https://doi.org/10.1007/978-3-031-56066-8_16)|Aman Sinha, Priyanshu Raj Mall, Dwaipayan Roy||Quantifying bias in retrieval functions through document retrievability
scores is vital for assessing recall-oriented retrieval systems. However, many
studies investigating retrieval model bias lack validation of their query
generation methods as accurate representations of retrievability for real users
and their queries. This limitation results from the absence of established
criteria for query generation in retrievability assessments. Typically,
researchers resort to using frequent collocations from document corpora when no
query log is available. In this study, we address the issue of reproducibility
and seek to validate query generation methods by comparing retrievability
scores generated from artificially generated queries to those derived from
query logs. Our findings demonstrate a minimal or negligible correlation
between retrievability scores from artificial queries and those from query
logs. This suggests that artificially generated queries may not accurately
reflect retrievability scores as derived from query logs. We further explore
alternative query generation techniques, uncovering a variation that exhibits
the highest correlation. This alternative approach holds promise for improving
reproducibility when query logs are unavailable.|通过文档可检索性评分量化检索功能中的偏差对于评估面向回忆的检索系统至关重要。然而，许多研究检索模型偏倚的研究缺乏验证其查询生成方法作为准确表示的可检索性的真实用户和他们的查询。这种局限性是由于在可检索性评估中缺乏确定的查询生成标准造成的。通常，当没有查询日志可用时，研究人员会使用文档语料库中的频繁搭配。在这项研究中，我们解决了重复性的问题，并寻求验证查询生成方法，通过比较从人工生成的查询和从查询日志得到的查询可检索性得分。我们的研究结果表明，人工查询和查询日志的可检索性得分之间的相关性很小，甚至可以忽略不计。这表明人工生成的查询可能不能准确地反映从查询日志中获得的可检索性得分。我们进一步探索替代的查询生成技术，发现具有最高相关性的变体。这种替代方法有望在查询日志不可用时提高可重复性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Nexus+Between+Retrievability+and+Query+Generation+Strategies)|0|
|[GLAD: Graph-Based Long-Term Attentive Dynamic Memory for Sequential Recommendation](https://doi.org/10.1007/978-3-031-56063-7_5)|Deepanshu Pandey, Arindam Sarkar, Prakash Mandayam Comar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GLAD:+Graph-Based+Long-Term+Attentive+Dynamic+Memory+for+Sequential+Recommendation)|0|
|[BertPE: A BERT-Based Pre-retrieval Estimator for Query Performance Prediction](https://doi.org/10.1007/978-3-031-56063-7_27)|Maryam Khodabakhsh, Fattane Zarrinkalam, Negar Arabzadeh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BertPE:+A+BERT-Based+Pre-retrieval+Estimator+for+Query+Performance+Prediction)|0|
|[Estimating the Usefulness of Clarifying Questions and Answers for Conversational Search](https://doi.org/10.1007/978-3-031-56063-7_30)|Ivan Sekulic, Weronika Lajewska, Krisztian Balog, Fabio Crestani||While the body of research directed towards constructing and generating
clarifying questions in mixed-initiative conversational search systems is vast,
research aimed at processing and comprehending users' answers to such questions
is scarce. To this end, we present a simple yet effective method for processing
answers to clarifying questions, moving away from previous work that simply
appends answers to the original query and thus potentially degrades retrieval
performance. Specifically, we propose a classifier for assessing usefulness of
the prompted clarifying question and an answer given by the user. Useful
questions or answers are further appended to the conversation history and
passed to a transformer-based query rewriting module. Results demonstrate
significant improvements over strong non-mixed-initiative baselines.
Furthermore, the proposed approach mitigates the performance drops when non
useful questions and answers are utilized.|尽管在混合主动会话搜索系统中，针对构建和生成澄清问题的研究机构非常庞大，但是针对处理和理解用户对这些问题的回答的研究却很少。为此，我们提出了一种简单而有效的方法，用于处理澄清问题的答案，避免了以前的工作，即只是将答案附加到原始查询，从而可能降低检索性能。具体来说，我们提出了一个分类器，用于评估提示的澄清问题和用户给出的答案的有用性。有用的问题或答案将进一步附加到会话历史中，并传递给基于转换器的查询重写模块。结果显示，与强大的非混合倡议基线相比，有了显著改善。此外，当使用非有用的问题和答案时，提出的方法可以减少性能下降。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Estimating+the+Usefulness+of+Clarifying+Questions+and+Answers+for+Conversational+Search)|0|
|[Measuring Bias in Search Results Through Retrieval List Comparison](https://doi.org/10.1007/978-3-031-56069-9_2)|Linda Ratz, Markus Schedl, Simone Kopeinik, Navid Rekabsaz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Bias+in+Search+Results+Through+Retrieval+List+Comparison)|0|
|[Cascading Ranking Pipelines for Sensitivity-Aware Search](https://doi.org/10.1007/978-3-031-56069-9_41)|Jack McKechnie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cascading+Ranking+Pipelines+for+Sensitivity-Aware+Search)|0|
|[Advancing Multimedia Retrieval in Medical, Social Media and Content Recommendation Applications with ImageCLEF 2024](https://doi.org/10.1007/978-3-031-56072-9_6)|Bogdan Ionescu, Henning Müller, AnaMaria Claudia Dragulinescu, Ahmad IdrissiYaghir, Ahmedkhan Radzhabov, Alba Garcia Seco de Herrera, Alexandra Andrei, Alexandru Stan, Andrea M. Storås, Asma Ben Abacha, Benjamin Lecouteux, Benno Stein, Cécile Macaire, Christoph M. Friedrich, Cynthia S. Schmidt, Didier Schwab, Emmanuelle EsperançaRodier, George Ioannidis, Griffin Adams, Henning Schäfer, Hugo Manguinhas, Ioan Coman, Johanna Schöler, Johannes Kiesel, Johannes Rückert, Louise Bloch, Martin Potthast, Maximilian Heinrich, Meliha Yetisgen, Michael A. Riegler, Neal Snider, Pål Halvorsen, Raphael Brüngel, Steven Alexander Hicks, Vajira Thambawita, Vassili Kovalev, Yuri Prokopchuk, Wenwai Yim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Multimedia+Retrieval+in+Medical,+Social+Media+and+Content+Recommendation+Applications+with+ImageCLEF+2024)|0|
|[Ranking Heterogeneous Search Result Pages Using the Interactive Probability Ranking Principle](https://doi.org/10.1007/978-3-031-56060-6_7)|Kanaad Pathak, Leif Azzopardi, Martin Halvey||The Probability Ranking Principle (PRP) ranks search results based on their
expected utility derived solely from document contents, often overlooking the
nuances of presentation and user interaction. However, with the evolution of
Search Engine Result Pages (SERPs), now comprising a variety of result cards,
the manner in which these results are presented is pivotal in influencing user
engagement and satisfaction. This shift prompts the question: How does the PRP
and its user-centric counterpart, the Interactive Probability Ranking Principle
(iPRP), compare in the context of these heterogeneous SERPs? Our study draws a
comparison between the PRP and the iPRP, revealing significant differences in
their output. The iPRP, accounting for item-specific costs and interaction
probabilities to determine the “Expected Perceived Utility" (EPU), yields
different result orderings compared to the PRP. We evaluate the effect of the
EPU on the ordering of results by observing changes in the ranking within a
heterogeneous SERP compared to the traditional “ten blue links”. We find that
changing the presentation affects the ranking of items according to the (iPRP)
by up to 48% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the
TREC WaPo Collection. This work suggests that the iPRP should be employed when
ranking heterogeneous SERPs to provide a user-centric ranking that adapts the
ordering based on the presentation and user engagement.|概率排序原则(PRP)根据搜索结果的预期效用来排序，这些效用完全来自文档内容，往往忽略了表示和用户交互的细微差别。然而，随着搜索引擎结果页面(SERP)的发展，现在包含了各种各样的结果卡，这些结果的呈现方式对于影响用户的参与度和满意度是至关重要的。这种转变提出了一个问题: PRP 和它的以用户为中心的对应物，交互式概率排序原则(iPRP) ，如何在这些异构 SERP 的上下文中进行比较？我们的研究对 PRP 和 iPRP 进行了比较，发现它们的输出存在显著差异。IPRP 考虑了项目特定成本和交互概率，以确定“预期感知效用”(EPU) ，与 PRP 相比产生了不同的结果排序。我们通过观察一个异构 SERP 中的排序变化来评估 EPU 对结果排序的影响，与传统的“十个蓝色链接”相比。我们发现，在 TREC WaPo 集合的特别搜索任务中，根据(iPRP)改变表示影响项目的排名高达48% (相对于 DCG，TBG 和 RBO)。这项工作表明，iPRP 应该被用来排名异构的 SERP 时，提供一个以用户为中心的排名，适应排序的基础上的表示和用户参与。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+Heterogeneous+Search+Result+Pages+Using+the+Interactive+Probability+Ranking+Principle)|0|
|[Query Exposure Prediction for Groups of Documents in Rankings](https://doi.org/10.1007/978-3-031-56060-6_10)|Thomas Jänich, Graham McDonald, Iadh Ounis||The main objective of an Information Retrieval system is to provide a user
with the most relevant documents to the user's query. To do this, modern IR
systems typically deploy a re-ranking pipeline in which a set of documents is
retrieved by a lightweight first-stage retrieval process and then re-ranked by
a more effective but expensive model. However, the success of a re-ranking
pipeline is heavily dependent on the performance of the first stage retrieval,
since new documents are not usually identified during the re-ranking stage.
Moreover, this can impact the amount of exposure that a particular group of
documents, such as documents from a particular demographic group, can receive
in the final ranking. For example, the fair allocation of exposure becomes more
challenging or impossible if the first stage retrieval returns too few
documents from certain groups, since the number of group documents in the
ranking affects the exposure more than the documents' positions. With this in
mind, it is beneficial to predict the amount of exposure that a group of
documents is likely to receive in the results of the first stage retrieval
process, in order to ensure that there are a sufficient number of documents
included from each of the groups. In this paper, we introduce the novel task of
query exposure prediction (QEP). Specifically, we propose the first approach
for predicting the distribution of exposure that groups of documents will
receive for a given query. Our new approach, called GEP, uses lexical
information from individual groups of documents to estimate the exposure the
groups will receive in a ranking. Our experiments on the TREC 2021 and 2022
Fair Ranking Track test collections show that our proposed GEP approach results
in exposure predictions that are up to 40 
of adapted existing query performance prediction and resource allocation
approaches.|信息检索系统的主要目的是向用户提供与其查询最相关的文件。为了做到这一点，现代 IR 系统通常部署一个重新排序的管道，其中一组文档通过轻量级的第一阶段检索过程检索，然后通过一个更有效但昂贵的模型重新排序。然而，重新排序管道的成功与否在很大程度上取决于第一阶段检索的性能，因为在重新排序阶段通常不能确定新文档。此外，这可能会影响特定文档组(如来自特定人口组的文档)在最终排名中可以接受的曝光量。例如，如果第一阶段检索从某些群组返回的文档太少，则公平分配曝光变得更具挑战性或不可能，因为排名中群组文档的数量比文档的位置更能影响曝光。考虑到这一点，有益的做法是预测一组文件在第一阶段检索过程的结果中可能接触的数量，以确保每组文件都有足够的数量。本文介绍了一种新的查询暴露预测(QEP)任务。具体来说，我们提出了第一种方法，用于预测给定查询将接收到的文档组的曝光分布。我们的新方法被称为 GEP，它使用来自单个文档组的词汇信息来估计这些组在一个排名中将接收到的信息。我们在 TREC 2021和2022公平排名跟踪测试集合上的实验表明，我们提出的 GEP 方法导致暴露预测，这是多达40种适应现有查询性能预测和资源分配方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Exposure+Prediction+for+Groups+of+Documents+in+Rankings)|0|
|[Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations](https://doi.org/10.1007/978-3-031-56060-6_14)|Filippo Betello, Federico Siciliano, Pushkar Mishra, Fabrizio Silvestri||Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs.|随着时间的推移，序贯推荐系统(SRS)已被广泛用于模拟用户行为，但是它们在训练数据受到干扰时的鲁棒性是一个关键问题。在本文中，我们进行了一个实证研究，以探讨删除项目在不同位置的时间顺序的影响。我们在多个数据集上评估两种不同的 SRS 模型，使用归一化贴现累积增益(NDCG)和秩敏感性列表度量衡量它们的性能。结果表明: 去除序列末端的项目对性能有显著影响，NDCG 下降幅度达60% ，而去除序列开头或中间的项目对性能无显著影响。这些发现强调了考虑受干扰项目在训练数据中的位置的重要性，并将为设计更强健的战略参考系提供信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+the+Robustness+of+Sequential+Recommender+Systems+Against+Training+Data+Perturbations)|0|
|[Conversational Search with Tail Entities](https://doi.org/10.1007/978-3-031-56060-6_20)|Hai Dang Tran, Andrew Yates, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Search+with+Tail+Entities)|0|
|[Event-Specific Document Ranking Through Multi-stage Query Expansion Using an Event Knowledge Graph](https://doi.org/10.1007/978-3-031-56060-6_22)|Sara Abdollahi, Tin Kuculo, Simon Gottschalk||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event-Specific+Document+Ranking+Through+Multi-stage+Query+Expansion+Using+an+Event+Knowledge+Graph)|0|
|[Simulating Follow-Up Questions in Conversational Search](https://doi.org/10.1007/978-3-031-56060-6_25)|Johannes Kiesel, Marcel Gohsen, Nailia Mirzakhmedova, Matthias Hagen, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simulating+Follow-Up+Questions+in+Conversational+Search)|0|
|[MOReGIn: Multi-Objective Recommendation at the Global and Individual Levels](https://doi.org/10.1007/978-3-031-56027-9_2)|Elizabeth Gómez, David Contreras, Ludovico Boratto, Maria Salamó||Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to guarantee multiple (often conflicting) goals. Besides accuracy, a MORS can operate at the global level, where additional beyond-accuracy goals are met for the system as a whole, or at the individual level, meaning that the recommendations are tailored to the needs of each user. The state-of-the-art MORSs either operate at the global or individual level, without assuming the co-existence of the two perspectives. In this study, we show that when global and individual objectives co-exist, MORSs are not able to meet both types of goals. To overcome this issue, we present an approach that regulates the recommendation lists so as to guarantee both global and individual perspectives, while preserving its effectiveness. Specifically, as individual perspective, we tackle genre calibration and, as global perspective, provider fairness. We validate our approach on two real-world datasets, publicly released with this paper.|多目标推荐系统(MORS)作为一种范式出现，以保证多个(经常相互冲突)的目标。除了准确性之外，一个 MORS 还可以在全球一级运作，在这一级可以为整个系统或在个人一级实现额外的超准确性目标，这意味着建议是根据每个用户的需要量身定制的。最先进的监测系统既可以在全球一级运作，也可以在个人一级运作，而不必假设这两种观点并存。在这项研究中，我们发现当全球目标和个人目标共存时，MORS 不能同时满足这两种目标。为了解决这一问题，我们提出了一种管理建议清单的办法，以保证全球和个人的观点，同时保持其有效性。具体来说，作为个人的角度，我们处理体裁校准和作为全球的角度，提供者的公平性。我们验证了我们的方法在两个真实世界的数据集，公开发布与本文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MOReGIn:+Multi-Objective+Recommendation+at+the+Global+and+Individual+Levels)|0|
|[VEMO: A Versatile Elastic Multi-modal Model for Search-Oriented Multi-task Learning](https://doi.org/10.1007/978-3-031-56027-9_4)|Nanyi Fei, Hao Jiang, Haoyu Lu, Jinqiang Long, Yanqi Dai, Tuo Fan, Zhao Cao, Zhiwu Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VEMO:+A+Versatile+Elastic+Multi-modal+Model+for+Search-Oriented+Multi-task+Learning)|0|
|[Lightweight Modality Adaptation to Sequential Recommendation via Correlation Supervision](https://doi.org/10.1007/978-3-031-56027-9_8)|Hengchang Hu, Qijiong Liu, Chuang Li, MinYen Kan||In Sequential Recommenders (SR), encoding and utilizing modalities in an
end-to-end manner is costly in terms of modality encoder sizes. Two-stage
approaches can mitigate such concerns, but they suffer from poor performance
due to modality forgetting, where the sequential objective overshadows modality
representation. We propose a lightweight knowledge distillation solution that
preserves both merits: retaining modality information and maintaining high
efficiency. Specifically, we introduce a novel method that enhances the
learning of embeddings in SR through the supervision of modality correlations.
The supervision signals are distilled from the original modality
representations, including both (1) holistic correlations, which quantify their
overall associations, and (2) dissected correlation types, which refine their
relationship facets (honing in on specific aspects like color or shape
consistency). To further address the issue of modality forgetting, we propose
an asynchronous learning step, allowing the original information to be retained
longer for training the representation learning module. Our approach is
compatible with various backbone architectures and outperforms the top
baselines by 6.8
original feature associations from modality encoders significantly boosts
task-specific recommendation adaptation. Additionally, we find that larger
modality encoders (e.g., Large Language Models) contain richer feature sets
which necessitate more fine-grained modeling to reach their full performance
potential.|在序列推荐器(SR)中，以端到端的方式编码和利用模式在模式编码器大小方面是昂贵的。两阶段方法可以减轻这种担忧，但是由于情态遗忘，它们的表现很差，其中连续的目标掩盖了情态表示。提出了一种轻量级的知识提取方法，该方法既保留了模态信息，又保持了高效率。具体来说，我们提出了一种新的方法，通过监督情态相关性来提高嵌入的学习效果。监督信号是从原始的模态表示中提取出来的，包括(1)量化其总体关联的整体相关性和(2)剖析的相关类型，这些相关类型细化了它们的关系方面(在特定方面如颜色或形状一致性上打磨)。为了进一步解决模态遗忘问题，我们提出了一个异步学习步骤，允许原始信息保留更长的时间来训练表征学习模块。我们的方法与各种骨干架构兼容，并优于最高基线6.8原始特征关联的形式编码器显着提高任务特定的推荐适应性。此外，我们发现较大的模态编码器(例如，大型语言模型)包含更丰富的特征集，这需要更细粒度的建模来达到其全部性能潜力。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+Modality+Adaptation+to+Sequential+Recommendation+via+Correlation+Supervision)|0|
|[DREQ: Document Re-ranking Using Entity-Based Query Understanding](https://doi.org/10.1007/978-3-031-56027-9_13)|Shubham Chatterjee, Iain Mackie, Jeff Dalton||While entity-oriented neural IR models have advanced significantly, they
often overlook a key nuance: the varying degrees of influence individual
entities within a document have on its overall relevance. Addressing this gap,
we present DREQ, an entity-oriented dense document re-ranking model. Uniquely,
we emphasize the query-relevant entities within a document's representation
while simultaneously attenuating the less relevant ones, thus obtaining a
query-specific entity-centric document representation. We then combine this
entity-centric document representation with the text-centric representation of
the document to obtain a "hybrid" representation of the document. We learn a
relevance score for the document using this hybrid representation. Using four
large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural
and non-neural re-ranking methods, highlighting the effectiveness of our
entity-oriented representation approach.|尽管面向实体的神经 IR 模型已经取得了显著的进步，但它们往往忽略了一个关键的细微差别: 文档中各个实体对其总体相关性的不同程度的影响。针对这一差距，我们提出了面向实体的密集文档重排序模型 DREQ。独特的是，我们强调文档表示中的查询相关实体，同时减弱相关性较差的实体，从而获得一个特定于查询的以实体为中心的文档表示。然后，我们将这种以实体为中心的文档表示与以文本为中心的文档表示结合起来，以获得文档的“混合”表示。我们使用这种混合表示学习文档的相关性得分。使用四个大规模的基准测试，我们表明 DREQ 优于最先进的神经元和非神经元重新排序方法，突出了我们的面向实体的表示方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DREQ:+Document+Re-ranking+Using+Entity-Based+Query+Understanding)|0|
|[Beyond Topicality: Including Multidimensional Relevance in Cross-encoder Re-ranking - The Health Misinformation Case Study](https://doi.org/10.1007/978-3-031-56027-9_16)|Rishabh Upadhyay, Arian Askari, Gabriella Pasi, Marco Viviani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Topicality:+Including+Multidimensional+Relevance+in+Cross-encoder+Re-ranking+-+The+Health+Misinformation+Case+Study)|0|
|[Query Obfuscation for Information Retrieval Through Differential Privacy](https://doi.org/10.1007/978-3-031-56027-9_17)|Guglielmo Faggioli, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Obfuscation+for+Information+Retrieval+Through+Differential+Privacy)|0|
|[On-Device Query Auto-completion for Email Search](https://doi.org/10.1007/978-3-031-56027-9_18)|Yifan Qiao, Otto Godwin, Hua Ouyang||AbstractTraditional query auto-completion (QAC) relies heavily on search logs collected over many users. However, in on-device email search, the scarcity of logs and the governing privacy constraints make QAC a challenging task. In this work, we propose an on-device QAC method that runs directly on users’ devices, where users’ sensitive data and interaction logs are not collected, shared, or aggregated through web services. This method retrieves candidates using pseudo relevance feedback, and ranks them based on relevance signals that explore the textual and structural information from users’ emails. We also propose a private corpora based evaluation method, and empirically demonstrate the effectiveness of our proposed method.|传统的查询自动完成(QAC)在很大程度上依赖于多个用户收集的搜索日志。然而，在设备上的电子邮件搜索，日志的稀缺性和管理隐私的约束使 QAC 一个具有挑战性的任务。在这项工作中，我们提出了一个在设备上的 QAC 方法，直接运行在用户的设备上，其中用户的敏感数据和交互日志不收集，共享，或通过 Web 服务聚合。这种方法使用伪关联反馈检索候选人，并根据相关信号对他们进行排名，这些相关信号探索用户电子邮件的文本和结构信息。我们还提出了一种基于私人语料库的评价方法，并通过实例验证了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On-Device+Query+Auto-completion+for+Email+Search)|0|
|[Does the Performance of Text-to-Image Retrieval Models Generalize Beyond Captions-as-a-Query?](https://doi.org/10.1007/978-3-031-56066-8_15)|Juan Manuel Rodriguez, Nima Tavassoli, Eliezer Levy, Gil Lederman, Dima Sivov, Matteo Lissandrini, Davide Mottin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Does+the+Performance+of+Text-to-Image+Retrieval+Models+Generalize+Beyond+Captions-as-a-Query?)|0|
|[Query Generation Using Large Language Models - A Reproducibility Study of Unsupervised Passage Reranking](https://doi.org/10.1007/978-3-031-56066-8_19)|David Rau, Jaap Kamps||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Generation+Using+Large+Language+Models+-+A+Reproducibility+Study+of+Unsupervised+Passage+Reranking)|0|
|[Ranking Distance Metric for Privacy Budget in Distributed Learning of Finite Embedding Data](https://doi.org/10.1007/978-3-031-56066-8_21)|Georgios Papadopoulos, Yash Satsangi, Shaltiel Eloul, Marco Pistoia||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+Distance+Metric+for+Privacy+Budget+in+Distributed+Learning+of+Finite+Embedding+Data)|0|
|[Effective Adhoc Retrieval Through Traversal of a Query-Document Graph](https://doi.org/10.1007/978-3-031-56063-7_6)|Erlend Frayling, Sean MacAvaney, Craig Macdonald, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Adhoc+Retrieval+Through+Traversal+of+a+Query-Document+Graph)|0|
|[MMCRec: Towards Multi-modal Generative AI in Conversational Recommendation](https://doi.org/10.1007/978-3-031-56063-7_23)|Tendai Mukande, Esraa Ali, Annalina Caputo, Ruihai Dong, Noel E. O'Connor||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMCRec:+Towards+Multi-modal+Generative+AI+in+Conversational+Recommendation)|0|
|[Federated Conversational Recommender Systems](https://doi.org/10.1007/978-3-031-56069-9_4)|Allen Lin, Jianling Wang, Ziwei Zhu, James Caverlee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Conversational+Recommender+Systems)|0|
|[Improving Exposure Allocation in Rankings by Query Generation](https://doi.org/10.1007/978-3-031-56069-9_9)|Thomas Jänich, Graham McDonald, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Exposure+Allocation+in+Rankings+by+Query+Generation)|0|
|[KnowFIRES: A Knowledge-Graph Framework for Interpreting Retrieved Entities from Search](https://doi.org/10.1007/978-3-031-56069-9_15)|Negar Arabzadeh, Kiarash Golzadeh, Christopher Risi, Charles L. A. Clarke, Jian Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KnowFIRES:+A+Knowledge-Graph+Framework+for+Interpreting+Retrieved+Entities+from+Search)|0|
|[A Conversational Search Framework for Multimedia Archives](https://doi.org/10.1007/978-3-031-56069-9_25)|Anastasia Potyagalova, Gareth J. F. Jones||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Conversational+Search+Framework+for+Multimedia+Archives)|0|
|[Effective and Efficient Transformer Models for Sequential Recommendation](https://doi.org/10.1007/978-3-031-56069-9_39)|Aleksandr V. Petrov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+and+Efficient+Transformer+Models+for+Sequential+Recommendation)|0|
|[Quantum Computing for Information Retrieval and Recommender Systems](https://doi.org/10.1007/978-3-031-56069-9_47)|Maurizio Ferrari Dacrema, Andrea Pasin, Paolo Cremonesi, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantum+Computing+for+Information+Retrieval+and+Recommender+Systems)|0|
|[Transformers for Sequential Recommendation](https://doi.org/10.1007/978-3-031-56069-9_49)|Aleksandr V. Petrov, Craig Macdonald|Wuhan University, Wuhan, China; University of Hong Kong, Hong Kong, China; National University of Singapore, Singapore, Singapore; Ocean University of China, Qingdao, China|Learning dynamic user preference has become an increasingly important component for many online platforms (e.g., video-sharing sites, e-commerce systems) to make sequential recommendations. Previous works have made many efforts to model item-item transitions over user interaction sequences, based on various architectures, e.g., recurrent neural networks and self-attention mechanism. Recently emerged graph neural networks also serve as useful backbone models to capture item dependencies in sequential recommendation scenarios. Despite their effectiveness, existing methods have far focused on item sequence representation with singular type of interactions, and thus are limited to capture dynamic heterogeneous relational structures between users and items (e.g., page view, add-to-favorite, purchase). To tackle this challenge, we design a Multi-Behavior Hypergraph-enhanced T ransformer framework (MBHT) to capture both short-term and long-term cross-type behavior dependencies. Specifically, a multi-scale Transformer is equipped with low-rank self-attention to jointly encode behavior-aware sequential patterns from fine-grained and coarse-grained levels. Additionally,we incorporate the global multi-behavior dependency into the hypergraph neural architecture to capture the hierarchical long-range item correlations in a customized manner. Experimental results demonstrate the superiority of our MBHT over various state-of- the-art recommendation solutions across different settings. Further ablation studies validate the effectiveness of our model design and benefits of the new MBHT framework. Our implementation code is released at: https://github.com/yuh-yang/MBHT-KDD22.|学习动态用户偏好已经成为许多在线平台(如视频分享网站、电子商务系统)提供连续推荐的一个越来越重要的组成部分。以往的研究基于多种体系结构，如递归神经网络和自我注意机制，对用户交互序列上的项目-项目转换进行了大量的研究。最近出现的图形神经网络也可以作为有用的骨干模型，以捕获项目依赖的顺序推荐场景。尽管现有的方法很有效，但是现有的方法都集中在单一交互类型的项目序列表示上，因此仅限于捕获用户和项目之间的动态异构关系结构(例如，页面查看、添加到收藏夹、购买)。为了应对这一挑战，我们设计了一个多行为超图增强型 T 变换器框架(MBHT)来捕获短期和长期的跨类型行为依赖。具体而言，多尺度变压器配备低级自注意，以从细粒度和粗粒度级别联合编码行为感知的序列模式。此外，我们将全局多行为依赖引入到超图神经结构中，以自定义的方式获取层次化的长期项目相关性。实验结果表明，我们的 MBHT 优于不同设置的各种最先进的推荐解决方案。进一步的消融研究验证了我们的模型设计的有效性和新的 MBHT 框架的好处。我们的实施代码在以下 https://github.com/yuh-yang/mbht-kdd22发布:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transformers+for+Sequential+Recommendation)|0|
|[Context-Aware Query Term Difficulty Estimation for Performance Prediction](https://doi.org/10.1007/978-3-031-56066-8_4)|Abbas Saleminezhad, Negar Arabzadeh, Soosan Beheshti, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-Aware+Query+Term+Difficulty+Estimation+for+Performance+Prediction)|0|
|[Navigating the Thin Line: Examining User Behavior in Search to Detect Engagement and Backfire Effects](https://doi.org/10.1007/978-3-031-56066-8_30)|Federico Maria Cau, Nava Tintarev||Opinionated users often seek information that aligns with their preexisting
beliefs while dismissing contradictory evidence due to confirmation bias. This
conduct hinders their ability to consider alternative stances when searching
the web. Despite this, few studies have analyzed how the diversification of
search results on disputed topics influences the search behavior of highly
opinionated users. To this end, we present a preregistered user study (n = 257)
investigating whether different levels (low and high) of bias metrics and
search results presentation (with or without AI-predicted stances labels) can
affect the stance diversity consumption and search behavior of opinionated
users on three debated topics (i.e., atheism, intellectual property rights, and
school uniforms). Our results show that exposing participants to
(counter-attitudinally) biased search results increases their consumption of
attitude-opposing content, but we also found that bias was associated with a
trend toward overall fewer interactions within the search page. We also found
that 19
any search results. When we removed these participants in a post-hoc analysis,
we found that stance labels increased the diversity of stances consumed by
users, particularly when the search results were biased. Our findings highlight
the need for future research to explore distinct search scenario settings to
gain insight into opinionated users' behavior.|固执己见的用户往往寻求与他们先前存在的信念相一致的信息，而由于确认偏见而排除相互矛盾的证据。这种行为妨碍了他们在搜索网页时考虑其他立场的能力。尽管如此，很少有研究分析有争议话题的搜索结果的多样化如何影响高度固执己见的用户的搜索行为。为此，我们提出了一项预先注册的用户研究(n = 257) ，调查不同水平(低和高)的偏倚指标和搜索结果表示(有或没有 AI 预测的立场标签)是否会影响立场多样性消费和搜索行为有意见的用户在三个有争议的话题(即无神论，知识产权和校服)。我们的研究结果显示，将参与者暴露在(反态度的)有偏见的搜索结果中，会增加他们对与态度相反的内容的消费，但是我们也发现，偏见与搜索页面内的整体互动减少的趋势有关。我们还发现19任何搜索结果。当我们在一个事后比较中移除这些参与者时，我们发现立场标签增加了用户使用的立场的多样性，特别是当搜索结果有偏见时。我们的研究结果强调了未来研究探索不同搜索场景设置的必要性，以深入了解固执己见的用户的行为。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+the+Thin+Line:+Examining+User+Behavior+in+Search+to+Detect+Engagement+and+Backfire+Effects)|0|
|[Measuring Bias in a Ranked List Using Term-Based Representations](https://doi.org/10.1007/978-3-031-56069-9_1)|Amin Abolghasemi, Leif Azzopardi, Arian Askari, Maarten de Rijke, Suzan Verberne||In most recent studies, gender bias in document ranking is evaluated with the
NFaiRR metric, which measures bias in a ranked list based on an aggregation
over the unbiasedness scores of each ranked document. This perspective in
measuring the bias of a ranked list has a key limitation: individual documents
of a ranked list might be biased while the ranked list as a whole balances the
groups' representations. To address this issue, we propose a novel metric
called TExFAIR (term exposure-based fairness), which is based on two new
extensions to a generic fairness evaluation framework, attention-weighted
ranking fairness (AWRF). TExFAIR assesses fairness based on the term-based
representation of groups in a ranked list: (i) an explicit definition of
associating documents to groups based on probabilistic term-level associations,
and (ii) a rank-biased discounting factor (RBDF) for counting
non-representative documents towards the measurement of the fairness of a
ranked list. We assess TExFAIR on the task of measuring gender bias in passage
ranking, and study the relationship between TExFAIR and NFaiRR. Our experiments
show that there is no strong correlation between TExFAIR and NFaiRR, which
indicates that TExFAIR measures a different dimension of fairness than NFaiRR.
With TExFAIR, we extend the AWRF framework to allow for the evaluation of
fairness in settings with term-based representations of groups in documents in
a ranked list.|在最近的大多数研究中，文档排名中的性别偏见是通过 NFaiRR 度量来评估的，该度量基于每个排名文档的无偏评分的聚合来衡量排名列表中的偏见。这种测量排名表偏差的视角有一个关键的局限性: 排名表的个别文档可能有偏差，而排名表作为一个整体平衡各组的表示。为了解决这个问题，我们提出了一种新的度量方法 TExFAIR (术语暴露公平性) ，它基于通用公平性评估框架的两个新的扩展，即注意力加权排序公平性(AWRF)。TExFAIR 基于排名列表中基于术语的群体表示来评估公平性: (i)基于概率术语水平关联的关联文档与群体的明确定义，以及(ii)用于计数非代表性文档的排名折扣因子(RBDF)对排名列表的公平性进行测量。我们通过测量文章排序中的性别偏见来评估 TExFAIR，并研究 TExFAIR 和 NFaiRR 之间的关系。我们的实验表明，TExFAIR 和 NFaiRR 之间没有很强的相关性，这表明 TExFAIR 测量的公平性维度不同于 NFaiRR。通过 TExFAIR，我们扩展了 AWRF 框架，允许在排名列表中的文档中使用基于术语的群组表示来评估设置中的公平性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Bias+in+a+Ranked+List+Using+Term-Based+Representations)|0|
|[Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation](https://doi.org/10.1007/978-3-031-56060-6_4)|Eugene Yang, Dawn J. Lawrie, James Mayfield, Douglas W. Oard, Scott Miller||Prior work on English monolingual retrieval has shown that a cross-encoder trained using a large number of relevance judgments for query-document pairs can be used as a teacher to train more efficient, but similarly effective, dual-encoder student models. Applying a similar knowledge distillation approach to training an efficient dual-encoder model for Cross-Language Information Retrieval (CLIR), where queries and documents are in different languages, is challenging due to the lack of a sufficiently large training collection when the query and document languages differ. The state of the art for CLIR thus relies on translating queries, documents, or both from the large English MS MARCO training set, an approach called Translate-Train. This paper proposes an alternative, Translate-Distill, in which knowledge distillation from either a monolingual cross-encoder or a CLIR cross-encoder is used to train a dual-encoder CLIR student model. This richer design space enables the teacher model to perform inference in an optimized setting, while training the student model directly for CLIR. Trained models and artifacts are publicly available on Huggingface.|先前关于英语单语检索的工作已经表明，使用大量的查询文档对相关性判断训练的交叉编码器可以作为教师来训练更有效但同样有效的双编码器学生模型。应用类似的知识提取方法来训练一个有效的双跨语检索编码器模型(CLIR) ，其中查询和文档使用不同的语言，这是一个挑战，因为当查询和文档语言不同时，缺乏足够大训练集合。因此，CLIR 的技术状态依赖于翻译查询、文档，或者两者都来自大型英文 MS MARCO 训练集，这种方法称为 Translate-Train。本文提出了一种翻译-提取的方法，利用从单语交叉编码器或 CLIR 交叉编码器中提取的知识来训练双语交叉编码器的学生模型。这个更丰富的设计空间使得教师模型能够在一个优化的设置中执行推理，同时直接为 CLIR 培训学生模型。受过训练的模型和工件可以在 Huggingface 上公开获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Translate-Distill:+Learning+Cross-Language+Dense+Retrieval+by+Translation+and+Distillation)|0|
|[DESIRE-ME: Domain-Enhanced Supervised Information Retrieval Using Mixture-of-Experts](https://doi.org/10.1007/978-3-031-56060-6_8)|Pranav Kasela, Gabriella Pasi, Raffaele Perego, Nicola Tonellotto||Open-domain question answering requires retrieval systems able to cope with
the diverse and varied nature of questions, providing accurate answers across a
broad spectrum of query types and topics. To deal with such topic heterogeneity
through a unique model, we propose DESIRE-ME, a neural information retrieval
model that leverages the Mixture-of-Experts framework to combine multiple
specialized neural models. We rely on Wikipedia data to train an effective
neural gating mechanism that classifies the incoming query and that weighs the
predictions of the different domain-specific experts correspondingly. This
allows DESIRE-ME to specialize adaptively in multiple domains. Through
extensive experiments on publicly available datasets, we show that our proposal
can effectively generalize domain-enhanced neural models. DESIRE-ME excels in
handling open-domain questions adaptively, boosting by up to 12
22|开放领域的问题回答要求检索系统能够处理各种各样的问题，提供准确的答案跨广泛的查询类型和主题。为了通过一个独特的模型来处理这样的话题异质性，我们提出了 DESIRE-ME，一个神经信息检索模型，它利用专家混合框架来结合多个专门的神经模型。我们依靠 Wikipedia 数据来训练一种有效的神经门控机制，该机制对传入的查询进行分类，并相应地权衡不同领域专家的预测。这使得 DESIRE-ME 可以自适应地专门处理多个域。通过在公开数据集上的大量实验，我们表明我们的方案可以有效地推广领域增强的神经模型。DESIRE-ME 擅长于自适应地处理开放领域的问题，最多可提高12|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DESIRE-ME:+Domain-Enhanced+Supervised+Information+Retrieval+Using+Mixture-of-Experts)|0|
|[A Deep Learning Approach for Selective Relevance Feedback](https://doi.org/10.1007/978-3-031-56060-6_13)|Suchana Datta, Debasis Ganguly, Sean MacAvaney, Derek Greene||Pseudo-relevance feedback (PRF) can enhance average retrieval effectiveness
over a sufficiently large number of queries. However, PRF often introduces a
drift into the original information need, thus hurting the retrieval
effectiveness of several queries. While a selective application of PRF can
potentially alleviate this issue, previous approaches have largely relied on
unsupervised or feature-based learning to determine whether a query should be
expanded. In contrast, we revisit the problem of selective PRF from a deep
learning perspective, presenting a model that is entirely data-driven and
trained in an end-to-end manner. The proposed model leverages a
transformer-based bi-encoder architecture. Additionally, to further improve
retrieval effectiveness with this selective PRF approach, we make use of the
model's confidence estimates to combine the information from the original and
expanded queries. In our experiments, we apply this selective feedback on a
number of different combinations of ranking and feedback models, and show that
our proposed approach consistently improves retrieval effectiveness for both
sparse and dense ranking models, with the feedback models being either sparse,
dense or generative.|伪相关反馈(PRF)可以提高对足够大数量查询的平均检索效率。然而，PRF 常常引入对原始信息需求的漂移，从而影响了多个查询的检索效率。尽管 PRF 的选择性应用有可能缓解这一问题，但以前的方法在很大程度上依赖于无监督或基于特征的学习来确定是否应该扩展查询。相比之下，我们从深度学习的角度重新审视选择性 PRF 的问题，提出了一个完全由数据驱动并以端到端方式进行训练的模型。该模型利用了基于变压器的双编码器结构。此外，为了进一步提高这种选择性 PRF 方法的检索效率，我们利用模型的置信度估计来组合来自原始和扩展查询的信息。在我们的实验中，我们将这种选择性反馈应用于许多不同的排序和反馈模型组合，并且表明我们提出的方法始终如一地提高了稀疏和密集排序模型的检索效率，反馈模型要么是稀疏的，要么是密集的，要么是生成的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Deep+Learning+Approach+for+Selective+Relevance+Feedback)|0|
|[Self Contrastive Learning for Session-Based Recommendation](https://doi.org/10.1007/978-3-031-56027-9_1)|Zhengxiang Shi, Xi Wang, Aldo Lipani||Session-based recommendation, which aims to predict the next item of users' interest as per an existing sequence interaction of items, has attracted growing applications of Contrastive Learning (CL) with improved user and item representations. However, these contrastive objectives: (1) serve a similar role as the cross-entropy loss while ignoring the item representation space optimisation; and (2) commonly require complicated modelling, including complex positive/negative sample constructions and extra data augmentation. In this work, we introduce Self-Contrastive Learning (SCL), which simplifies the application of CL and enhances the performance of state-of-the-art CL-based recommendation techniques. Specifically, SCL is formulated as an objective function that directly promotes a uniform distribution among item representations and efficiently replaces all the existing contrastive objective components of state-of-the-art models. Unlike previous works, SCL eliminates the need for any positive/negative sample construction or data augmentation, leading to enhanced interpretability of the item representation space and facilitating its extensibility to existing recommender systems. Through experiments on three benchmark datasets, we demonstrate that SCL consistently improves the performance of state-of-the-art models with statistical significance. Notably, our experiments show that SCL improves the performance of two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and 11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks. Additionally, our analysis elucidates the improvement in terms of alignment and uniformity of representations, as well as the effectiveness of SCL with a low computational cost.|基于会话的推荐，旨在根据已有的项目序列交互预测用户的下一个兴趣项目，已经吸引了越来越多的应用对比学习(CL)与改进的用户和项目表示。然而，这些对比的目标: (1)服务于类似的作用作为交叉熵损失，而忽略项目表示空间优化; (2)通常需要复杂的建模，包括复杂的正/负样本结构和额外的数据增强。本文介绍了自对比学习(SCL) ，简化了 CL 的应用，提高了基于 CL 的推荐技术的性能。具体来说，SCL 是一个直接促进项目表征之间均匀分布的目标函数，它有效地替代了现有最先进模型的所有对比性目标成分。与以前的工作不同，SCL 消除了任何正/负样本构建或数据增强的需要，从而增强了项目表示空间的可解释性，并促进了其对现有推荐系统的可扩展性。通过对三个基准数据集的实验，我们证明了 SCL 能够持续地提高具有统计学意义的最先进模型的性能。值得注意的是，我们的实验表明，在不同的基准测试中，SCL 提高了两个性能最好的模型的性能，P@10(精度)平均提高了8.2% 和9.5% ，MRR@10(平均倒数排名)平均提高了9.9% 和11.2% 。此外，我们的分析阐明了改进方面的对齐和一致性的表示，以及有效的 SCL 与低计算成本。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self+Contrastive+Learning+for+Session-Based+Recommendation)|0|
|[Revealing the Hidden Impact of Top-N Metrics on Optimization in Recommender Systems](https://doi.org/10.1007/978-3-031-56027-9_9)|Lukas Wegmeth, Tobias Vente, Lennart Purucker||The hyperparameters of recommender systems for top-n predictions are
typically optimized to enhance the predictive performance of algorithms.
Thereby, the optimization algorithm, e.g., grid search or random search,
searches for the best hyperparameter configuration according to an
optimization-target metric, like nDCG or Precision. In contrast, the optimized
algorithm, internally optimizes a different loss function during training, like
squared error or cross-entropy. To tackle this discrepancy, recent work focused
on generating loss functions better suited for recommender systems. Yet, when
evaluating an algorithm using a top-n metric during optimization, another
discrepancy between the optimization-target metric and the training loss has so
far been ignored. During optimization, the top-n items are selected for
computing a top-n metric; ignoring that the top-n items are selected from the
recommendations of a model trained with an entirely different loss function.
Item recommendations suitable for optimization-target metrics could be outside
the top-n recommended items; hiddenly impacting the optimization performance.
Therefore, we were motivated to analyze whether the top-n items are optimal for
optimization-target top-n metrics. In pursuit of an answer, we exhaustively
evaluate the predictive performance of 250 selection strategies besides
selecting the top-n. We extensively evaluate each selection strategy over
twelve implicit feedback and eight explicit feedback data sets with eleven
recommender systems algorithms. Our results show that there exist selection
strategies other than top-n that increase predictive performance for various
algorithms and recommendation domains. However, the performance of the top  43
of selection strategies is not significantly different. We discuss the impact
of our findings on optimization and re-ranking in recommender systems and
feasible solutions.|为了提高算法的预测性能，对推荐系统的超参数进行了典型的优化。因此，优化算法，例如网格搜索或随机搜索，根据优化目标度量(如 nDCG 或 Precision)搜索最佳超参数配置。相比之下，优化后的算法，在训练期间内部优化了不同的损失函数，如平方误差或交叉熵。为了解决这个差异，最近的工作集中在产生更适合推荐系统的损失函数。然而，当在优化过程中使用 top-n 度量对算法进行评估时，优化目标度量与训练损失之间的另一个差异被忽略了。在优化过程中，选择 top-n 项目来计算 top-n 度量; 忽略 top-n 项目是从使用完全不同的损失函数训练的模型的建议中选择的。适合于优化的项目推荐——目标指标可能不在推荐项目的前列; 这会对优化性能产生隐性影响。因此，我们被激励去分析是否前 n 个项目对于优化目标的前 n 个度量是最佳的。在寻找答案的过程中，我们除了选择前 n 个选择策略外，还对250个选择策略的预测性能进行了详尽的评估。我们使用十二个隐式反馈和8个显式反馈数据集和十一个推荐系统算法对每个选择策略进行了广泛的评估。我们的研究结果表明，除了 top-n 之外，还存在其他的选择策略可以提高各种算法和推荐域的预测性能。然而，前43名选择策略的表现并没有显著差异。我们讨论了我们的研究结果对优化和重新排序的推荐系统和可行的解决方案的影响。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revealing+the+Hidden+Impact+of+Top-N+Metrics+on+Optimization+in+Recommender+Systems)|0|
|[TWOLAR: A TWO-Step LLM-Augmented Distillation Method for Passage Reranking](https://doi.org/10.1007/978-3-031-56027-9_29)|Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni||In this paper, we present TWOLAR: a two-stage pipeline for passage reranking
based on the distillation of knowledge from Large Language Models (LLM). TWOLAR
introduces a new scoring strategy and a distillation process consisting in the
creation of a novel and diverse training dataset. The dataset consists of 20K
queries, each associated with a set of documents retrieved via four distinct
retrieval methods to ensure diversity, and then reranked by exploiting the
zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate
the contribution of each new component we introduced. Our experimental results
show that TWOLAR significantly enhances the document reranking ability of the
underlying model, matching and in some cases even outperforming
state-of-the-art models with three orders of magnitude more parameters on the
TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate
future work we release our data set, finetuned models, and code.|在本文中，我们提出了 TWOLAR: 一个基于从大语言模型(LLM)中提取知识的两阶段通道重新排序流水线。TWOLAR 引入了一个新的评分策略和一个精馏过程，包括创建一个新的和多样化的训练数据集。该数据集由20K 个查询组成，每个查询与一组文档相关联，这些文档通过四种不同的检索方法检索以确保多样性，然后通过利用 LLM 的零拍重新排序功能进行重新排序。我们的消融研究证明了我们引入的每个新组件的贡献。我们的实验结果显示，TWOLAR 显著提高了基础模型的文档重新排序能力，在 TREC-dL 测试集和零拍评估基准 BEIR 上，通过三个以上的参数，匹配甚至在某些情况下超越了最先进的模型，从而提高了文档重新排序的数量级。为了方便未来的工作，我们发布了我们的数据集、微调模型和代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TWOLAR:+A+TWO-Step+LLM-Augmented+Distillation+Method+for+Passage+Reranking)|0|
|[Estimating Query Performance Through Rich Contextualized Query Representations](https://doi.org/10.1007/978-3-031-56066-8_6)|Sajad Ebrahimi, Maryam Khodabakhsh, Negar Arabzadeh, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Estimating+Query+Performance+Through+Rich+Contextualized+Query+Representations)|0|
|[Performance Comparison of Session-Based Recommendation Algorithms Based on GNNs](https://doi.org/10.1007/978-3-031-56066-8_12)|Faisal Shehzad, Dietmar Jannach||In session-based recommendation settings, a recommender system has to base
its suggestions on the user interactions that are ob served in an ongoing
session. Since such sessions can consist of only a small set of interactions,
various approaches based on Graph Neural Networks (GNN) were recently proposed,
as they allow us to integrate various types of side information about the items
in a natural way. Unfortunately, a variety of evaluation settings are used in
the literature, e.g., in terms of protocols, metrics and baselines, making it
difficult to assess what represents the state of the art. In this work, we
present the results of an evaluation of eight recent GNN-based approaches that
were published in high-quality outlets. For a fair comparison, all models are
systematically tuned and tested under identical conditions using three common
datasets. We furthermore include k-nearest-neighbor and sequential rules-based
models as baselines, as such models have previously exhibited competitive
performance results for similar settings. To our surprise, the evaluation
showed that the simple models outperform all recent GNN models in terms of the
Mean Reciprocal Rank, which we used as an optimization criterion, and were only
outperformed in three cases in terms of the Hit Rate. Additional analyses
furthermore reveal that several other factors that are often not deeply
discussed in papers, e.g., random seeds, can markedly impact the performance of
GNN-based models. Our results therefore (a) point to continuing issues in the
community in terms of research methodology and (b) indicate that there is ample
room for improvement in session-based recommendation.|在基于会话的推荐设置中，推荐系统必须根据当前会话中的用户交互情况提出建议。由于这样的会议可以只包括一小组交互，最近提出了各种基于图神经网络(GNN)的方法，因为它们允许我们以一种自然的方式整合关于项目的各种类型的副信息。不幸的是，文献中使用了各种各样的评估设置，例如，在协议、指标和基线方面，这使得评估什么代表了最先进的技术变得困难。在这项工作中，我们介绍了最近在高质量网点发表的八种基于 GNN 的方法的评价结果。为了进行公平的比较，使用三个共同的数据集，在相同的条件下系统地调整和测试所有模型。我们进一步包括 k 最近邻和顺序规则为基线的模型，因为这样的模型已经表现出竞争性能结果在类似的设置。令我们惊讶的是，评估显示，简单的模型在平均倒数排名方面表现优于所有最近的 GNN 模型，我们将其作为优化标准，在命中率方面只有三种情况表现优于 GNN 模型。进一步的分析表明，论文中通常不深入讨论的其他几个因素，例如随机种子，可以显著影响基于 GNN 的模型的性能。因此，我们的研究结果(a)指出了社区在研究方法方面仍然存在的问题，(b)表明在基于会话的推荐方面还有很大的改进空间。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Performance+Comparison+of+Session-Based+Recommendation+Algorithms+Based+on+GNNs)|0|
|[Weighted AUReC: Handling Skew in Shard Map Quality Estimation for Selective Search](https://doi.org/10.1007/978-3-031-56066-8_10)|Gijs Hendriksen, Djoerd Hiemstra, Arjen P. de Vries||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weighted+AUReC:+Handling+Skew+in+Shard+Map+Quality+Estimation+for+Selective+Search)|0|
|[Measuring Item Fairness in Next Basket Recommendation: A Reproducibility Study](https://doi.org/10.1007/978-3-031-56066-8_18)|Yuanna Liu, Ming Li, Mozhdeh Ariannezhad, Masoud Mansoury, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+Item+Fairness+in+Next+Basket+Recommendation:+A+Reproducibility+Study)|0|
|[Is Interpretable Machine Learning Effective at Feature Selection for Neural Learning-to-Rank?](https://doi.org/10.1007/978-3-031-56066-8_29)|Lijun Lyu, Nirmal Roy, Harrie Oosterhuis, Avishek Anand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Interpretable+Machine+Learning+Effective+at+Feature+Selection+for+Neural+Learning-to-Rank?)|0|
|[The Impact of Differential Privacy on Recommendation Accuracy and Popularity Bias](https://doi.org/10.1007/978-3-031-56066-8_33)|Peter Müllner, Elisabeth Lex, Markus Schedl, Dominik Kowald||Collaborative filtering-based recommender systems leverage vast amounts of
behavioral user data, which poses severe privacy risks. Thus, often, random
noise is added to the data to ensure Differential Privacy (DP). However, to
date, it is not well understood, in which ways this impacts personalized
recommendations. In this work, we study how DP impacts recommendation accuracy
and popularity bias, when applied to the training data of state-of-the-art
recommendation models. Our findings are three-fold: First, we find that nearly
all users' recommendations change when DP is applied. Second, recommendation
accuracy drops substantially while recommended item popularity experiences a
sharp increase, suggesting that popularity bias worsens. Third, we find that DP
exacerbates popularity bias more severely for users who prefer unpopular items
than for users that prefer popular items.|基于协同过滤的推荐系统利用了大量的行为用户数据，这带来了严重的隐私风险。因此，随机噪音往往被添加到数据中，以确保差分隐私(DP)。然而，到目前为止，人们还没有很好地理解这对个性化推荐的影响。在本研究中，我们研究了当应用于最先进的推荐模型的训练数据时，DP 如何影响推荐的准确性和受欢迎程度偏差。我们的发现有三个方面: 首先，我们发现几乎所有用户的建议在应用 DP 时都会发生变化。其次，推荐的准确性大幅下降，而推荐项目的流行经历了急剧增加，这表明流行偏差恶化。第三，我们发现对于喜欢不受欢迎项目的用户而言，DP 加剧流行偏见的程度要比喜欢受欢迎项目的用户严重得多。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Differential+Privacy+on+Recommendation+Accuracy+and+Popularity+Bias)|0|
|[How to Forget Clients in Federated Online Learning to Rank?](https://doi.org/10.1007/978-3-031-56063-7_7)|Shuyi Wang, Bing Liu, Guido Zuccon||Data protection legislation like the European Union's General Data Protection
Regulation (GDPR) establishes the right to be forgotten: a user
(client) can request contributions made using their data to be removed from
learned models. In this paper, we study how to remove the contributions made by
a client participating in a Federated Online Learning to Rank (FOLTR) system.
In a FOLTR system, a ranker is learned by aggregating local updates to the
global ranking model. Local updates are learned in an online manner at a
client-level using queries and implicit interactions that have occurred within
that specific client. By doing so, each client's local data is not shared with
other clients or with a centralised search service, while at the same time
clients can benefit from an effective global ranking model learned from
contributions of each client in the federation.
  In this paper, we study an effective and efficient unlearning method that can
remove a client's contribution without compromising the overall ranker
effectiveness and without needing to retrain the global ranker from scratch. A
key challenge is how to measure whether the model has unlearned the
contributions from the client c^* that has requested removal. For this, we
instruct c^* to perform a poisoning attack (add noise to this client updates)
and then we measure whether the impact of the attack is lessened when the
unlearning process has taken place. Through experiments on four datasets, we
demonstrate the effectiveness and efficiency of the unlearning strategy under
different combinations of parameter settings.|数据保护立法，如欧盟的一般数据保护条例(GDPR)规定了被遗忘的权利: 用户(客户)可以要求使用他们的数据作出贡献，从学习的模型中删除。在本文中，我们研究了如何删除参与联邦在线学习排名(FOLTR)系统的客户所做的贡献。在 FOLTR 系统中，通过将本地更新聚合到全局排名模型中来学习排名器。使用在特定客户端中发生的查询和隐式交互，在客户端级别以联机方式学习本地更新。通过这样做，每个客户的本地数据不会与其他客户共享，也不会与中央搜索服务共享，同时客户可以从联合会中每个客户贡献的有效全球排名模型中受益。在本文中，我们研究了一个有效和高效的去除方法，可以消除客户的贡献，而不损害整体排名有效性，不需要从头再培训全球排名。一个关键的挑战是如何衡量模型是否已经从请求删除的客户机 c ^ * 那里忘记了贡献。为此，我们指示 c ^ * 执行中毒攻击(为客户端更新添加噪声) ，然后在发生忘记过程时测量攻击的影响是否减轻。通过对四个数据集的实验，验证了在不同的参数设置组合下，忘却策略的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+Forget+Clients+in+Federated+Online+Learning+to+Rank?)|0|
|[InDi: Informative and Diverse Sampling for Dense Retrieval](https://doi.org/10.1007/978-3-031-56063-7_16)|Nachshon Cohen, Hedda Cohen Indelman, Yaron Fairstein, Guy Kushilevitz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InDi:+Informative+and+Diverse+Sampling+for+Dense+Retrieval)|0|
|[Learning-to-Rank with Nested Feedback](https://doi.org/10.1007/978-3-031-56063-7_22)|Hitesh Sagtani, Olivier Jeunen, Aleksei Ustimenko||Many platforms on the web present ranked lists of content to users, typically
optimized for engagement-, satisfaction- or retention- driven metrics. Advances
in the Learning-to-Rank (LTR) research literature have enabled rapid growth in
this application area. Several popular interfaces now include nested lists,
where users can enter a 2nd-level feed via any given 1st-level item. Naturally,
this has implications for evaluation metrics, objective functions, and the
ranking policies we wish to learn. We propose a theoretically grounded method
to incorporate 2nd-level feedback into any 1st-level ranking model. Online
experiments on a large-scale recommendation system confirm our theoretical
findings.|网络上的许多平台对用户的内容列表进行排序，通常针对参与度、满意度或保留驱动的指标进行优化。学习到等级(LTR)研究文献的进步使得这一应用领域的快速增长成为可能。一些流行的界面现在包括嵌套列表，用户可以通过任何给定的第一级项目输入第二级提要。当然，这对评估指标、目标函数和我们希望学习的排名策略都有影响。我们提出了一个理论基础的方法，将二级反馈纳入任何一级排名模型。在一个大规模推荐系统上的在线实验证实了我们的理论发现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning-to-Rank+with+Nested+Feedback)|0|
|[Simple Domain Adaptation for Sparse Retrievers](https://doi.org/10.1007/978-3-031-56063-7_32)|Mathias Vast, Yuxuan Zong, Benjamin Piwowarski, Laure Soulier||In Information Retrieval, and more generally in Natural Language Processing,
adapting models to specific domains is conducted through fine-tuning. Despite
the successes achieved by this method and its versatility, the need for
human-curated and labeled data makes it impractical to transfer to new tasks,
domains, and/or languages when training data doesn't exist. Using the model
without training (zero-shot) is another option that however suffers an
effectiveness cost, especially in the case of first-stage retrievers. Numerous
research directions have emerged to tackle these issues, most of them in the
context of adapting to a task or a language. However, the literature is scarcer
for domain (or topic) adaptation. In this paper, we address this issue of
cross-topic discrepancy for a sparse first-stage retriever by transposing a
method initially designed for language adaptation. By leveraging pre-training
on the target data to learn domain-specific knowledge, this technique
alleviates the need for annotated data and expands the scope of domain
adaptation. Despite their relatively good generalization ability, we show that
even sparse retrievers can benefit from our simple domain adaptation method.|在自然语言处理信息检索，以及更广泛的自然语言处理领域，通过微调来调整模型以适应特定的领域。尽管这种方法取得了成功，而且通用性强，但是对人工管理和标记数据的需求使得在培训数据不存在的情况下将数据转移到新的任务、领域和/或语言是不切实际的。不经训练就使用该模型(零射击)是另一种选择，但是这种方法会带来有效性损失，特别是对于第一阶段的检索器。为了解决这些问题，出现了许多研究方向，其中大多数是在适应一项任务或一种语言的背景下。然而，文献对领域(或主题)的适应性较少。在本文中，我们解决这个问题的跨主题差异的稀疏第一阶段的检索，移位的方法最初设计的语言适应。通过利用对目标数据的预训练来学习特定领域的知识，该技术减轻了对带注释数据的需求，并扩大了领域适应的范围。尽管它们具有相对较好的泛化能力，但是我们表明即使是稀疏的检索器也可以从我们简单的领域自适应方法中受益。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+Domain+Adaptation+for+Sparse+Retrievers)|0|
|[Selma: A Semantic Local Code Search Platform](https://doi.org/10.1007/978-3-031-56069-9_21)|Anja Reusch, Guilherme C. Lopes, Wilhelm Pertsch, Hannes Ueck, Julius Gonsior, Wolfgang Lehner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Selma:+A+Semantic+Local+Code+Search+Platform)|0|
|[FAR-AI: A Modular Platform for Investment Recommendation in the Financial Domain](https://doi.org/10.1007/978-3-031-56069-9_30)|Javier SanzCruzado, Edward Richards, Richard McCreadie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FAR-AI:+A+Modular+Platform+for+Investment+Recommendation+in+the+Financial+Domain)|0|
|[Semantic Content Search on IKEA.com](https://doi.org/10.1007/978-3-031-56069-9_32)|Mateusz Slominski, Ezgi Yildirim, Martin Tegner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Content+Search+on+IKEA.com)|0|
|[Semantic Search in Archive Collections Through Interpretable and Adaptable Relation Extraction About Person and Places](https://doi.org/10.1007/978-3-031-56069-9_37)|Nicolas Gutehrlé||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semantic+Search+in+Archive+Collections+Through+Interpretable+and+Adaptable+Relation+Extraction+About+Person+and+Places)|0|
|[Reproduction and Simulation of Interactive Retrieval Experiments](https://doi.org/10.1007/978-3-031-56069-9_40)|Jana Isabelle Friese||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reproduction+and+Simulation+of+Interactive+Retrieval+Experiments)|0|
|[Efficient Multi-vector Dense Retrieval with Bit Vectors](https://doi.org/10.1007/978-3-031-56060-6_1)|Franco Maria Nardini, Cosimo Rulli, Rossano Venturini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Multi-vector+Dense+Retrieval+with+Bit+Vectors)|0|
|[Prompt-Based Generative News Recommendation (PGNR): Accuracy and Controllability](https://doi.org/10.1007/978-3-031-56060-6_5)|Xinyi Li, Yongfeng Zhang, Edward C. Malthouse||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt-Based+Generative+News+Recommendation+(PGNR):+Accuracy+and+Controllability)|0|
|[CaseGNN: Graph Neural Networks for Legal Case Retrieval with Text-Attributed Graphs](https://doi.org/10.1007/978-3-031-56060-6_6)|Yanran Tang, Ruihong Qiu, Yilun Liu, Xue Li, Zi Huang||Legal case retrieval is an information retrieval task in the legal domain, which aims to retrieve relevant cases with a given query case. Recent research of legal case retrieval mainly relies on traditional bag-of-words models and language models. Although these methods have achieved significant improvement in retrieval accuracy, there are still two challenges: (1) Legal structural information neglect. Previous neural legal case retrieval models mostly encode the unstructured raw text of case into a case representation, which causes the lack of important legal structural information in a case and leads to poor case representation; (2) Lengthy legal text limitation. When using the powerful BERT-based models, there is a limit of input text lengths, which inevitably requires to shorten the input via truncation or division with a loss of legal context information. In this paper, a graph neural networks-based legal case retrieval model, CaseGNN, is developed to tackle these challenges. To effectively utilise the legal structural information during encoding, a case is firstly converted into a Text-Attributed Case Graph (TACG), followed by a designed Edge Graph Attention Layer and a readout function to obtain the case graph representation. The CaseGNN model is optimised with a carefully designed contrastive loss with easy and hard negative sampling. Since the text attributes in the case graph come from individual sentences, the restriction of using language models is further avoided without losing the legal context. Extensive experiments have been conducted on two benchmarks from COLIEE 2022 and COLIEE 2023, which demonstrate that CaseGNN outperforms other state-of-the-art legal case retrieval methods. The code has been released on https://github.com/yanran-tang/CaseGNN.|法律案例检索是法律领域的一项信息检索工作，其目的是检索具有给定查询案例的相关案例。目前法律案例检索的研究主要依赖于传统的词袋模型和语言模型。虽然这些方法在检索精度方面取得了显著的进步，但仍然存在两个挑战: (1)法律结构信息的忽视。以往的神经网络法律案例检索模型大多将非结构化的原始案例文本编码为案例表示，导致案例缺乏重要的法律结构信息，导致案例表示效果不佳;。在使用基于 BERT 的强大模型时，存在输入文本长度的限制，这就不可避免地要求通过截断或除法来缩短输入，同时丢失法律上下文信息。本文提出了一种基于图神经网络的法律案例检索模型 CaseGNN，以解决这些问题。为了在编码过程中有效地利用法律结构信息，首先将案例转换为文本属性案例图(TACG) ，然后设计边缘图注意层和读出功能，得到案例图表示。CaseGNN 模型通过精心设计的对比损失和简单和硬负采样进行优化。由于案例图中的文本属性来自于单个句子，因此在不失去法律上下文的前提下，进一步避免了语言模型的使用限制。对 COLIEE 2022和 COLIEE 2023的两个基准进行了广泛的实验，证明 CaseGNN 优于其他最先进的法律案例检索方法。密码已经在 https://github.com/yanran-tang/casegnn 上发布了。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaseGNN:+Graph+Neural+Networks+for+Legal+Case+Retrieval+with+Text-Attributed+Graphs)|0|
|[Context-Driven Interactive Query Simulations Based on Generative Large Language Models](https://doi.org/10.1007/978-3-031-56060-6_12)|Björn Engelmann, Timo Breuer, Jana Isabelle Friese, Philipp Schaer, Norbert Fuhr||Simulating user interactions enables a more user-oriented evaluation of
information retrieval (IR) systems. While user simulations are cost-efficient
and reproducible, many approaches often lack fidelity regarding real user
behavior. Most notably, current user models neglect the user's context, which
is the primary driver of perceived relevance and the interactions with the
search results. To this end, this work introduces the simulation of
context-driven query reformulations. The proposed query generation methods
build upon recent Large Language Model (LLM) approaches and consider the user's
context throughout the simulation of a search session. Compared to simple
context-free query generation approaches, these methods show better
effectiveness and allow the simulation of more efficient IR sessions.
Similarly, our evaluations consider more interaction context than current
session-based measures and reveal interesting complementary insights in
addition to the established evaluation protocols. We conclude with directions
for future work and provide an entirely open experimental setup.|通过模拟用户交互，可以对信息检索系统进行更加面向用户的评估。虽然用户模拟具有成本效益和可重复性，但许多方法通常缺乏真实用户行为的保真度。最值得注意的是，当前的用户模型忽视了用户的上下文，而上下文是感知相关性和与搜索结果交互的主要驱动因素。为此，本文介绍了上下文驱动的查询重构的仿真。提出的查询生成方法建立在最新的大型语言模型(LLM)方法的基础上，并在搜索会话的整个仿真过程中考虑用户的上下文。与简单的上下文无关的查询生成方法相比，这些方法显示出更好的效率，并允许模拟更有效的 IR 会话。同样，我们的评价考虑了比目前基于会议的措施更多的互动背景，除了既定的评价方案之外，还揭示了有趣的互补见解。我们总结了未来工作的方向，并提供了一个完全开放的实验装置。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Context-Driven+Interactive+Query+Simulations+Based+on+Generative+Large+Language+Models)|0|
|[Emotional Insights for Food Recommendations](https://doi.org/10.1007/978-3-031-56060-6_16)|Mehrdad Rostami, Ali Vardasbi, Mohammad Aliannejadi, Mourad Oussalah||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Emotional+Insights+for+Food+Recommendations)|0|
|[LaQuE: Enabling Entity Search at Scale](https://doi.org/10.1007/978-3-031-56060-6_18)|Negar Arabzadeh, Amin Bigdeli, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LaQuE:+Enabling+Entity+Search+at+Scale)|0|
|[Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models](https://doi.org/10.1007/978-3-031-56060-6_19)|Andrew Parry, Maik Fröbe, Sean MacAvaney, Martin Potthast, Matthias Hagen||Modern sequence-to-sequence relevance models like monoT5 can effectively
capture complex textual interactions between queries and documents through
cross-encoding. However, the use of natural language tokens in prompts, such as
Query, Document, and Relevant for monoT5, opens an attack vector for malicious
documents to manipulate their relevance score through prompt injection, e.g.,
by adding target words such as true. Since such possibilities have not yet been
considered in retrieval evaluation, we analyze the impact of query-independent
prompt injection via manually constructed templates and LLM-based rewriting of
documents on several existing relevance models. Our experiments on the TREC
Deep Learning track show that adversarial documents can easily manipulate
different sequence-to-sequence relevance models, while BM25 (as a typical
lexical model) is not affected. Remarkably, the attacks also affect
encoder-only relevance models (which do not rely on natural language prompt
tokens), albeit to a lesser extent.|现代的序列-序列相关模型，如 monoT5，可以通过交叉编码有效地捕获查询和文档之间复杂的文本交互。然而，在提示符中使用自然语言标记，比如 Query、 Document 和 RelationformonoT5，为恶意文档打开了一个攻击向量，通过提示注入操纵它们的相关性得分，例如，通过添加目标词，比如 true。由于在检索评估中还没有考虑到这种可能性，我们通过手工构建模板和基于 LLM 的文档重写来分析与查询无关的提示注入对几种现有相关性模型的影响。我们在 TREC Deep Learning 进行的实验表明，对抗性文档可以轻易地操纵不同的顺序-顺序关联模型，而 BM25(作为一个典型的词汇模型)不受影响。值得注意的是，这些攻击还会影响编码器相关性模型(不依赖于自然语言提示符) ，尽管影响程度较小。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Adversarial+Attacks+on+Sequence-to-Sequence+Relevance+Models)|0|
|[Two-Step SPLADE: Simple, Efficient and Effective Approximation of SPLADE](https://doi.org/10.1007/978-3-031-56060-6_23)|Carlos Lassance, Hervé Déjean, Stéphane Clinchant, Nicola Tonellotto||Learned sparse models such as SPLADE have successfully shown how to
incorporate the benefits of state-of-the-art neural information retrieval
models into the classical inverted index data structure. Despite their
improvements in effectiveness, learned sparse models are not as efficient as
classical sparse model such as BM25. The problem has been investigated and
addressed by recently developed strategies, such as guided traversal query
processing and static pruning, with different degrees of success on in-domain
and out-of-domain datasets. In this work, we propose a new query processing
strategy for SPLADE based on a two-step cascade. The first step uses a pruned
and reweighted version of the SPLADE sparse vectors, and the second step uses
the original SPLADE vectors to re-score a sample of documents retrieved in the
first stage. Our extensive experiments, performed on 30 different in-domain and
out-of-domain datasets, show that our proposed strategy is able to improve mean
and tail response times over the original single-stage SPLADE processing by up
to 30× and 40×, respectively, for in-domain datasets, and by 12x
to 25x, for mean response on out-of-domain datasets, while not incurring in
statistical significant difference in 60% of datasets.|像 SPLADE 这样的稀疏学习模型已经成功地展示了如何将最先进的神经信息检索模型的优点融入到经典的倒排索引数据结构中。尽管学习稀疏模型的有效性有所提高，但其效率不如经典稀疏模型如 BM25。该问题已经通过最近开发的策略得到了研究和解决，如引导遍历查询处理和静态剪枝，在域内和域外数据集上取得了不同程度的成功。本文提出了一种新的基于两步级联的 SPLADE 查询处理策略。第一步使用 SPLADE 稀疏向量的修剪和重新加权版本，第二步使用原始 SPLADE 向量对在第一阶段检索到的文档样本进行重新评分。我们在30个不同的域内和域外数据集上进行的广泛实验表明，我们提出的策略能够将原始单阶段 SPLADE 处理的平均和尾部响应时间分别提高30倍和40倍，对于域内数据集，提高12倍至25倍，对于域外数据集的平均响应，同时在60% 的数据集中不引起统计学显着差异。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Two-Step+SPLADE:+Simple,+Efficient+and+Effective+Approximation+of+SPLADE)|0|
|[Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers](https://doi.org/10.1007/978-3-031-56060-6_26)|Negar Arabzadeh, Amin Bigdeli, Charles L. A. Clarke||Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the generated answers. In the second, we compare generated answers to the top results retrieved by a diverse set of retrieval models, ranging from traditional approaches to advanced methods, allowing us to measure improvements without human judgments. In both cases, we measure the similarity between an embedded representation of the generated answer and an embedded representation of a known, or assumed, relevant passage from the retrieval benchmark.|大型语言模型现在可以直接生成许多实际问题的答案，而无需引用外部资源。遗憾的是，对于评价这些答案的质量和正确性、比较一个模型与另一个模型的表现或比较一个提示与另一个提示的方法，人们的关注相对较少。此外，生成的答案的质量很少直接比较检索的答案的质量。随着模型的发展和提示的修改，我们没有系统的方法来衡量改进而不诉诸昂贵的人类判断。为了解决这个问题，我们采用标准的检索基准来评估由大型语言模型生成的答案。受到用于总结的 BERTScore 度量的启发，我们探索了两种方法。首先，我们以基准相关性判断为基础进行评价。我们通过实验来研究信息检索相关性判断是如何被用来作为评估生成的答案的锚的。在第二个实验中，我们将生成的答案与不同检索模型(从传统方法到高级方法)检索到的最高结果进行比较，使我们能够在没有人为判断的情况下衡量改进情况。在这两种情况下，我们测量生成的答案的嵌入表示和检索基准中已知或假定的相关段落的嵌入表示之间的相似性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+Standard+Retrieval+Benchmarks+to+Evaluate+Generated+Answers)|0|
|[Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control](https://doi.org/10.1007/978-3-031-56060-6_29)|Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke||Learned sparse retrieval (LSR) is a family of neural methods that encode
queries and documents into sparse lexical vectors that can be indexed and
retrieved efficiently with an inverted index. We explore the application of LSR
to the multi-modal domain, with a focus on text-image retrieval. While LSR has
seen success in text retrieval, its application in multimodal retrieval remains
underexplored. Current approaches like LexLIP and STAIR require complex
multi-step training on massive datasets. Our proposed approach efficiently
transforms dense vectors from a frozen dense model into sparse lexical vectors.
We address issues of high dimension co-activation and semantic deviation
through a new training algorithm, using Bernoulli random variables to control
query expansion. Experiments with two dense models (BLIP, ALBEF) and two
datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively
reduces co-activation and semantic deviation. Our best-performing sparsified
model outperforms state-of-the-art text-image LSR models with a shorter
training time and lower GPU memory requirements. Our approach offers an
effective solution for training LSR retrieval models in multimodal settings.
Our code and model checkpoints are available at
github.com/thongnt99/lsr-multimodal|学习稀疏检索(LSR)是一类将查询和文档编码成稀疏词汇向量的神经元方法，可以通过反向索引有效地进行索引和检索。我们探讨了 LSR 在多模态领域的应用，重点研究了文本图像检索。虽然 LSR 在文本检索方面取得了成功，但它在多模态检索中的应用仍然有待探索。目前的方法如 LexLIP 和 STAIR 需要对大量数据集进行复杂的多步训练。我们提出的方法有效地将密集向量从一个冻结的密集模型转换成稀疏的词汇向量。通过一种新的训练算法，利用贝努利随机变量控制查询扩展，解决了高维共激活和语义偏差的问题。对两个密集模型(BLIP，ALBEF)和两个数据集(MSCOCO，Flickr30k)的实验表明，该算法有效地减少了协同激活和语义偏差。我们性能最好的稀疏模型优于最先进的文本图像 LSR 模型，具有更短的训练时间和更低的 GPU 内存需求。该方法为在多模态环境下训练 LSR 检索模型提供了一种有效的解决方案。我们的代码和模型检查点在 github.com/thongnt99/lsr-multimodal 都有|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Learned+Sparse+Retrieval+with+Probabilistic+Expansion+Control)|0|
|[Alleviating Confounding Effects with Contrastive Learning in Recommendation](https://doi.org/10.1007/978-3-031-56060-6_30)|Di You, Kyumin Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Alleviating+Confounding+Effects+with+Contrastive+Learning+in+Recommendation)|0|
|[Align MacridVAE: Multimodal Alignment for Disentangled Recommendations](https://doi.org/10.1007/978-3-031-56027-9_5)|Ignacio Avas, Liesbeth Allein, Katrien Laenen, MarieFrancine Moens||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Align+MacridVAE:+Multimodal+Alignment+for+Disentangled+Recommendations)|0|
|[Learning Action Embeddings for Off-Policy Evaluation](https://doi.org/10.1007/978-3-031-56027-9_7)|Matej Cief, Jacek Golebiowski, Philipp Schmidt, Ziawasch Abedjan, Artur Bekasov||Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to define action embeddings for MIPS. This approach extends MIPS to more applications, and in our experiments improves upon MIPS with pre-defined embeddings, as well as standard baselines, both on synthetic and real-world data. Our method does not make assumptions about the reward model class, and supports using additional action information to further improve the estimates. The proposed approach presents an appealing alternative to DR for combining the low variance of DM with the low bias of IPS.|非策略评估(OPE)方法允许我们通过使用不同策略收集的日志数据来计算策略的预期回报。相对于运行昂贵的在线 A/B 测试，OPE 是一种可行的替代方案: 它可以加快新政策的制定，并降低客户接触次优治疗的风险。然而，当操作的数量很大，或者某些操作被日志策略低估时，基于逆倾向评分(IPS)的现有估计量可能会有很高甚至无限的方差。Saito 和 Joachims (arXiv: 2202.06317 v2[ cs.LG ])提出使用动作嵌入的边缘化 IPS (MIPS) ，这减少了大动作空间中 IPS 的方差。MIPS 假设好的操作嵌入可以由从业人员定义，这在许多实际应用程序中是很难做到的。在这项工作中，我们探讨了从日志数据学习动作嵌入。特别地，我们使用训练过的奖励模型的中间输出来定义 MIPS 的行动嵌入。这种方法将 MIPS 扩展到更多的应用程序，并且在我们的实验中通过预定义的嵌入以及在合成和真实世界数据上的标准基线改进了 MIPS。我们的方法不对奖励模型类做假设，并支持使用额外的行动信息，以进一步改善估计。提出的方法提出了一个吸引人的替代 DR 相结合的 DM 的低方差和 IPS 的低偏差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Action+Embeddings+for+Off-Policy+Evaluation)|0|
|[Simulated Task Oriented Dialogues for Developing Versatile Conversational Agents](https://doi.org/10.1007/978-3-031-56027-9_10)|Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simulated+Task+Oriented+Dialogues+for+Developing+Versatile+Conversational+Agents)|0|
|[Hypergraphs with Attention on Reviews for Explainable Recommendation](https://doi.org/10.1007/978-3-031-56027-9_14)|Theis E. Jendal, TrungHoang Le, Hady W. Lauw, Matteo Lissandrini, Peter Dolog, Katja Hose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypergraphs+with+Attention+on+Reviews+for+Explainable+Recommendation)|0|
|[Investigating the Usage of Formulae in Mathematical Answer Retrieval](https://doi.org/10.1007/978-3-031-56027-9_15)|Anja Reusch, Julius Gonsior, Claudio Hartmann, Wolfgang Lehner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+the+Usage+of+Formulae+in+Mathematical+Answer+Retrieval)|0|
|[Empowering Legal Citation Recommendation via Efficient Instruction-Tuning of Pre-trained Language Models](https://doi.org/10.1007/978-3-031-56027-9_19)|Jie Wang, Kanha Bansal, Ioannis Arapakis, Xuri Ge, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+Legal+Citation+Recommendation+via+Efficient+Instruction-Tuning+of+Pre-trained+Language+Models)|0|
|[Fine-Tuning CLIP via Explainability Map Propagation for Boosting Image and Video Retrieval](https://doi.org/10.1007/978-3-031-56027-9_22)|Yoav Shalev, Lior Wolf||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-Tuning+CLIP+via+Explainability+Map+Propagation+for+Boosting+Image+and+Video+Retrieval)|0|
|[Cross-Modal Retrieval for Knowledge-Based Visual Question Answering](https://doi.org/10.1007/978-3-031-56027-9_26)|Paul Lerner, Olivier Ferret, Camille Guinaudeau||Knowledge-based Visual Question Answering about Named Entities is a
challenging task that requires retrieving information from a multimodal
Knowledge Base. Named entities have diverse visual representations and are
therefore difficult to recognize. We argue that cross-modal retrieval may help
bridge the semantic gap between an entity and its depictions, and is foremost
complementary with mono-modal retrieval. We provide empirical evidence through
experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE,
InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different
strategies to fine-tune such a model: mono-modal, cross-modal, or joint
training. Our method, which combines mono-and cross-modal retrieval, is
competitive with billion-parameter models on the three datasets, while being
conceptually simpler and computationally cheaper.|基于知识的命名实体可视化问答是一项具有挑战性的任务，需要从多模态知识库中检索信息。命名实体具有不同的可视化表示，因此难以识别。我们认为，跨模态检索有助于弥合实体与其描述之间的语义鸿沟，并与单模态检索相辅相成。我们提供经验证明通过实验与多模态双编码器，即 CLIP，在最近的 ViQuAE，资讯搜寻和百科全书-VQA 数据集。此外，我们还研究了三种不同的策略来微调这种模型: 单模态、跨模态或联合训练。我们的方法结合了单模态检索和跨模态检索，与三个数据集上的十亿参数模型相比具有竞争力，同时在概念上更简单，计算成本更低。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Modal+Retrieval+for+Knowledge-Based+Visual+Question+Answering)|0|
|[Learning to Jointly Transform and Rank Difficult Queries](https://doi.org/10.1007/978-3-031-56066-8_5)|Amin Bigdeli, Negar Arabzadeh, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Jointly+Transform+and+Rank+Difficult+Queries)|0|
|[Instant Answering in E-Commerce Buyer-Seller Messaging Using Message-to-Question Reformulation](https://doi.org/10.1007/978-3-031-56066-8_7)|Besnik Fetahu, Tejas Mehta, Qun Song, Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi||E-commerce customers frequently seek detailed product information for
purchase decisions, commonly contacting sellers directly with extended queries.
This manual response requirement imposes additional costs and disrupts buyer's
shopping experience with response time fluctuations ranging from hours to days.
We seek to automate buyer inquiries to sellers in a leading e-commerce store
using a domain-specific federated Question Answering (QA) system. The main
challenge is adapting current QA systems, designed for single questions, to
address detailed customer queries. We address this with a low-latency,
sequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates
buyer messages into succinct questions by identifying and extracting the most
salient information from a message. Evaluation against baselines shows that M2Q
yields relative increases of 757
answering rate from the federated QA system. Live deployment shows that
automatic answering saves sellers from manually responding to millions of
messages per year, and also accelerates customer purchase decisions by
eliminating the need for buyers to wait for a reply|电子商务客户经常为购买决策寻找详细的产品信息，通常直接与销售商进行扩展查询。这种手动响应要求增加了额外的成本，并且由于响应时间从几小时到几天的波动而扰乱了买家的购物体验。我们寻求在一个领先的电子商务商店使用领域特定的联邦问题回答(QA)系统自动化的买方询问卖方。主要的挑战是适应当前的 QA 系统，为单个问题设计，以解决详细的客户查询。我们使用低延迟、序列到序列的方法 MESSAGE-TO-QUESTION (M2Q)来解决这个问题。它通过从消息中识别和提取最突出的信息，将买方消息重新表述为简洁的问题。对基线的评估表明，M2Q 在联邦 QA 系统中的应答率相对提高了757。实时部署显示，自动回复可以节省卖家每年手动回复数百万条消息的时间，还可以消除买家等待回复的需要，从而加快客户的购买决策|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Instant+Answering+in+E-Commerce+Buyer-Seller+Messaging+Using+Message-to-Question+Reformulation)|0|
|[Towards Automated End-to-End Health Misinformation Free Search with a Large Language Model](https://doi.org/10.1007/978-3-031-56066-8_9)|Ronak Pradeep, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Automated+End-to-End+Health+Misinformation+Free+Search+with+a+Large+Language+Model)|0|
|[Reproducibility Analysis and Enhancements for Multi-aspect Dense Retriever with Aspect Learning](https://doi.org/10.1007/978-3-031-56066-8_17)|Keping Bi, Xiaojie Sun, Jiafeng Guo, Xueqi Cheng||Multi-aspect dense retrieval aims to incorporate aspect information (e.g.,
brand and category) into dual encoders to facilitate relevance matching. As an
early and representative multi-aspect dense retriever, MADRAL learns several
extra aspect embeddings and fuses the explicit aspects with an implicit aspect
"OTHER" for final representation. MADRAL was evaluated on proprietary data and
its code was not released, making it challenging to validate its effectiveness
on other datasets. We failed to reproduce its effectiveness on the public
MA-Amazon data, motivating us to probe the reasons and re-examine its
components. We propose several component alternatives for comparisons,
including replacing "OTHER" with "CLS" and representing aspects with the first
several content tokens. Through extensive experiments, we confirm that learning
"OTHER" from scratch in aspect fusion is harmful. In contrast, our proposed
variants can greatly enhance the retrieval performance. Our research not only
sheds light on the limitations of MADRAL but also provides valuable insights
for future studies on more powerful multi-aspect dense retrieval models. Code
will be released at:
https://github.com/sunxiaojie99/Reproducibility-for-MADRAL.|多方面密集检索旨在将方面信息(例如，品牌和类别)合并到双编码器中，以促进相关性匹配。作为一个早期的、有代表性的多方面密集检索器，MADRAL 学习了一些额外的方面嵌入，并将显式方面与隐式方面“ OTHER”融合以得到最终的表示。MADRAL 是根据专有数据进行评估的，其代码没有发布，这使得在其他数据集上验证其有效性具有挑战性。我们未能在公开的 MA-Amazon 数据上再现其有效性，这促使我们探究其原因并重新检查其组成部分。我们提出了几种可供比较的组件替代方案，包括用“ CLS”替换“ OTHER”，以及用前几个内容标记表示方面。通过大量的实验，我们证实了在方面融合中从头学习“其他”是有害的。相比之下，我们提出的变量可以大大提高检索性能。我们的研究不仅揭示了 MADRAL 的局限性，而且为未来更强大的多方面密集检索模型的研究提供了有价值的见解。密码将在下列 https://github.com/sunxiaojie99/reproducibility-for-madral 公布:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reproducibility+Analysis+and+Enhancements+for+Multi-aspect+Dense+Retriever+with+Aspect+Learning)|0|
|[An Empirical Analysis of Intervention Strategies' Effectiveness for Countering Misinformation Amplification by Recommendation Algorithms](https://doi.org/10.1007/978-3-031-56066-8_23)|Royal Pathak, Francesca Spezzano||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Analysis+of+Intervention+Strategies'+Effectiveness+for+Countering+Misinformation+Amplification+by+Recommendation+Algorithms)|0|
|[Not Just Algorithms: Strategically Addressing Consumer Impacts in Information Retrieval](https://doi.org/10.1007/978-3-031-56066-8_25)|Michael D. Ekstrand, Lex Beattie, Maria Soledad Pera, Henriette Cramer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+Just+Algorithms:+Strategically+Addressing+Consumer+Impacts+in+Information+Retrieval)|0|
|[A Study of Pre-processing Fairness Intervention Methods for Ranking People](https://doi.org/10.1007/978-3-031-56066-8_26)|Clara Rus, Andrew Yates, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+of+Pre-processing+Fairness+Intervention+Methods+for+Ranking+People)|0|
|[Evaluating the Explainability of Neural Rankers](https://doi.org/10.1007/978-3-031-56066-8_28)|Saran Pandian, Debasis Ganguly, Sean MacAvaney||Information retrieval models have witnessed a paradigm shift from
unsupervised statistical approaches to feature-based supervised approaches to
completely data-driven ones that make use of the pre-training of large language
models. While the increasing complexity of the search models have been able to
demonstrate improvements in effectiveness (measured in terms of relevance of
top-retrieved results), a question worthy of a thorough inspection is - "how
explainable are these models?", which is what this paper aims to evaluate. In
particular, we propose a common evaluation platform to systematically evaluate
the explainability of any ranking model (the explanation algorithm being
identical for all the models that are to be evaluated). In our proposed
framework, each model, in addition to returning a ranked list of documents,
also requires to return a list of explanation units or rationales for each
document. This meta-information from each document is then used to measure how
locally consistent these rationales are as an intrinsic measure of
interpretability - one that does not require manual relevance assessments.
Additionally, as an extrinsic measure, we compute how relevant these rationales
are by leveraging sub-document level relevance assessments. Our findings show a
number of interesting observations, such as sentence-level rationales are more
consistent, an increase in complexity mostly leads to less consistent
explanations, and that interpretability measures offer a complementary
dimension of evaluation of IR systems because consistency is not
well-correlated with nDCG at top ranks.|信息检索模型已经见证了从无监督统计方法到基于特征的监督方法到完全数据驱动方法的范式转变，这种方法利用了大型语言模型的预训练。虽然搜索模型日益增加的复杂性已经能够证明有效性的改善(根据检索结果的相关性衡量) ，但是一个值得彻底检查的问题是——“这些模型如何解释?”这就是本文的目的。特别是，我们提出了一个通用的评估平台，系统地评估任何排名模型的可解释性(解释算法对于所有待评估的模型都是相同的)。在我们提出的框架中，每个模型除了返回排序的文档列表之外，还需要返回每个文档的解释单元或基本原理的列表。然后，利用每份文件中的元信息来衡量这些理由在当地的一致程度，作为衡量可解释性的内在尺度，而不需要人工进行相关性评估。此外，作为一个外在的测量，我们通过利用子文档级别的相关性评估来计算这些基本原理的相关性。我们的研究结果显示，许多有趣的观察结果，例如句子水平的基本原理更加一致，复杂性的增加主要导致不一致的解释，并且可解释性测量提供了 IR 系统评估的补充维度，因为一致性与顶级的 nDCG 不相关。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+the+Explainability+of+Neural+Rankers)|0|
|[Knowledge Graph Cross-View Contrastive Learning for Recommendation](https://doi.org/10.1007/978-3-031-56063-7_1)|Zeyuan Meng, Iadh Ounis, Craig Macdonald, Zixuan Yi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Cross-View+Contrastive+Learning+for+Recommendation)|0|
|[Recommendation Fairness in eParticipation: Listening to Minority, Vulnerable and NIMBY Citizens](https://doi.org/10.1007/978-3-031-56066-8_31)|Marina AlonsoCortés, Iván Cantador, Alejandro Bellogín||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recommendation+Fairness+in+eParticipation:+Listening+to+Minority,+Vulnerable+and+NIMBY+Citizens)|0|
|[Responsible Opinion Formation on Debated Topics in Web Search](https://doi.org/10.1007/978-3-031-56066-8_32)|Alisa Rieger, Tim Draws, Nicolas Mattis, David Maxwell, David Elsweiler, Ujwal Gadiraju, Dana McKay, Alessandro Bozzon, Maria Soledad Pera||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Responsible+Opinion+Formation+on+Debated+Topics+in+Web+Search)|0|
|[Is Google Getting Worse? A Longitudinal Investigation of SEO Spam in Search Engines](https://doi.org/10.1007/978-3-031-56063-7_4)|Janek Bevendorff, Matti Wiegmann, Martin Potthast, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Google+Getting+Worse?+A+Longitudinal+Investigation+of+SEO+Spam+in+Search+Engines)|0|
|[Robustness in Fairness Against Edge-Level Perturbations in GNN-Based Recommendation](https://doi.org/10.1007/978-3-031-56063-7_3)|Ludovico Boratto, Francesco Fabbri, Gianni Fenu, Mirko Marras, Giacomo Medda||Efforts in the recommendation community are shifting from the sole emphasis
on utility to considering beyond-utility factors, such as fairness and
robustness. Robustness of recommendation models is typically linked to their
ability to maintain the original utility when subjected to attacks. Limited
research has explored the robustness of a recommendation model in terms of
fairness, e.g., the parity in performance across groups, under attack
scenarios. In this paper, we aim to assess the robustness of graph-based
recommender systems concerning fairness, when exposed to attacks based on
edge-level perturbations. To this end, we considered four different fairness
operationalizations, including both consumer and provider perspectives.
Experiments on three datasets shed light on the impact of perturbations on the
targeted fairness notion, uncovering key shortcomings in existing evaluation
protocols for robustness. As an example, we observed perturbations affect
consumer fairness on a higher extent than provider fairness, with alarming
unfairness for the former. Source code:
https://github.com/jackmedda/CPFairRobust|推荐社区的工作正在从单纯强调效用转向考虑效用以外的因素，如公平性和稳健性。推荐模型的健壮性通常与它们在受到攻击时维护原始实用程序的能力有关。有限的研究已经探索了推荐模型在公平性方面的健壮性，例如，在攻击场景下组间的性能均等。本文旨在评估基于图形的推荐系统在受到基于边界层扰动的攻击时对公平性的鲁棒性。为此，我们考虑了四种不同的公平可操作性，包括消费者和提供者视角。在三个数据集上的实验揭示了扰动对目标公平性概念的影响，揭示了现有鲁棒性评估协议的关键缺陷。作为一个例子，我们观察到扰动对消费者公平性的影响程度高于提供者公平性，前者的不公平性令人担忧。源代码:  https://github.com/jackmedda/cpfairrobust|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robustness+in+Fairness+Against+Edge-Level+Perturbations+in+GNN-Based+Recommendation)|0|
|[Shallow Cross-Encoders for Low-Latency Retrieval](https://doi.org/10.1007/978-3-031-56063-7_10)|Aleksandr V. Petrov, Sean MacAvaney, Craig Macdonald||Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in
text retrieval. However, Cross-Encoders based on large transformer models (such
as BERT or T5) are computationally expensive and allow for scoring only a small
number of documents within a reasonably small latency window. However, keeping
search latencies low is important for user satisfaction and energy usage. In
this paper, we show that weaker shallow transformer models (i.e., transformers
with a limited number of layers) actually perform better than full-scale models
when constrained to these practical low-latency settings since they can
estimate the relevance of more documents in the same time budget. We further
show that shallow transformers may benefit from the generalized Binary
Cross-Entropy (gBCE) training scheme, which has recently demonstrated success
for recommendation tasks. Our experiments with TREC Deep Learning passage
ranking query sets demonstrate significant improvements in shallow and
full-scale models in low-latency scenarios. For example, when the latency limit
is 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT
model) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while
TinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches
NDCG@10 of 0.652, a +51
Cross-Encoders are effective even when used without a GPU (e.g., with CPU
inference, NDCG@10 decreases only by 3
latency), which makes Cross-Encoders practical to run even without specialized
hardware acceleration.|基于变压器的交叉编码器在文本检索中取得了最先进的效果。然而，基于大型转换器模型(如 BERT 或 T5)的交叉编码器在计算上是昂贵的，并且只允许在一个相当小的延迟窗口内对少量文档进行评分。然而，保持较低的搜索延迟对于用户满意度和能量使用非常重要。在本文中，我们表明，较弱的浅层变压器模型(即，有限层数的变压器)实际上比全尺寸模型表现更好，当约束到这些实际的低延迟设置，因为他们可以估计相关性更多的文件在同一时间预算。我们进一步表明，浅变压器可以受益于广义二进制交叉熵(gBCE)训练方案，最近证明了推荐任务的成功。我们对 TREC 深度学习段落排序查询集的实验表明，在低延迟场景中，浅层和全尺度模型有了显著的改进。例如，当每个查询的延迟限制为25毫秒时，MonoBERT-Large (一种基于全尺寸 BERT 模型的交叉编码器)在 TREC dL 2019上只能达到0.431的 NDCG@10，而 TinyBERT-gBCE (一种基于 TinyBERT 的交叉编码器，经 gBCE 训练)达到0.652的 NDCG@10，a + 51交叉编码器即使在没有图形处理器的情况下也是有效的(例如，根据 CPU 推断，NDCG@10只减少了3个延迟) ，这使得交叉编码器即使没有专门的硬件加速也能实际运行。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shallow+Cross-Encoders+for+Low-Latency+Retrieval)|0|
|[Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies](https://doi.org/10.1007/978-3-031-56063-7_12)|Puxuan Yu, Antonio Mallia, Matthias Petri||We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems.|我们探索利用特定于语料库的词汇来提高学习的稀疏检索系统的效率和有效性。我们发现，在目标语料库上预先训练基础的 BERT 模型，特别是针对文档扩展过程中包含的不同词汇量，可以提高检索质量达12% ，而在某些情况下可以减少50% 的延迟。我们的实验表明，采用特定语料库词汇和增加词汇量减少了平均发布列表长度，从而减少了延迟。消融研究显示了自定义词汇表、文档扩展技术和稀疏模型的稀疏化目标之间有趣的交互作用。成效和效率的提高都转移到不同的检索方法，如 uniCOIL 和 SPLADE，并提供了一种简单而有效的方法，为学习的稀疏检索系统提供新的效率效益权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improved+Learned+Sparse+Retrieval+with+Corpus-Specific+Vocabularies)|0|
|[An Adaptive Framework of Geographical Group-Specific Network on O2O Recommendation](https://doi.org/10.1007/978-3-031-56063-7_19)|Luo Ji, Jiayu Mao, Hailong Shi, Qian Li, Yunfei Chu, Hongxia Yang||Online to offline recommendation strongly correlates with the user and
service's spatiotemporal information, therefore calling for a higher degree of
model personalization. The traditional methodology is based on a uniform model
structure trained by collected centralized data, which is unlikely to capture
all user patterns over different geographical areas or time periods. To tackle
this challenge, we propose a geographical group-specific modeling method called
GeoGrouse, which simultaneously studies the common knowledge as well as
group-specific knowledge of user preferences. An automatic grouping paradigm is
employed and verified based on users' geographical grouping indicators. Offline
and online experiments are conducted to verify the effectiveness of our
approach, and substantial business improvement is achieved.|Online To Offline线上到线下推荐与用户和服务的时空信息密切相关，因此需要更高程度的模型个性化。传统的方法是基于由收集的中央数据训练的统一模型结构，这种结构不太可能捕获不同地理区域或不同时期的所有用户模式。为了应对这一挑战，我们提出了一种名为 GeoGrouse 的地理组特定建模方法，该方法同时研究用户偏好的常识和组特定知识。基于用户的地理分组指标，采用自动分组范式进行验证。通过离线和在线实验验证了该方法的有效性，并取得了实质性的业务改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Adaptive+Framework+of+Geographical+Group-Specific+Network+on+O2O+Recommendation)|0|
|[GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation](https://doi.org/10.1007/978-3-031-56063-7_24)|Kaustubh D. Dhole, Eugene Agichtein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GenQREnsemble:+Zero-Shot+LLM+Ensemble+Prompting+for+Generative+Query+Reformulation)|0|
|[Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning](https://doi.org/10.1007/978-3-031-56063-7_21)|Georgios Sidiropoulos, Evangelos Kanoulas||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Robustness+of+Dense+Retrievers+Against+Typos+via+Multi-Positive+Contrastive+Learning)|0|
|[Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations](https://doi.org/10.1007/978-3-031-56063-7_25)|Weronika Lajewska, Krisztian Balog||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Reliable+and+Factual+Response+Generation:+Detecting+Unanswerable+Questions+in+Information-Seeking+Conversations)|0|
|[On the Influence of Reading Sequences on Knowledge Gain During Web Search](https://doi.org/10.1007/978-3-031-56063-7_28)|Wolfgang Gritz, Anett Hoppe, Ralph Ewerth||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Influence+of+Reading+Sequences+on+Knowledge+Gain+During+Web+Search)|0|
|[SPARe: Supercharged Lexical Retrievers on GPU with Sparse Kernels](https://doi.org/10.1007/978-3-031-56063-7_33)|Tiago Almeida, Sérgio Matos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPARe:+Supercharged+Lexical+Retrievers+on+GPU+with+Sparse+Kernels)|0|
|[Beneath the [MASK]: An Analysis of Structural Query Tokens in ColBERT](https://doi.org/10.1007/978-3-031-56063-7_35)|Ben Giacalone, Greg Paiement, Quinn Tucker, Richard Zanibbi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beneath+the+[MASK]:+An+Analysis+of+Structural+Query+Tokens+in+ColBERT)|0|
|[A Cost-Sensitive Meta-learning Strategy for Fair Provider Exposure in Recommendation](https://doi.org/10.1007/978-3-031-56063-7_36)|Ludovico Boratto, Giulia Cerniglia, Mirko Marras, Alessandra Perniciano, Barbara Pes||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Cost-Sensitive+Meta-learning+Strategy+for+Fair+Provider+Exposure+in+Recommendation)|0|
|[Multiple Testing for IR and Recommendation System Experiments](https://doi.org/10.1007/978-3-031-56063-7_37)|Ngozi Ihemelandu, Michael D. Ekstrand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multiple+Testing+for+IR+and+Recommendation+System+Experiments)|0|
|[An In-Depth Comparison of Neural and Probabilistic Tree Models for Learning-to-rank](https://doi.org/10.1007/978-3-031-56063-7_39)|Haonan Tan, Kaiyu Yang, Haitao Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+In-Depth+Comparison+of+Neural+and+Probabilistic+Tree+Models+for+Learning-to-rank)|0|
|[GenRec: Large Language Model for Generative Recommendation](https://doi.org/10.1007/978-3-031-56063-7_42)|Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, Yongfeng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GenRec:+Large+Language+Model+for+Generative+Recommendation)|0|
|[News Gathering: Leveraging Transformers to Rank News](https://doi.org/10.1007/978-3-031-56063-7_41)|Carlos Muñoz, María José Apolo, Maximiliano Ojeda, Hans Lobel, Marcelo Mendoza||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=News+Gathering:+Leveraging+Transformers+to+Rank+News)|0|
|[Answer Retrieval in Legal Community Question Answering](https://doi.org/10.1007/978-3-031-56063-7_40)|Arian Askari, Zihui Yang, Zhaochun Ren, Suzan Verberne||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Answer+Retrieval+in+Legal+Community+Question+Answering)|0|
|[Towards Optimizing Ranking in Grid-Layout for Provider-Side Fairness](https://doi.org/10.1007/978-3-031-56069-9_7)|Amifa Raj, Michael D. Ekstrand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Optimizing+Ranking+in+Grid-Layout+for+Provider-Side+Fairness)|0|
|[A Conversational Robot for Children's Access to a Cultural Heritage Multimedia Archive](https://doi.org/10.1007/978-3-031-56069-9_11)|Thomas Beelen, Roeland Ordelman, Khiet P. Truong, Vanessa Evers, Theo Huibers||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Conversational+Robot+for+Children's+Access+to+a+Cultural+Heritage+Multimedia+Archive)|0|
|[MathMex: Search Engine for Math Definitions](https://doi.org/10.1007/978-3-031-56069-9_17)|Shea Durgin, James Gore, Behrooz Mansouri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MathMex:+Search+Engine+for+Math+Definitions)|0|
|[XSearchKG: A Platform for Explainable Keyword Search over Knowledge Graphs](https://doi.org/10.1007/978-3-031-56069-9_18)|Leila Feddoul, Martin Birke, Sirko Schindler||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XSearchKG:+A+Platform+for+Explainable+Keyword+Search+over+Knowledge+Graphs)|0|
|[Result Assessment Tool: Software to Support Studies Based on Data from Search Engines](https://doi.org/10.1007/978-3-031-56069-9_19)|Sebastian Sünkler, Nurce Yagci, Sebastian Schultheiß, Sonja von Mach, Dirk Lewandowski||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Result+Assessment+Tool:+Software+to+Support+Studies+Based+on+Data+from+Search+Engines)|0|
|[Translating Justice: A Cross-Lingual Information Retrieval System for Maltese Case Law Documents](https://doi.org/10.1007/978-3-031-56069-9_24)|Joel Azzopardi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Translating+Justice:+A+Cross-Lingual+Information+Retrieval+System+for+Maltese+Case+Law+Documents)|0|
|[Displaying Evolving Events Via Hierarchical Information Threads for Sensitivity Review](https://doi.org/10.1007/978-3-031-56069-9_29)|Hitarth Narvala, Graham McDonald, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Displaying+Evolving+Events+Via+Hierarchical+Information+Threads+for+Sensitivity+Review)|0|
|[Analyzing Mathematical Content for Plagiarism and Recommendations](https://doi.org/10.1007/978-3-031-56069-9_42)|Ankit Satpute||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Mathematical+Content+for+Plagiarism+and+Recommendations)|0|
|[Explainable Recommender Systems with Knowledge Graphs and Language Models](https://doi.org/10.1007/978-3-031-56069-9_46)|Giacomo Balloccu, Ludovico Boratto, Gianni Fenu, Francesca Maridina Malloci, Mirko Marras||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Recommender+Systems+with+Knowledge+Graphs+and+Language+Models)|0|
|[Recent Advances in Generative Information Retrieval](https://doi.org/10.1007/978-3-031-56069-9_48)|Yubao Tang, Ruqing Zhang, Zhaochun Ren, Jiafeng Guo, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Generative+Information+Retrieval)|0|
|[Affective Computing for Social Good Applications: Current Advances, Gaps and Opportunities in Conversational Setting](https://doi.org/10.1007/978-3-031-56069-9_50)|Priyanshu Priya, Mauajama Firdaus, Gopendra Vikram Singh, Asif Ekbal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Affective+Computing+for+Social+Good+Applications:+Current+Advances,+Gaps+and+Opportunities+in+Conversational+Setting)|0|
|[Query Performance Prediction: From Fundamentals to Advanced Techniques](https://doi.org/10.1007/978-3-031-56069-9_51)|Negar Arabzadeh, Chuan Meng, Mohammad Aliannejadi, Ebrahim Bagheri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Performance+Prediction:+From+Fundamentals+to+Advanced+Techniques)|0|
|[Fairness Through Domain Awareness: Mitigating Popularity Bias for Music Discovery](https://doi.org/10.1007/978-3-031-56066-8_27)|Rebecca Salganik, Fernando Diaz, Golnoosh Farnadi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+Through+Domain+Awareness:+Mitigating+Popularity+Bias+for+Music+Discovery)|0|
|[Countering Mainstream Bias via End-to-End Adaptive Local Learning](https://doi.org/10.1007/978-3-031-56069-9_6)|Jinhao Pan, Ziwei Zhu, Jianling Wang, Allen Lin, James Caverlee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Countering+Mainstream+Bias+via+End-to-End+Adaptive+Local+Learning)|0|
|[BioASQ at CLEF2024: The Twelfth Edition of the Large-Scale Biomedical Semantic Indexing and Question Answering Challenge](https://doi.org/10.1007/978-3-031-56069-9_67)|Anastasios Nentidis, Anastasia Krithara, Georgios Paliouras, Martin Krallinger, Luis Gascó Sánchez, Salvador LimaLópez, Eulàlia Farré, Natalia V. Loukachevitch, Vera Davydova, Elena Tutubalina||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BioASQ+at+CLEF2024:+The+Twelfth+Edition+of+the+Large-Scale+Biomedical+Semantic+Indexing+and+Question+Answering+Challenge)|0|
|[ProMap: Product Mapping Datasets](https://doi.org/10.1007/978-3-031-56060-6_11)|Katerina Macková, Martin Pilát||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProMap:+Product+Mapping+Datasets)|0|
|[Eliminating Contextual Bias in Aspect-Based Sentiment Analysis](https://doi.org/10.1007/978-3-031-56027-9_6)|Ruize An, Chen Zhang, Dawei Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Eliminating+Contextual+Bias+in+Aspect-Based+Sentiment+Analysis)|0|
|[A Streaming Approach to Neural Team Formation Training](https://doi.org/10.1007/978-3-031-56027-9_20)|Hossein Fani, Reza Barzegar, Arman Dashti, Mahdis Saeedi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Streaming+Approach+to+Neural+Team+Formation+Training)|0|
|[A Second Look on BASS - Boosting Abstractive Summarization with Unified Semantic Graphs - A Replication Study](https://doi.org/10.1007/978-3-031-56066-8_11)|Osman Alperen Koras, Jörg Schlötterer, Christin Seifert||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Second+Look+on+BASS+-+Boosting+Abstractive+Summarization+with+Unified+Semantic+Graphs+-+A+Replication+Study)|0|
|[Absolute Variation Distance: An Inversion Attack Evaluation Metric for Federated Learning](https://doi.org/10.1007/978-3-031-56066-8_20)|Georgios Papadopoulos, Yash Satsangi, Shaltiel Eloul, Marco Pistoia||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Absolute+Variation+Distance:+An+Inversion+Attack+Evaluation+Metric+for+Federated+Learning)|0|
|[Experiments in News Bias Detection with Pre-trained Neural Transformers](https://doi.org/10.1007/978-3-031-56066-8_22)|Tim Menzner, Jochen L. Leidner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Experiments+in+News+Bias+Detection+with+Pre-trained+Neural+Transformers)|0|
|[A Transformer-Based Object-Centric Approach for Date Estimation of Historical Photographs](https://doi.org/10.1007/978-3-031-56063-7_9)|Francesc Net, Núria Hernández, Adrià Molina, Lluís Gómez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Transformer-Based+Object-Centric+Approach+for+Date+Estimation+of+Historical+Photographs)|0|
|[Bias Detection and Mitigation in Textual Data: A Study on Fake News and Hate Speech Detection](https://doi.org/10.1007/978-3-031-56063-7_29)|Apostolos Kasampalis, Despoina Chatzakou, Theodora Tsikrika, Stefanos Vrochidis, Ioannis Kompatsiaris||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bias+Detection+and+Mitigation+in+Textual+Data:+A+Study+on+Fake+News+and+Hate+Speech+Detection)|0|
|[DQNC2S: DQN-Based Cross-Stream Crisis Event Summarizer](https://doi.org/10.1007/978-3-031-56063-7_34)|Daniele Rege Cambrin, Luca Cagliero, Paolo Garza||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DQNC2S:+DQN-Based+Cross-Stream+Crisis+Event+Summarizer)|0|
|[QuantPlorer: Exploration of Quantities in Text](https://doi.org/10.1007/978-3-031-56069-9_13)|Satya Almasian, Alexander Kosnac, Michael Gertz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QuantPlorer:+Exploration+of+Quantities+in+Text)|0|
|[ARElight: Context Sampling of Large Texts for Deep Learning Relation Extraction](https://doi.org/10.1007/978-3-031-56069-9_23)|Nicolay Rusnachenko, Huizhi Liang, Maksim Kalameyets, Lei Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ARElight:+Context+Sampling+of+Large+Texts+for+Deep+Learning+Relation+Extraction)|0|
|[Variance Reduction in Ratio Metrics for Efficient Online Experiments](https://doi.org/10.1007/978-3-031-56069-9_34)|Shubham Baweja, Neeti Pokharna, Aleksei Ustimenko, Olivier Jeunen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Variance+Reduction+in+Ratio+Metrics+for+Efficient+Online+Experiments)|0|
|[CLEF 2024 SimpleText Track - Improving Access to Scientific Texts for Everyone](https://doi.org/10.1007/978-3-031-56072-9_4)|Liana Ermakova, Eric SanJuan, Stéphane Huet, Hosein Azarbonyad, Giorgio Maria Di Nunzio, Federica Vezzani, Jennifer D'Souza, Salomon Kabongo, Hamed Babaei Giglou, Yue Zhang, Sören Auer, Jaap Kamps||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLEF+2024+SimpleText+Track+-+Improving+Access+to+Scientific+Texts+for+Everyone)|0|
|[LifeCLEF 2024 Teaser: Challenges on Species Distribution Prediction and Identification](https://doi.org/10.1007/978-3-031-56072-9_3)|Alexis Joly, Lukás Picek, Stefan Kahl, Hervé Goëau, Vincent Espitalier, Christophe Botella, Benjamin Deneu, Diego Marcos, Joaquim Estopinan, César Leblanc, Théo Larcher, Milan Sulc, Marek Hrúz, Maximilien Servajean, Jirí Matas, Hervé Glotin, Robert Planqué, WillemPier Vellinga, Holger Klinck, Tom Denton, Andrew M. Durso, Ivan Eggel, Pierre Bonnet, Henning Müller||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LifeCLEF+2024+Teaser:+Challenges+on+Species+Distribution+Prediction+and+Identification)|0|
|[The CLEF 2024 Monster Track: One Lab to Rule Them All](https://doi.org/10.1007/978-3-031-56072-9_2)|Nicola Ferro, Julio Gonzalo, Jussi Karlgren, Henning Müller||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+CLEF+2024+Monster+Track:+One+Lab+to+Rule+Them+All)|0|
|[CLEF 2024 JOKER Lab: Automatic Humour Analysis](https://doi.org/10.1007/978-3-031-56072-9_5)|Liana Ermakova, AnneGwenn Bosser, Tristan Miller, Tremaine Thomas, Victor Manuel PalmaPreciado, Grigori Sidorov, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLEF+2024+JOKER+Lab:+Automatic+Humour+Analysis)|0|
|[iDPP@CLEF 2024: The Intelligent Disease Progression Prediction Challenge](https://doi.org/10.1007/978-3-031-56072-9_7)|Helena Aidos, Roberto Bergamaschi, Paola Cavalla, Adriano Chiò, Arianna Dagliati, Barbara Di Camillo, Mamede de Carvalho, Nicola Ferro, Piero Fariselli, Jose Manuel García Dominguez, Sara C. Madeira, Eleonora Tavazzi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=iDPP@CLEF+2024:+The+Intelligent+Disease+Progression+Prediction+Challenge)|0|
|[LongEval: Longitudinal Evaluation of Model Performance at CLEF 2024](https://doi.org/10.1007/978-3-031-56072-9_8)|Rabab Alkhalifa, Hsuvas Borkakoty, Romain Deveaud, Alaa ElEbshihy, Luis Espinosa Anke, Tobias Fink, Gabriela González Sáez, Petra Galuscáková, Lorraine Goeuriot, David Iommi, Maria Liakata, Harish Tayyar Madabushi, Pablo MedinaAlias, Philippe Mulhem, Florina Piroi, Martin Popel, Christophe Servan, Arkaitz Zubiaga||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LongEval:+Longitudinal+Evaluation+of+Model+Performance+at+CLEF+2024)|0|
|[CrisisKAN: Knowledge-Infused and Explainable Multimodal Attention Network for Crisis Event Classification](https://doi.org/10.1007/978-3-031-56060-6_2)|Shubham Gupta, Nandini Saini, Suman Kundu, Debasis Das||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CrisisKAN:+Knowledge-Infused+and+Explainable+Multimodal+Attention+Network+for+Crisis+Event+Classification)|0|
|[Probing Pretrained Language Models with Hierarchy Properties](https://doi.org/10.1007/978-3-031-56060-6_9)|Jesús LovónMelgarejo, José G. Moreno, Romaric Besançon, Olivier Ferret, Lynda Tamine||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Probing+Pretrained+Language+Models+with+Hierarchy+Properties)|0|
|[HyperPIE: Hyperparameter Information Extraction from Scientific Publications](https://doi.org/10.1007/978-3-031-56060-6_17)|Tarek Saier, Mayumi Ohta, Takuto Asakura, Michael Färber||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HyperPIE:+Hyperparameter+Information+Extraction+from+Scientific+Publications)|0|
|[An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue Assistant](https://doi.org/10.1007/978-3-031-56060-6_21)|Mohit Tomar, Abhisek Tiwari, Tulika Saha, Prince Jha, Sriparna Saha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+EcoSage+Assistant:+Towards+Building+A+Multimodal+Plant+Care+Dialogue+Assistant)|0|
|[Controllable Decontextualization of Yes/No Question and Answers into Factual Statements](https://doi.org/10.1007/978-3-031-56060-6_27)|Lingbo Mo, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controllable+Decontextualization+of+Yes/No+Question+and+Answers+into+Factual+Statements)|0|
|[Reading Between the Frames: Multi-modal Depression Detection in Videos from Non-verbal Cues](https://doi.org/10.1007/978-3-031-56027-9_12)|David GimenoGómez, AnaMaria Bucur, Adrian Cosma, Carlos David MartínezHinarejos, Paolo Rosso||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reading+Between+the+Frames:+Multi-modal+Depression+Detection+in+Videos+from+Non-verbal+Cues)|0|
|[Investigating the Effects of Sparse Attention on Cross-Encoders](https://doi.org/10.1007/978-3-031-56027-9_11)|Ferdinand Schlatt, Maik Fröbe, Matthias Hagen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+the+Effects+of+Sparse+Attention+on+Cross-Encoders)|0|
|[SumBlogger: Abstractive Summarization of Large Collections of Scientific Articles](https://doi.org/10.1007/978-3-031-56027-9_23)|Pavlos Zakkas, Suzan Verberne, Jakub Zavrel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SumBlogger:+Abstractive+Summarization+of+Large+Collections+of+Scientific+Articles)|0|
|[Role-Guided Contrastive Learning for Event Argument Extraction](https://doi.org/10.1007/978-3-031-56027-9_21)|Chunyu Yao, Yi Guo, Xue Chen, Zhenzhen Duan, Jiaojiao Fu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Role-Guided+Contrastive+Learning+for+Event+Argument+Extraction)|0|
|[Attend All Options at Once: Full Context Input for Multi-choice Reading Comprehension](https://doi.org/10.1007/978-3-031-56027-9_24)|Runda Wang, Suzan Verberne, Marco Spruit||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attend+All+Options+at+Once:+Full+Context+Input+for+Multi-choice+Reading+Comprehension)|0|
|[Zero-Shot Generative Large Language Models for Systematic Review Screening Automation](https://doi.org/10.1007/978-3-031-56027-9_25)|Shuai Wang, Harrisen Scells, Shengyao Zhuang, Martin Potthast, Bevan Koopman, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Zero-Shot+Generative+Large+Language+Models+for+Systematic+Review+Screening+Automation)|0|
|[WebSAM-Adapter: Adapting Segment Anything Model for Web Page Segmentation](https://doi.org/10.1007/978-3-031-56027-9_27)|Bowen Ren, Zefeng Qian, Yuchen Sun, Chao Gao, Chongyang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WebSAM-Adapter:+Adapting+Segment+Anything+Model+for+Web+Page+Segmentation)|0|
|[A Phrase-Level Attention Enhanced CRF for Keyphrase Extraction](https://doi.org/10.1007/978-3-031-56027-9_28)|Shinian Li, Tao Jiang, Yuxiang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Phrase-Level+Attention+Enhanced+CRF+for+Keyphrase+Extraction)|0|
|[Taxonomy of Mathematical Plagiarism](https://doi.org/10.1007/978-3-031-56066-8_2)|Ankit Satpute, André GreinerPetter, Noah Gießing, Isabel Beckenbach, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Taxonomy+of+Mathematical+Plagiarism)|0|
|[Unraveling Disagreement Constituents in Hateful Speech](https://doi.org/10.1007/978-3-031-56066-8_3)|Giulia Rizzi, Alessandro Astorino, Paolo Rosso, Elisabetta Fersini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+Disagreement+Constituents+in+Hateful+Speech)|0|
|[SoftQE: Learned Representations of Queries Expanded by LLMs](https://doi.org/10.1007/978-3-031-56066-8_8)|Varad Pimpalkhute, John Heyer, Xusen Yin, Sameer Gupta||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SoftQE:+Learned+Representations+of+Queries+Expanded+by+LLMs)|0|
|[Optimizing BERTopic: Analysis and Reproducibility Study of Parameter Influences on Topic Modeling](https://doi.org/10.1007/978-3-031-56066-8_14)|Martin Borcin, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+BERTopic:+Analysis+and+Reproducibility+Study+of+Parameter+Influences+on+Topic+Modeling)|0|
|[A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR](https://doi.org/10.1007/978-3-031-56066-8_13)|Xinyu Mao, Bevan Koopman, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Reproducibility+Study+of+Goldilocks:+Just-Right+Tuning+of+BERT+for+TAR)|0|
|[Good for Children, Good for All?](https://doi.org/10.1007/978-3-031-56066-8_24)|Monica Landoni, Theo Huibers, Emiliana Murgia, Maria Soledad Pera||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Good+for+Children,+Good+for+All?)|0|
|[Mu2STS: A Multitask Multimodal Sarcasm-Humor-Differential Teacher-Student Model for Sarcastic Meme Detection](https://doi.org/10.1007/978-3-031-56063-7_2)|Gitanjali Kumari, Chandranath Adak, Asif Ekbal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mu2STS:+A+Multitask+Multimodal+Sarcasm-Humor-Differential+Teacher-Student+Model+for+Sarcastic+Meme+Detection)|0|
|[An Adaptive Feature Selection Method for Learning-to-Enumerate Problem](https://doi.org/10.1007/978-3-031-56063-7_8)|Satoshi Horikawa, Chiyonosuke Nemoto, Keishi Tajima, Masaki Matsubara, Atsuyuki Morishima||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Adaptive+Feature+Selection+Method+for+Learning-to-Enumerate+Problem)|0|
|[Asking Questions Framework for Oral History Archives](https://doi.org/10.1007/978-3-031-56063-7_11)|Jan Svec, Martin Bulín, Adam Frémund, Filip Polák||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asking+Questions+Framework+for+Oral+History+Archives)|0|
|[Yes, This Is What I Was Looking For! Towards Multi-modal Medical Consultation Concern Summary Generation](https://doi.org/10.1007/978-3-031-56063-7_14)|Abhisek Tiwari, Shreyangshu Bera, Sriparna Saha, Pushpak Bhattacharyya, Samrat Ghosh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Yes,+This+Is+What+I+Was+Looking+For!+Towards+Multi-modal+Medical+Consultation+Concern+Summary+Generation)|0|
|[Interactive Topic Tagging in Community Question Answering Platforms](https://doi.org/10.1007/978-3-031-56063-7_13)|Radin Hamidi Rad, Silviu Cucerzan, Nirupama Chandrasekaran, Michael Gamon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Topic+Tagging+in+Community+Question+Answering+Platforms)|0|
|[Mitigating Data Sparsity via Neuro-Symbolic Knowledge Transfer](https://doi.org/10.1007/978-3-031-56063-7_15)|Tommaso Carraro, Alessandro Daniele, Fabio Aiolli, Luciano Serafini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Data+Sparsity+via+Neuro-Symbolic+Knowledge+Transfer)|0|
|[Enhancing Legal Named Entity Recognition Using RoBERTa-GCN with CRF: A Nuanced Approach for Fine-Grained Entity Recognition](https://doi.org/10.1007/978-3-031-56063-7_17)|Arihant Jain, Raksha Sharma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Legal+Named+Entity+Recognition+Using+RoBERTa-GCN+with+CRF:+A+Nuanced+Approach+for+Fine-Grained+Entity+Recognition)|0|
|[A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ Generation Using GPT](https://doi.org/10.1007/978-3-031-56063-7_18)|Subhankar Maity, Aniket Deroy, Sudeshna Sarkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Multi-Stage+Prompting+Approach+for+Language+Agnostic+MCQ+Generation+Using+GPT)|0|
|[A Study on Hierarchical Text Classification as a Seq2seq Task](https://doi.org/10.1007/978-3-031-56063-7_20)|Fatos Torba, Christophe Gravier, Charlotte Laclau, Abderrhammen Kammoun, Julien Subercaze||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+on+Hierarchical+Text+Classification+as+a+Seq2seq+Task)|0|
|[MFVIEW: Multi-modal Fake News Detection with View-Specific Information Extraction](https://doi.org/10.1007/978-3-031-56063-7_26)|Marium Malik, Jiaojiao Jiang, Yang Song, Sanjay Jha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MFVIEW:+Multi-modal+Fake+News+Detection+with+View-Specific+Information+Extraction)|0|
|[Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book QA](https://doi.org/10.1007/978-3-031-56063-7_31)|Pierre Erbacher, Louis Falissard, Vincent Guigue, Laure Soulier||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Navigating+Uncertainty:+Optimizing+API+Dependency+for+Hallucination+Reduction+in+Closed-Book+QA)|0|
|[Can We Predict QPP? An Approach Based on Multivariate Outliers](https://doi.org/10.1007/978-3-031-56063-7_38)|AdrianGabriel Chifu, Sébastien Déjean, Moncef Garouani, Josiane Mothe, Diégo Ortiz, Md Zia Ullah||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+We+Predict+QPP?+An+Approach+Based+on+Multivariate+Outliers)|0|
|[SALSA: Salience-Based Switching Attack for Adversarial Perturbations in Fake News Detection Models](https://doi.org/10.1007/978-3-031-56069-9_3)|Chahat Raj, Anjishnu Mukherjee, Hemant Purohit, Antonios Anastasopoulos, Ziwei Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SALSA:+Salience-Based+Switching+Attack+for+Adversarial+Perturbations+in+Fake+News+Detection+Models)|0|
|[FakeClaim: A Multiple Platform-Driven Dataset for Identification of Fake News on 2023 Israel-Hamas War](https://doi.org/10.1007/978-3-031-56069-9_5)|Gautam Kishore Shahi, Amit Kumar Jaiswal, Thomas Mandl||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FakeClaim:+A+Multiple+Platform-Driven+Dataset+for+Identification+of+Fake+News+on+2023+Israel-Hamas+War)|0|
|[MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries](https://doi.org/10.1007/978-3-031-56069-9_8)|Akash Ghosh, Arkadeep Acharya, Prince Jha, Sriparna Saha, Aniket Gaudgaul, Rajdeep Majumdar, Aman Chadha, Raghav Jain, Setu Sinha, Shivani Agarwal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MedSumm:+A+Multimodal+Approach+to+Summarizing+Code-Mixed+Hindi-English+Clinical+Queries)|0|
|[The Open Web Index - Crawling and Indexing the Web for Public Use](https://doi.org/10.1007/978-3-031-56069-9_10)|Gijs Hendriksen, Michael Dinzinger, Sheikh Mastura Farzana, Noor Afshan Fathima, Maik Fröbe, Sebastian Schmidt, Saber Zerhoudi, Michael Granitzer, Matthias Hagen, Djoerd Hiemstra, Martin Potthast, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Open+Web+Index+-+Crawling+and+Indexing+the+Web+for+Public+Use)|0|
|[Towards Robust Expert Finding in Community Question Answering Platforms](https://doi.org/10.1007/978-3-031-56069-9_12)|Maddalena Amendola, Andrea Passarella, Raffaele Perego||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+Expert+Finding+in+Community+Question+Answering+Platforms)|0|
|[Interactive Document Summarization](https://doi.org/10.1007/978-3-031-56069-9_14)|Raoufdine Said, Adrien Guille||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interactive+Document+Summarization)|0|
|[Physio: An LLM-Based Physiotherapy Advisor](https://doi.org/10.1007/978-3-031-56069-9_16)|Rúben Almeida, Hugo O. Sousa, Luís Filipe Cunha, Nuno Guimarães, Ricardo Campos, Alípio Jorge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physio:+An+LLM-Based+Physiotherapy+Advisor)|0|
|[eval-rationales: An End-to-End Toolkit to Explain and Evaluate Transformers-Based Models](https://doi.org/10.1007/978-3-031-56069-9_20)|Khalil Maachou, Jesús LovónMelgarejo, José G. Moreno, Lynda Tamine||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=eval-rationales:+An+End-to-End+Toolkit+to+Explain+and+Evaluate+Transformers-Based+Models)|0|
|[VADIS - A Variable Detection, Interlinking and Summarization System](https://doi.org/10.1007/978-3-031-56069-9_22)|Yavuz Selim Kartal, Muhammad Ahsan Shahid, Sotaro Takeshita, Tornike Tsereteli, Andrea Zielinski, Benjamin Zapilko, Philipp Mayr||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VADIS+-+A+Variable+Detection,+Interlinking+and+Summarization+System)|0|
|[Building and Evaluating a WebApp for Effortless Deep Learning Model Deployment](https://doi.org/10.1007/978-3-031-56069-9_26)|Ruikun Wu, Jiaxuan Han, Jerome Ramos, Aldo Lipani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Building+and+Evaluating+a+WebApp+for+Effortless+Deep+Learning+Model+Deployment)|0|
|[indxr: A Python Library for Indexing File Lines](https://doi.org/10.1007/978-3-031-56069-9_27)|Elias Bassani, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=indxr:+A+Python+Library+for+Indexing+File+Lines)|0|
|[SciSpace Literature Review: Harnessing AI for Effortless Scientific Discovery](https://doi.org/10.1007/978-3-031-56069-9_28)|Siddhant Jain, Asheesh Kumar, Trinita Roy, Kartik Shinde, Goutham Vignesh, Rohan Tondulkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SciSpace+Literature+Review:+Harnessing+AI+for+Effortless+Scientific+Discovery)|0|
|[Let's Get It Started: Fostering the Discoverability of New Releases on Deezer](https://doi.org/10.1007/978-3-031-56069-9_33)|Léa Briand, Théo Bontempelli, Walid Bendada, Mathieu Morlon, François Rigaud, Benjamin Chapus, Thomas Bouabça, Guillaume SalhaGalvan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Let's+Get+It+Started:+Fostering+the+Discoverability+of+New+Releases+on+Deezer)|0|
|[Augmenting KG Hierarchies Using Neural Transformers](https://doi.org/10.1007/978-3-031-56069-9_35)|Sanat Sharma, Mayank Poddar, Jayant Kumar, Kosta Blank, Tracy Holloway King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+KG+Hierarchies+Using+Neural+Transformers)|0|
|[Document Level Event Extraction from Narratives](https://doi.org/10.1007/978-3-031-56069-9_38)|Luís Filipe Cunha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Document+Level+Event+Extraction+from+Narratives)|0|
|[Shuffling a Few Stalls in a Crowded Bazaar: Potential Impact of Document-Side Fairness on Unprivileged Info-Seekers](https://doi.org/10.1007/978-3-031-56069-9_43)|Sean Healy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shuffling+a+Few+Stalls+in+a+Crowded+Bazaar:+Potential+Impact+of+Document-Side+Fairness+on+Unprivileged+Info-Seekers)|0|
|[Knowledge Transfer from Resource-Rich to Resource-Scarce Environments](https://doi.org/10.1007/978-3-031-56069-9_44)|Negin Ghasemi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Transfer+from+Resource-Rich+to+Resource-Scarce+Environments)|0|
|[PhD Candidacy: A Tutorial on Overcoming Challenges and Achieving Success](https://doi.org/10.1007/978-3-031-56069-9_45)|Johanne R. Trippas, David Maxwell||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PhD+Candidacy:+A+Tutorial+on+Overcoming+Challenges+and+Achieving+Success)|0|
|[The CLEF-2024 CheckThat! Lab: Check-Worthiness, Subjectivity, Persuasion, Roles, Authorities, and Adversarial Robustness](https://doi.org/10.1007/978-3-031-56069-9_62)|Alberto BarrónCedeño, Firoj Alam, Tanmoy Chakraborty, Tamer Elsayed, Preslav Nakov, Piotr Przybyla, Julia Maria Struß, Fatima Haouari, Maram Hasanain, Federico Ruggeri, Xingyi Song, Reem Suwaileh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+CLEF-2024+CheckThat!+Lab:+Check-Worthiness,+Subjectivity,+Persuasion,+Roles,+Authorities,+and+Adversarial+Robustness)|0|
|[ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality](https://doi.org/10.1007/978-3-031-56069-9_63)|Jussi Karlgren, Luise Dürlich, Evangelia Gogoulou, Liane Guillou, Joakim Nivre, Magnus Sahlgren, Aarne Talman||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ELOQUENT+CLEF+Shared+Tasks+for+Evaluation+of+Generative+Language+Model+Quality)|0|
|[Overview of Touché 2024: Argumentation Systems](https://doi.org/10.1007/978-3-031-56069-9_64)|Johannes Kiesel, Çagri Çöltekin, Maximilian Heinrich, Maik Fröbe, Milad Alshomary, Bertrand De Longueville, Tomaz Erjavec, Nicolas Handke, Matyás Kopp, Nikola Ljubesic, Katja Meden, Nailia Mirzakhmedova, Vaidas Morkevicius, Theresa ReitisMünstermann, Mario Scharfbillig, Nicolas Stefanovitch, Henning Wachsmuth, Martin Potthast, Benno Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overview+of+Touché+2024:+Argumentation+Systems)|0|
|[eRisk 2024: Depression, Anorexia, and Eating Disorder Challenges](https://doi.org/10.1007/978-3-031-56069-9_65)|Javier Parapar, Patricia MartínRodilla, David E. Losada, Fabio Crestani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=eRisk+2024:+Depression,+Anorexia,+and+Eating+Disorder+Challenges)|0|
|[QuantumCLEF - Quantum Computing at CLEF](https://doi.org/10.1007/978-3-031-56069-9_66)|Andrea Pasin, Maurizio Ferrari Dacrema, Paolo Cremonesi, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QuantumCLEF+-+Quantum+Computing+at+CLEF)|0|
|[EXIST 2024: sEXism Identification in Social neTworks and Memes](https://doi.org/10.1007/978-3-031-56069-9_68)|Laura Plaza, Jorge CarrillodeAlbornoz, Enrique Amigó, Julio Gonzalo, Roser Morante, Paolo Rosso, Damiano Spina, Berta Chulvi, Alba Maeso, Víctor Ruiz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EXIST+2024:+sEXism+Identification+in+Social+neTworks+and+Memes)|0|
