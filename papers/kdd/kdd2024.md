# KDD2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[On the Convergence of Zeroth-Order Federated Tuning for Large Language Models](https://doi.org/10.1145/3637528.3671865)|Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Convergence+of+Zeroth-Order+Federated+Tuning+for+Large+Language+Models)|2|
|[LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?](https://doi.org/10.1145/3637528.3671709)|Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Wenwu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM4DyG:+Can+Large+Language+Models+Solve+Spatial-Temporal+Problems+on+Dynamic+Graphs?)|2|
|[FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning](https://doi.org/10.1145/3637528.3671573)|Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FederatedScope-LLM:+A+Comprehensive+Package+for+Fine-tuning+Large+Language+Models+in+Federated+Learning)|2|
|[Ads Recommendation in a Collapsed and Entangled World](https://doi.org/10.1145/3637528.3671607)|Junwei Pan, Wei Xue, Ximei Wang, Haibin Yu, Xun Liu, Shijie Quan, Xueming Qiu, Dapeng Liu, Lei Xiao, Jie Jiang|Tencent Inc., Shenzhen, China|We present Tencent's ads recommendation system and examine the challenges and practices of learning appropriate recommendation representations. Our study begins by showcasing our approaches to preserving prior knowledge when encoding features of diverse types into embedding representations. We specifically address sequence features, numeric features, and pre-trained embedding features. Subsequently, we delve into two crucial challenges related to feature representation: the dimensional collapse of embeddings and the interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Additionally, we introduce three analysis tools that enable us to study feature correlation, dimensional collapse, and interest entanglement. This work builds upon the continuous efforts of Tencent's ads recommendation team over the past decade. It summarizes general design principles and presents a series of readily applicable solutions and analysis tools. The reported performance is based on our online advertising platform, which handles hundreds of billions of requests daily and serves millions of ads to billions of users.|我们将介绍腾讯的广告推荐系统，并探讨学习适当的推荐表达的挑战和实践。我们的研究首先展示了我们在将不同类型的特征编码到嵌入表示中时保留先验知识的方法。我们专门讨论序列特征、数字特征和预先训练的嵌入特征。随后，我们深入研究了与特征表示相关的两个关键挑战: 嵌入的维度崩溃和不同任务或场景之间的兴趣纠缠。我们提出了几个实用的方法来解决这些挑战，导致健壮的和分离的建议表示。然后，我们探讨了几种训练技术，以促进模型优化，减少偏差，并加强探索。此外，我们还介绍了三种分析工具，使我们能够研究特征相关性、维度折叠和利益纠缠。这项工作建立在腾讯广告推荐团队过去十年不断努力的基础上。它总结了一般的设计原则，并提出了一系列容易适用的解决方案和分析工具。报告的性能是基于我们的在线广告平台，该平台每天处理数千亿个请求，为数十亿用户提供数百万个广告。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ads+Recommendation+in+a+Collapsed+and+Entangled+World)|1|
|[EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration](https://doi.org/10.1145/3637528.3671775)|Ye Wang, Jiahao Xun, Minjie Hong, Jieming Zhu, Tao Jin, Wang Lin, Haoyuan Li, Linjun Li, Yan Xia, Zhou Zhao, Zhenhua Dong|Huawei Noah's Ark Lab, Shenzhen, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, Zhejiang, China|Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods. Our source code will be publicly available on PapersWithCode.com.|生成性检索是近年来出现的一种有前途的序列推荐方法，它将候选项检索框架为一个自回归序列生成问题。然而，现有的生成方法通常只关注项目信息的行为方面或语义方面，忽视了它们的互补性，因此效果有限。为了解决这个问题，我们引入了一个新的生成推荐框架 EAGER，它能够无缝地整合行为推荐和语义信息推荐。具体来说，我们确定了将这两种类型的信息结合起来的三个关键挑战: 一个能够处理两种特征类型的统一生成体系结构，确保对每种类型进行充分和独立的学习，以及培养能够提高协作信息利用率的微妙交互。为了实现这些目标，我们提出(1)利用共享编码器和两个单独的解码器的两流生成架构，以基于置信度的排序策略解码行为标记和语义标记; (2)具有摘要标记的全局对比任务，以实现每种类型的信息的区分性解码; 和(3)语义指导的转移任务，旨在通过重建和评估目标隐式促进交叉互动。我们在四个公共基准上验证了 EAGER 算法的有效性，证明了与现有方法相比，EAGER 算法具有更好的性能。我们的源代码将在 paperswithcode.com 上公开。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EAGER:+Two-Stream+Generative+Recommender+with+Behavior-Semantic+Collaboration)|1|
|[Debiased Recommendation with Noisy Feedback](https://doi.org/10.1145/3637528.3671915)|Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, XiaoHua Zhou|University of California, San Diego, Beijing, China; University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, Singapore; Peking University, Beijing, China; Zhejiang University, Hangzhou, China|Ratings of a user to most items in recommender systems are usually missing not at random (MNAR), largely because users are free to choose which items to rate. To achieve unbiased learning of the prediction model under MNAR data, three typical solutions have been proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. However, these methods ignore an alternative form of bias caused by the inconsistency between the observed ratings and the users' true preferences, also known as noisy feedback or outcome measurement errors (OME), e.g., due to public opinion or low-quality data collection process. In this work, we study intersectional threats to the unbiased learning of the prediction model from data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and OME-DR estimators, which largely extend the existing estimators to combat OME in real-world recommendation scenarios. Next, we theoretically prove the unbiasedness and generalization bound of the proposed estimators. We further propose an alternate denoising training approach to achieve unbiased learning of the prediction model under MNAR data with OME. Extensive experiments are conducted on three real-world datasets and one semi-synthetic dataset to show the effectiveness of our proposed approaches. The code is available at https://github.com/haoxuanli-pku/KDD24-OME-DR.|在推荐系统中，用户对大多数项目的评分通常不是随机丢失的(MNAR) ，主要是因为用户可以自由选择对哪些项目进行评分。为了实现 MNAR 数据下预测模型的无偏学习，提出了三种典型的解决方案，包括基于错误插补(EIB)、逆倾向评分(IPS)和双鲁棒(DR)方法。然而，这些方法忽略了由观察到的评分和用户的真实偏好之间的不一致引起的另一种形式的偏差，也称为噪声反馈或结果测量错误(OME) ，例如由于公众意见或低质量的数据收集过程。在这项工作中，我们研究交叉威胁的预测模型无偏学习的数据 MNAR 和 OME 收集的数据。首先，我们设计了 OME-EIB、 OME-IPS 和 OME-DR 估计器，它们在很大程度上扩展了现有的估计器，以便在实际推荐场景中对抗 OME。接着，我们从理论上证明了所提出的估计量的无偏性和广义界。我们进一步提出了一种交替去噪训练方法，用 OME 实现 MNAR 数据下预测模型的无偏学习。在三个实际数据集和一个半合成数据集上进行了广泛的实验，以证明我们提出的方法的有效性。密码可在 https://github.com/haoxuanli-pku/kdd24-ome-dr 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiased+Recommendation+with+Noisy+Feedback)|1|
|[Harm Mitigation in Recommender Systems under User Preference Dynamics](https://doi.org/10.1145/3637528.3671925)|Jerry Chee, Shankar Kalyanaraman, Sindhu Kiranmai Ernala, Udi Weinsberg, Sarah Dean, Stratis Ioannidis|Meta, Menlo Park, CA, USA; Cornell University, Ithaca, NY, USA; Northeastern University, Boston, MA, USA|We consider a recommender system that takes into account the interplaybetween recommendations, the evolution of user interests, and harmful content.We model the impact of recommendations on user behavior, particularly thetendency to consume harmful content. We seek recommendation policies thatestablish a tradeoff between maximizing click-through rate (CTR) and mitigatingharm. We establish conditions under which the user profile dynamics have astationary point, and propose algorithms for finding an optimal recommendationpolicy at stationarity. We experiment on a semi-synthetic movie recommendationsetting initialized with real data and observe that our policies outperformbaselines at simultaneously maximizing CTR and mitigating harm.|我们考虑建议之间的相互作用，用户兴趣的演变和有害内容的推荐系统。我们模拟建议对用户行为的影响，特别是消费有害内容的趋势。我们寻求建议政策，在最大化点进率和减少伤害之间建立平衡。我们建立了用户轮廓动态具有平稳点的条件，并提出了在平稳点寻找最优推荐策略的算法。我们在一个半合成的电影推荐设置上进行了实验，初始化为真实数据，并观察到我们的策略在同时最大化点击率和减少危害方面优于基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harm+Mitigation+in+Recommender+Systems+under+User+Preference+Dynamics)|1|
|[Face4Rag: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese](https://doi.org/10.1145/3637528.3671656)|Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Face4Rag:+Factual+Consistency+Evaluation+for+Retrieval+Augmented+Generation+in+Chinese)|1|
|[A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)](https://doi.org/10.1145/3637528.3671474)|Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, Silvia Milano||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Review+of+Modern+Recommender+Systems+Using+Generative+Models+(Gen-RecSys))|1|
|[A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction](https://doi.org/10.1145/3637528.3671984)|Jiahui Gong, Jingtao Ding, Fanjin Meng, Guilong Chen, Hong Chen, Shen Zhao, Haisheng Lu, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Population-to-individual+Tuning+Framework+for+Adapting+Pretrained+LM+to+On-device+User+Intent+Prediction)|1|
|[Efficient Exploration of the Rashomon Set of Rule-Set Models](https://doi.org/10.1145/3637528.3671818)|Martino Ciaperoni, Han Xiao, Aristides Gionis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Exploration+of+the+Rashomon+Set+of+Rule-Set+Models)|1|
|[Learning the Covariance of Treatment Effects Across Many Weak Experiments](https://doi.org/10.1145/3637528.3672034)|Aurélien Bibaut, Winston Chou, Simon Ejdemyr, Nathan Kallus||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+the+Covariance+of+Treatment+Effects+Across+Many+Weak+Experiments)|1|
|[Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors](https://doi.org/10.1145/3637528.3671846)|Taehyung Kwon, Jihoon Ko, Jinhong Jung, JunGi Jang, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Compact+Decomposition+of+Irregular+Tensors+for+Data+Compression:+From+Sparse+to+Dense+to+High-Order+Tensors)|1|
|[TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics](https://doi.org/10.1145/3637528.3671934)|Chang Liu, Jingtao Ding, Yiwen Song, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TDNetGen:+Empowering+Complex+Network+Resilience+Prediction+with+Generative+Augmentation+of+Topology+and+Dynamics)|1|
|[Scalable Temporal Motif Densest Subnetwork Discovery](https://doi.org/10.1145/3637528.3671889)|Ilie Sarpe, Fabio Vandin, Aristides Gionis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Temporal+Motif+Densest+Subnetwork+Discovery)|1|
|[UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction](https://doi.org/10.1145/3637528.3671662)|Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniST:+A+Prompt-Empowered+Universal+Model+for+Urban+Spatio-Temporal+Prediction)|1|
|[UrbanGPT: Spatio-Temporal Large Language Models](https://doi.org/10.1145/3637528.3671578)|Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UrbanGPT:+Spatio-Temporal+Large+Language+Models)|1|
|[Choosing a Proxy Metric from Past Experiments](https://doi.org/10.1145/3637528.3671543)|Nilesh Tripuraneni, Lee Richardson, Alexander D'Amour, Jacopo Soriano, Steve Yadlowsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Choosing+a+Proxy+Metric+from+Past+Experiments)|1|
|[A Review of Graph Neural Networks in Epidemic Modeling](https://doi.org/10.1145/3637528.3671455)|Zewen Liu, Guancheng Wan, B. Aditya Prakash, Max S. Y. Lau, Wei Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Review+of+Graph+Neural+Networks+in+Epidemic+Modeling)|1|
|[Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction](https://doi.org/10.1145/3637528.3671601)|Ruijie Hou, Zhaoyang Yang, Ming Yu, Hongyu Lu, Zhuobin Zheng, Yu Chen, Qinsong Zeng, Ming Chen|Wechat, Tencent, Guangzhou, China; Wechat, Tencent, Beijing, China|Lifelong sequential modeling (LSM) has significantly advanced recommendation systems on social media platforms. Diverging from single-domain LSM, cross-domain LSM involves modeling lifelong behavior sequences from a source domain to a different target domain. In this paper, we propose the Lifelong Cross Network (LCN), a novel approach for cross-domain LSM. LCN features a Cross Representation Production (CRP) module that utilizes contrastive loss to improve the learning of item embeddings, effectively bridging items across domains. This is important for enhancing the retrieval of relevant items in cross-domain lifelong sequences. Furthermore, we propose the Lifelong Attention Pyramid (LAP) module, which contains three cascading attention levels. By adding an intermediate level and integrating the results from all three levels, the LAP module can capture a broad spectrum of user interests and ensure gradient propagation throughout the sequence. The proposed LAP can also achieve remarkable consistency across attention levels, making it possible to further narrow the candidate item pool of the top level. This allows for the use of advanced attention techniques to effectively mitigate the impact of the noise in cross-domain sequences and improve the non-linearity of the representation, all while maintaining computational efficiency. Extensive experiments conducted on both a public dataset and an industrial dataset from the WeChat Channels platform reveal that the LCN outperforms current methods in terms of prediction accuracy and online performance metrics.|终身顺序建模(LSM)在社交媒体平台上拥有非常先进的推荐系统。与单域 LSM 不同，跨域 LSM 涉及从源域到不同目标域的终身行为序列建模。本文提出了一种新的跨域 LSM 方法——终身交叉网络(LCN)。LCN 提供了一个交叉表示生成(CRP)模块，该模块利用对比度损失来改进项目嵌入的学习，有效地跨域连接项目。这对于提高跨域终身序列中相关项目的检索是非常重要的。此外，我们提出了终身注意金字塔(LAP)模块，它包含三个级联注意水平。通过增加一个中间层，并整合来自所有三个层次的结果，LAP 模块可以捕获广泛的用户兴趣，并确保梯度传播整个序列。提出的 LAP 还可以在不同的注意水平上实现显著的一致性，从而有可能进一步缩小最高水平的候选项库。这允许使用先进的注意力技术，以有效地减轻跨域序列中噪声的影响，并改善非线性表示，同时保持计算效率。在公共数据集和来自微信频道平台的工业数据集上进行的大量实验表明，LCN 在预测准确性和在线性能指标方面优于目前的方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Domain+LifeLong+Sequential+Modeling+for+Online+Click-Through+Rate+Prediction)|0|
|[Mitigating Pooling Bias in E-commerce Search via False Negative Estimation](https://doi.org/10.1145/3637528.3671630)|Xiaochen Wang, Xiao Xiao, Ruhan Zhang, Xuan Zhang, Taesik Na, Tejaswi Tenneti, Haixun Wang, Fenglong Ma|The Pennsylvania State University, University Park, PA, USA; Instacart, San Francisco, CA, USA|Efficient and accurate product relevance assessment is critical for user experiences and business success. Training a proficient relevance assessment model requires high-quality query-product pairs, often obtained through negative sampling strategies. Unfortunately, current methods introduce pooling bias by mistakenly sampling false negatives, diminishing performance and business impact. To address this, we present Bias-mitigating Hard Negative Sampling (BHNS), a novel negative sampling strategy tailored to identify and adjust for false negatives, building upon our original False Negative Estimation algorithm. Our experiments in the Instacart search setting confirm BHNS as effective for practical e-commerce use. Furthermore, comparative analyses on public dataset showcase its domain-agnostic potential for diverse applications.|高效、准确的产品相关性评估对于用户体验和商业成功至关重要。训练一个熟练的相关性评估模型需要高质量的查询产品对，通常通过负抽样策略获得。不幸的是，目前的方法通过错误地采样错误的否定，减少性能和业务影响引入汇集偏见。为了解决这个问题，我们提出了消除偏差的硬负采样(BHNS) ，一种新的负采样策略，专门用于识别和调整假阴性，建立在我们原来的假阴性估计算法的基础上。我们在 Instacart 搜索设置中的实验证实了 BHNS 对于实际电子商务的使用是有效的。此外，对公共数据集的比较分析表明，其领域不可知的潜力适用于不同的应用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Pooling+Bias+in+E-commerce+Search+via+False+Negative+Estimation)|0|
|[Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations](https://doi.org/10.1145/3637528.3671715)|Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, Yihua Huang|State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China|Multi-task learning (MTL), which aims to make full use of knowledge contained in multiple tasks to enhance overall performance and efficiency, has been broadly applied in recommendations. The main challenge for MTL models is negative transfer. Existing MTL models, mainly built on the Mixture-of-Experts (MoE) structure, seek enhancements in performance through feature selection and specific expert sharing mode design. However, one expert sharing mode may not be universally applicable due to the complex correlations and diverse demands among various tasks. Additionally, homogeneous expert architectures in such models further limit their performance. To address these issues, in this paper, we propose an innovative automatic MTL framework, AutoMTL, leveraging neural architecture search (NAS) to design optimal expert architectures and sharing modes. The Dual-level Expert Sharing mode and Architecture Navigator (DESAN) search space of AutoMTL can not only efficiently explore expert sharing modes and feature selection schemes but also focus on the architectures of expert subnetworks. Along with this, we introduce an efficient Progressively Discretizing Differentiable Architecture Search (PD-DARTS) algorithm for search space exploration. Extensive experiments demonstrate that AutoMTL can consistently outperform state-of-the-art, human-crafted MTL models. Moreover, the insights obtained from the discovered architectures provide valuable guidance for building new multi-task recommendation models.|多任务学习(Multi-Task Learning，MTL)旨在充分利用多任务中包含的知识，提高整体绩效和效率，已被广泛应用于建议学习中。MTL 模型的主要挑战是负迁移。现有的 MTL 模型主要建立在专家混合(MoE)结构的基础上，通过特征选择和专家共享模式设计来提高性能。然而，由于各种任务之间复杂的相关性和不同的需求，一种专家共享模式并不能普遍适用。此外，这些模型中的同类专家体系结构进一步限制了它们的性能。为了解决这些问题，本文提出了一种创新的自动 MTL 框架 AutoMTL，利用神经结构搜索(NAS)来设计最优的专家结构和共享模式。AutoMTL 的双层专家共享模式和体系结构导航器(DESAN)搜索空间不仅可以有效地探索专家共享模式和特征选择方案，而且可以集中研究专家子网的体系结构。在此基础上，提出了一种高效的逐次离散可微体系结构搜索(PD-DARTS)算法。大量的实验表明，AutoMTL 可以持续优于最先进的，人工制作的 MTL 模型。此外，从所发现的体系结构中获得的见解为构建新的多任务推荐模型提供了有价值的指导。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Multi-Task+Learning+Framework+with+Neural+Architecture+Search+in+Recommendations)|0|
|[CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation](https://doi.org/10.1145/3637528.3671901)|Junda Wu, ChengChun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng Hou, Julian J. McAuley|University of California San Diego, La Jolla, CA, USA; Columbia University, New York, NY, USA; Adobe Research, San Jose, CA, USA|The long-tail recommendation is a challenging task for traditionalrecommender systems, due to data sparsity and data imbalance issues. The recentdevelopment of large language models (LLMs) has shown their abilities incomplex reasoning, which can help to deduce users' preferences based on veryfew previous interactions. However, since most LLM-based systems rely on items'semantic meaning as the sole evidence for reasoning, the collaborativeinformation of user-item interactions is neglected, which can cause the LLM'sreasoning to be misaligned with task-specific collaborative information of thedataset. To further align LLMs' reasoning to task-specific user-iteminteraction knowledge, we introduce collaborative retrieval-augmented LLMs,CoRAL, which directly incorporate collaborative evidence into the prompts.Based on the retrieved user-item interactions, the LLM can analyze shared anddistinct preferences among users, and summarize the patterns indicating whichtypes of users would be attracted by certain items. The retrieved collaborativeevidence prompts the LLM to align its reasoning with the user-item interactionpatterns in the dataset. However, since the capacity of the input prompt islimited, finding the minimally-sufficient collaborative information forrecommendation tasks can be challenging. We propose to find the optimalinteraction set through a sequential decision-making process and develop aretrieval policy learned through a reinforcement learning (RL) framework,CoRAL. Our experimental results show that CoRAL can significantly improve LLMs'reasoning abilities on specific recommendation tasks. Our analysis also revealsthat CoRAL can more efficiently explore collaborative information throughreinforcement learning.|由于数据稀疏和数据不平衡的问题，长尾推荐对于传统的推荐系统来说是一项具有挑战性的任务。大型语言模型(LLM)的最新发展已经显示出它们在复杂推理方面的能力，这种能力可以帮助推断用户基于极少数以前的交互的偏好。然而，由于大多数基于 LLM 的系统依赖于项目的语义作为推理的唯一证据，用户-项目交互的协作信息被忽视，这可能导致 LLM 的推理与数据集的特定任务的协作信息不一致。为了进一步将 LLM 的推理与特定于任务的用户项目交互知识结合起来，我们引入了协作检索增强 LLM，CoRAL，它直接将协作证据合并到提示中。基于检索到的用户-项目交互，LLM 可以分析用户之间的共享和不同偏好，并总结模式，指出哪些类型的用户会被某些项目吸引。检索到的协作证据提示 LLM 使其推理与数据集中的用户项交互模式保持一致。然而，由于输入提示的能力是有限的，找到最低限度-足够的协作信息的推荐任务可能是具有挑战性的。我们建议通过一个连续的决策过程来寻找最佳的交互集合，并通过一个强化学习(RL)框架 CoRAL 来发展检索策略。实验结果表明，CoRAL 可以显著提高 LLM 对特定推荐任务的推理能力。我们的分析还显示，通过强化学习，CoRAL 可以更有效地探索协作信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoRAL:+Collaborative+Retrieval-Augmented+Large+Language+Models+Improve+Long-tail+Recommendation)|0|
|[Text Matching Indexers in Taobao Search](https://doi.org/10.1145/3637528.3671654)|Sen Li, Fuyu Lv, Ruqing Zhang, Dan Ou, Zhixuan Zhang, Maarten de Rijke|CAS Key Lab of Network Data Science and Technology, ICT, CAS, Beijing, China; University of Amsterdam, Amsterdam, Netherlands; Alibaba Group, Hangzhou, China|Product search is an important service on Taobao, the largest e-commerce platform in China. Through this service, users can easily find products relevant to their specific needs. Coping with billion-size query loads, Taobao product search has traditionally relied on classical term-based retrieval models due to their powerful and interpretable indexes. In essence, efficient retrieval hinges on the proper storage of the inverted index. Recent successes involve reducing the size (pruning) of the inverted index but the construction and deployment of lossless static index pruning in practical product search still pose non-trivial challenges. In this work, we introduce a novel SM art INDexing (SMIND) solution in Taobao product search. SMIND is designed to reduce information loss during the static pruning process by incorporating user search preferences. Specifically, we first construct "user-query-item'' hypergraphs for four different search preferences, namely purchase, click, exposure, and relevance. Then, we develop an efficient TermRank algorithm applied to these hypergraphs, to preserve relevant items based on specific user preferences during the pruning of the inverted indexer. Our approach offers fresh insights into the field of product search, emphasizing that term dependencies in user search preferences go beyond mere text relevance. Moreover, to address the vocabulary mismatch problem inherent in term-based models, we also incorporate an multi-granularity semantic retrieval model to facilitate semantic matching. Empirical results from both offline evaluation and online A/B tests showcase the superiority of SMIND over state-of-the-art methods, especially in commerce metrics with significant improvements of 1.34% in Pay Order Count and 1.50% in Gross Merchandise Value. Besides, SMIND effectively mitigates the Matthew effect of user queries and has been in service for hundreds of millions of daily users since November 2022.|产品搜索是中国最大的电子商务平台淘宝上的一项重要服务。通过这项服务，用户可以很容易地找到与他们的具体需求相关的产品。为了应对数十亿大小的查询负载，淘宝产品搜索传统上依赖于传统的基于词汇的检索模型，因为它们的索引功能强大且易于解释。实质上，有效的检索取决于适当存储倒排索引。最近的成功包括减少了反向索引的大小(修剪) ，但是在实际的产品搜索中构建和部署无损静态索引修剪仍然带来了不小的挑战。在本文中，我们介绍了一个新颖的 SM 艺术索引(SMIND)解决方案在淘宝产品搜索。SMIND 的目的是通过合并用户搜索偏好，减少静态修剪过程中的信息损失。具体来说，我们首先为四种不同的搜索偏好(即购买、点击、曝光和相关性)构建“用户查询项目”超图。然后，我们开发了一个有效的 TermRank 算法应用于这些超图，以保留相关的项目基于特定的用户喜好在反向索引器的修剪过程中。我们的方法为产品搜索领域提供了新的视角，强调用户搜索偏好中的术语依赖性不仅仅是文本相关性。此外，为了解决基于词汇的模型所固有的词汇不匹配问题，我们还引入了一个多粒度的语义检索模型来促进语义匹配。线下评估和在线 A/B 测试的实证结果表明，SMIND 相对于最先进的方法具有优势，尤其是在商业指标方面，支付订单计数和商品总价值分别显著提高了1.34% 和1.50% 。此外，SMIND 有效地减轻了用户查询的“马太效应”，自2022年11月以来已为数亿日常用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text+Matching+Indexers+in+Taobao+Search)|0|
|[Unified Low-rank Compression Framework for Click-through Rate Prediction](https://doi.org/10.1145/3637528.3671520)|Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou, Jianxin Wu|Researcher, Shanghai, China; Nanjing University, Nanjing, Jiangsu, China|Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3--5× model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking.|深点进率(ctrl)预测模型在现代工业推荐方案中扮演着重要角色。但是，较高的内存开销和计算成本限制了它们在资源受限环境中的部署。低秩近似是计算机视觉和自然语言处理模型的一种有效方法，但其在压缩 CTR 预测模型方面的应用研究较少。由于有限的内存和计算资源，CTR 预测模型的压缩往往面临三个基本挑战，即(1)。如何减小模型尺寸以适应边缘设备？(2).如何加快 CTR 预测模型的推导？(3).如何保留原始模型压缩后的能力？先前的低秩压缩研究大多使用张量分解，这可以实现高参数的压缩比，但是带来了 AUC 降解和额外的计算开销。为了应对这些挑战，我们提出了一个统一的低秩分解框架来压缩 CTR 预测模型。我们发现，即使使用最经典的矩阵分解奇异值分解方法，我们的框架也能取得比原始模型更好的性能。为了进一步提高框架的有效性，我们对输出特征进行了局部压缩，而不是对模型权重进行压缩。我们统一的低秩压缩框架可以应用于各种 CTR 预测模型中的表和 MLP 层的嵌入。在两个学术数据集和一个实际工业基准上的大量实验表明，与未压缩的原始模型相比，通过3-5 × 模型尺寸的缩减，我们的压缩模型可以实现更快的推理和更高的 AUC。我们的代码是 https://github.com/yuhao318/atomic_feature_mimicking。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Low-rank+Compression+Framework+for+Click-through+Rate+Prediction)|0|
|[Optimizing Smartphone App Usage Prediction: A Click-Through Rate Ranking Approach](https://doi.org/10.1145/3637528.3671567)|Yuqi Zhang, Meiying Kang, Xiucheng Li, Yu Qiu, Zhijun Li|Harbin Institute of Technology, Harbin, China; Independent, Chengdu, China; Soochow University, Suzhou, China; Harbin Institute of Technology, Shenzhen, China|Over the past decade, smartphones have become indispensable personal mobile devices, experiencing a remarkable surge in software apps. These apps empower users to seamlessly connect with various internet services, such as social communication and online shopping. Accurately predicting smartphone app usage can effectively improve user experience and optimize resource utilization. However, existing models often treat app usage prediction as a classification problem, which suffers from issues of app usage imbalance and out-of-distribution (OOD) during deployment. To address these challenges, this paper proposes a novel click-through rate (CTR) ranking-based method for predicting app usage. By transforming the classification problem into a CTR problem, we can eliminate the negative impact of the app usage imbalance issue. To address the OOD issue during deployment, we generate the app click sequence and three types of discriminative features, which enable generalization on unseen apps. The app click sequence and the three types of features serve as inputs for training a CTR estimation model in the cloud, and the trained model is then deployed on the user's smartphone to predict the CTR for each installed app. The decision-making process involves ranking these CTR values and selecting the app with the highest CTR as the final prediction. Our method has been extensively tested with large-scale app usage data. The results demonstrate that our approach is able to outperform state-of-the-art methods, with improvements over 4.93% in top-3 accuracy and 6.64% in top-5 accuracy. It achieves approximately twice the accuracy in predicting apps with low usage frequencies in comparison to baseline methods. Our method has been successfully deployed on the app recommendation system of a leading smartphone manufacturer.|在过去十年里，智能手机已经成为不可或缺的个人移动设备，软件应用程序出现了惊人的增长。这些应用程序使用户能够无缝连接各种互联网服务，如社会交流和网上购物。准确预测智能手机应用程序的使用情况可以有效地改善用户体验和优化资源利用。然而，现有的模型往往将应用程序使用预测视为一个分类问题，在部署过程中存在应用程序使用不平衡和分布不均衡(OOD)的问题。为了应对这些挑战，本文提出了一种新的基于点进率排名(ctrr)的方法来预测应用程序的使用情况。通过将分类问题转化为 CTR 问题，我们可以消除应用程序使用不平衡问题的负面影响。为了解决部署过程中的 OOD 问题，我们生成了应用程序的点击序列和三种类型的区分特性，这些特性可以对看不见的应用程序进行泛化。应用程序点击序列和三种类型的功能作为输入，用于在云中训练一个点击率估计模型，然后将训练好的模型部署到用户的智能手机上，以预测每个安装的应用程序的点击率。决策过程包括对这些点击率值进行排序，并选择点击率最高的应用程序作为最终预测。我们的方法已经通过大规模的应用程序使用数据进行了广泛的测试。结果表明，我们的方法能够优于国家的最先进的方法，提高了4.93% 以上的前3名的准确性和6.64% 以上的前5名的准确性。与基线方法相比，它在预测使用频率较低的应用程序方面达到了大约两倍的准确性。我们的方法已成功应用于一家领先的智能手机制造商的应用程序推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Smartphone+App+Usage+Prediction:+A+Click-Through+Rate+Ranking+Approach)|0|
|[Relevance Meets Diversity: A User-Centric Framework for Knowledge Exploration Through Recommendations](https://doi.org/10.1145/3637528.3671949)|Erica Coppolillo, Giuseppe Manco, Aristides Gionis|Division of Theoretical Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, University of Calabria & ICAR-CNR, Rende, Italy; ICAR-CNR, Rende, Italy|Providing recommendations that are both relevant and diverse is a key consideration of modern recommender systems. Optimizing both of these measures presents a fundamental trade-off, as higher diversity typically comes at the cost of relevance, resulting in lower user engagement. Existing recommendation algorithms try to resolve this trade-off by combining the two measures, relevance and diversity, into one aim and then seeking recommendations that optimize the combined objective, for a given number of items. Traditional approaches, however, do not consider the user interaction with the suggested items. In this paper, we put the user at the central stage, and build on the interplay between relevance, diversity, and user behavior. In contrast to applications where the goal is solely to maximize engagement, we focus on scenarios aiming at maximizing the total amount of knowledge encountered by the user. We use diversity as a surrogate for the amount of knowledge obtained by the user while interacting with the system, and we seek to maximize diversity. We propose a probabilistic user-behavior model in which users keep interacting with the recommender system as long as they receive relevant suggestions, but they may stop if the relevance of the recommended items drops. Thus, for a recommender system to achieve a high-diversity measure, it will need to produce recommendations that are both relevant and diverse. Finally, we propose a novel recommendation strategy that combines relevance and diversity by a copula function. We conduct an extensive evaluation of the proposed methodology over multiple datasets, and we show that our strategy outperforms several state-of-the-art competitors. Our implementation is publicly available at https://github.com/EricaCoppolillo/EXPLORE.|提供相关和多样化的建议是现代推荐系统的一个关键考虑因素。优化这两个措施提出了一个基本的权衡，因为更高的多样性通常以相关性为代价，导致用户参与度降低。现有的推荐算法试图通过将相关性和多样性这两个衡量标准结合为一个目标来解决这种权衡，然后寻求建议，优化合并目标，以满足给定数量的项目。但是，传统方法不考虑用户与建议项的交互。在本文中，我们把用户放在中心阶段，并建立在相关性，多样性和用户行为之间的相互作用。与目标仅仅是最大化参与的应用程序不同，我们关注的场景旨在最大化用户遇到的知识总量。我们使用多样性作为用户在与系统交互时获得的知识量的替代指标，并寻求最大化多样性。我们提出了一个概率用户行为模型，在这个模型中，用户只要收到相关的建议，就会与推荐系统保持互动，但是如果推荐项目的相关性下降，他们可能会停止。因此，推荐系统要实现高度多样化的措施，就需要提出相关且多样化的建议。最后，我们提出了一个新的推荐策略，通过一个 Copula 函数结合相关性和多样性。我们在多个数据集上对提出的方法进行了广泛的评估，我们表明我们的策略优于几个最先进的竞争对手。我们的实施 https://github.com/ericacoppolillo/explore 公开发布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relevance+Meets+Diversity:+A+User-Centric+Framework+for+Knowledge+Exploration+Through+Recommendations)|0|
|[Understanding the Ranking Loss for Recommendation with Sparse User Feedback](https://doi.org/10.1145/3637528.3671565)|Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang|Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Tencent Inc., Shenzhen, China|Click-through rate (CTR) prediction is a crucial area of research in online advertising. While binary cross entropy (BCE) has been widely used as the optimization objective for treating CTR prediction as a binary classification problem, recent advancements have shown that combining BCE loss with an auxiliary ranking loss can significantly improve performance. However, the full effectiveness of this combination loss is not yet fully understood. In this paper, we uncover a new challenge associated with the BCE loss in scenarios where positive feedback is sparse: the issue of gradient vanishing for negative samples. We introduce a novel perspective on the effectiveness of the auxiliary ranking loss in CTR prediction: it generates larger gradients on negative samples, thereby mitigating the optimization difficulties when using the BCE loss only and resulting in improved classification ability. To validate our perspective, we conduct theoretical analysis and extensive empirical evaluations on public datasets. Additionally, we successfully integrate the ranking loss into Tencent's online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios. The code is openly accessible at: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.|点进率预测是在线广告研究的一个关键领域。二进制交叉熵(BCE)已被广泛用作将 CTR 预测作为二进制分类问题处理的优化目标，但最近的研究表明，将 BCE 损失与辅助排序损失相结合可以显著提高性能。然而，这种组合损失的全部有效性尚未完全了解。在本文中，我们揭示了一个新的挑战与 BCE 损失相关的情况下，正反馈是稀疏的: 问题的梯度消失的负样本。我们介绍了一种新的视角辅助排序损失在 CTR 预测中的有效性: 它在负样本上产生较大的梯度，从而减少了优化时仅使用 BCE 损失的困难，并导致分类能力的提高。为了验证我们的观点，我们对公共数据集进行了理论分析和广泛的实证评估。此外，我们成功地将排名损失纳入腾讯的在线广告系统，在两个主要情况下，商品总值(GMV)分别显著提高了0.70% 和1.26% 。该守则可在以下 https://github.com/skylerlinn/understanding-The-ranking-loss 公开查阅:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Ranking+Loss+for+Recommendation+with+Sparse+User+Feedback)|0|
|[Multi-Task Neural Linear Bandit for Exploration in Recommender Systems](https://doi.org/10.1145/3637528.3671649)|Yi Su, Haokai Lu, Yuening Li, Liang Liu, Shuchao Bi, Ed H. Chi, Minmin Chen|Google Deepmind, Mountain View, CA, USA; Google, Mountain View, CA, USA|Exposure bias and its induced feedback loop effect are well-known problems in recommender systems. Exploration is believed to be the key to break such feedback loops. While classical contextual bandit algorithms such as Upper-Confidence-Bound and Thompson Sampling have been successful in addressing the exploration-exploitation trade-off in the single-task settings with one clear reward signal, modern recommender systems often leverage multiple rich sources of feedback such as clicks, likes, dislikes, shares, satisfaction survey responses, and employ multi-task learning in practice. It is unclear how one can incorporate exploration in the multi-task setup with different objectives. In this paper, we study an efficient bandit algorithm tailored to multi-task recommender systems, named Multi-task Neural Linear Bandit (mtNLB). In particular, we investigate efficient feature embeddings in the multi-task setups that could be used as contextual features in the Neural Linear Bandit, a contextual bandit algorithm that nicely combines the representation power from DNN and simplicity in uncertainty calculation from linear models. We further study cost-effective approximations of the uncertainty estimate and principled ways to incorporate uncertainty into the multi-task scoring of items. To showcase the efficacy of our proposed method, we conduct live experiments on a large-scale commercial recommendation platform that serves billions of users. We evaluate the quality of the uncertainty estimate and demonstrate its ability to improve exploration across the different dimensions of the reward signals in comparison to baseline approaches.|在推荐系统中，曝光偏差及其诱导反馈回路效应是一个众所周知的问题。勘探被认为是打破这种反馈循环的关键。尽管传统的情境强盗算法如 Upper-Confidence-Bound 和 Thompson Sampling 已经成功地解决了单任务环境中的探索-开发权衡问题，但是现代推荐系统往往利用多种丰富的反馈来源，如点击，喜欢，不喜欢，分享，满意度调查反馈，并在实践中采用多任务学习。目前还不清楚如何将探索结合到具有不同目标的多任务设置中。本文研究了一种适用于多任务推荐系统的高效盗贼算法——多任务神经网络线性盗贼(mtNLB)。特别地，我们研究了在多任务设置中有效的特征嵌入，这些特征可以作为神经网络线性匪徒算法中的上下文特征，这种上下文匪徒算法很好地结合了 DNN 的表示能力和线性模型的不确定性计算的简单性。我们进一步研究了不确定性估计的成本效益近似，以及将不确定性纳入多任务项目评分的原则方法。为了展示我们提出的方法的有效性，我们在一个服务于数十亿用户的大规模商业推荐平台上进行了现场实验。我们评估不确定性估计的质量，并证明其能力，以改善探索的不同维度的奖励信号相比，基线方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Task+Neural+Linear+Bandit+for+Exploration+in+Recommender+Systems)|0|
|[Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems](https://doi.org/10.1145/3637528.3671580)|Jianping Wei, Yujie Zhou, Zhengwei Wu, Ziqi Liu|Ant Group, HangZhou, China|Large-scale search engines and recommendation systems utilize a three-stage cascading architecture-recall, pre-ranking, and ranking-to deliver relevant results within stringent latency limits. The pre-ranking stage is crucial for filtering a large number of recalled items into a manageable set for the ranking stage, greatly affecting the system's performance. Pre-ranking faces two intermediary challenges: Sample Selection Bias (SSB) arises when training is based on ranking stage feedback but the evaluation is on a broader recall dataset. Also, compared to the ranking stage, simpler pre-rank models may perform worse and less consistently. Traditional methods to tackle SSB issues include using all recall results and treating unexposed portions as negatives for training, which can be costly and noisy. To boost performance and consistency, some pre-ranking feature interaction enhancers don't fully fix consistency issues, while methods like knowledge distillation in ranking models ignore exposure bias. Our proposed framework targets these issues with three integral modules: Sample Selection, Domain Adaptation, and Unbiased Distillation. Sample Selection filters recall results to mitigate SSB and compute costs. Domain Adaptation enhances model robustness by assigning pseudo-labels to unexposed samples. Unbiased Distillation uses exposure-independent scores from Domain Adaptation to implement unbiased distillation for the pre-ranking model. The framework focuses on optimizing pre-ranking while maintaining training efficiency. We introduce new metrics for pre-ranking evaluation, while experiments confirm the effectiveness of our framework. Our framework is also deployed in real industrial systems.|大型搜索引擎和推荐系统利用三阶段级联架构——召回、预先排序和排序——在严格的延迟限制内交付相关结果。预排序阶段对于将大量被召回的项目过滤到一个可管理的集合中以进行排序至关重要，这极大地影响了系统的性能。预排序面临两个中间挑战: 样本选择偏差(SSB)出现时，训练是基于排序阶段的反馈，但评估是在一个更广泛的召回数据集。此外，与排名阶段相比，更简单的预排名模型可能表现得更差，更不一致。解决 SSB 问题的传统方法包括使用所有的召回结果，并将未暴露的部分作为培训的负面因素，这可能是昂贵和嘈杂的。为了提高性能和一致性，一些预排序特征交互增强器不能完全解决一致性问题，而排序模型中的知识提取等方法忽略了暴露偏差。我们提出的框架通过三个整体模块来解决这些问题: 样本选择、领域适应和无偏提取。示例选择过滤器召回结果以减少 SSB 和计算成本。领域自适应通过为未暴露的样本分配伪标签来增强模型的鲁棒性。无偏蒸馏使用领域适应的暴露无关分数实现预排序模型的无偏蒸馏。该框架的重点是优化预排序，同时保持培训效率。在实验的基础上，我们引入了新的指标来进行预排序评价，并验证了该框架的有效性。我们的框架也部署在真实的工业系统中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Pre-Ranking+Performance:+Tackling+Intermediary+Challenges+in+Multi-Stage+Cascading+Recommendation+Systems)|0|
|[Explicit and Implicit Modeling via Dual-Path Transformer for Behavior Set-informed Sequential Recommendation](https://doi.org/10.1145/3637528.3671755)|Ming Chen, Weike Pan, Zhong Ming|; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Shenzhen University & Shenzhen Technology University, Shenzhen, China|Sequential recommendation (SR) and multi-behavior sequential recommendation (MBSR) both come from real-world scenarios. Compared with SR, MBSR takes into account the dependencies of different behaviors. We find that most existing works on MBSR are studied in the context of e-commerce scenarios. In terms of the data format of the behavior types, we observe that the conventional label-formatted data carries limited information and is inadequate for scenarios like social media. With this observation, we introducebehavior set and extend MBSR to behavior set-informed sequential recommendation (BSSR). In BSSR, behavior dependencies become more complex and personalized, and user interest arousal may lack explicit contextual associations. To delve into the dynamics inhered within a behavior set and adaptively tailor recommendation lists upon its variability, we propose a novel solution called Explicit and Implicit modeling via Dual-Path Transformer (EIDP) for BSSR. Our EIDP adopts a dual-path architecture, distinguishing between explicit modeling path (EMP) and implicit modeling path (IMP) based on whether to directly incorporate the behavior representations. EMP features the personalized behavior set-wise transition pattern extractor (PBS-TPE) as its core component. It couples behavioral representations with both the items and positions to explore intra-behavior dynamics within a behavior set at a fine granularity. IMP utilizes light multi-head self-attention blocks (L-MSAB) as encoders under specific behavior types. The obtained multi-view representations are then aggregated by cross-behavior attention fusion (CBAF), using the behavior set of the next time step as a guidance to extract collaborative semantics at the behavioral level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our EIDP. We release the implementation code at: https://github.com/OshiNoCSMA/EIDP.|序贯推荐(SR)和多行为序贯推荐(MBSR)都来自于现实场景。与 SR 相比，MBSR 考虑了不同行为的依赖性。我们发现现有的大多数 MBSR 的研究工作都是在电子商务环境下进行的。就行为类型的数据格式而言，我们观察到传统的标签格式的数据携带有限的信息，对于像社交媒体这样的场景来说是不够的。在此基础上，我们引入了行为集合，并将 MBSR 扩展到行为集合知情序列推荐(BSSR)。在 BSSR，行为依赖变得更加复杂和个性化，用户兴趣唤起可能缺乏明确的上下文关联。为了深入研究行为集内部的动态性，并根据其可变性自适应调整推荐列表，我们提出了一种新的解决方案，即通过双路径转换器(EIDP)对 BSSR 进行显式和隐式建模。我们的 EIDP 采用双路径结构，根据是否直接合并行为表示，区分显式建模路径(EMP)和隐式建模路径(IMP)。EMP 以个性化行为集合过渡模式提取器(PBS-TPE)为核心组件。它将行为表示与项目和位置耦合起来，以便在一个细粒度的行为集内探索行为内动态。IMP 利用轻型多头自我注意块(L-MSAB)作为特定行为类型下的编码器。然后通过交叉行为注意融合(CBAF)对所获得的多视图表示进行聚合，利用下一个时间步骤的行为集作为指导，从行为层面提取协作语义。在两个实际数据集上的大量实验证明了我们的 EIDP 算法的有效性。我们在以下 https://github.com/oshinocsma/eidp 发布实现代码:。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicit+and+Implicit+Modeling+via+Dual-Path+Transformer+for+Behavior+Set-informed+Sequential+Recommendation)|0|
|[Disentangled Multi-interest Representation Learning for Sequential Recommendation](https://doi.org/10.1145/3637528.3671800)|Yingpeng Du, Ziyan Wang, Zhu Sun, Yining Ma, Hongzhi Liu, Jie Zhang|Nanyang Technological University, Singapore, Singapore; Peking University, Beijing, China; Singapore University of Technology and Design, Singapore, Singapore|Recently, much effort has been devoted to modeling users' multi-interests (aka multi-faceted preferences) based on their behaviors, aiming to accurately capture users' complex preferences. Existing methods attempt to model each interest of users through a distinct representation, but these multi-interest representations easily collapse into similar ones due to a lack of effective guidance. In this paper, we propose a generic multi-interest method for sequential recommendation, achieving disentangled representation learning of diverse interests technically and theoretically. To alleviate the collapse issue of multi-interests, we propose to conduct item partition guided by their likelihood of being co-purchased in a global view. It can encourage items in each group to focus on a discriminated interest, thus achieving effective disentangled learning of multi-interests. Specifically, we first prove the theoretical connection between item partition and spectral clustering, demonstrating its effectiveness in alleviating item-level and facet-level collapse issues that hinder existing disentangled methods. To efficiently optimize this problem, we then propose a Markov Random Field (MRF)-based method that samples small-scale sub-graphs from two separate MRFs, thus it can be approximated with a cross-entropy loss and optimized through contrastive learning. Finally, we perform multi-task learning to seamlessly align item partition learning with multi-interest modeling for more accurate recommendation. Experiments on three real-world datasets show that our method significantly outperforms state-of-the-art methods and can flexibly integrate with existing multi-interest models as a plugin to enhance their performances.|近年来，人们致力于根据用户的行为建立用户的多重兴趣(即多方面偏好)模型，以准确捕捉用户的复杂偏好。现有的方法试图通过一个不同的表示来对用户的每个兴趣进行建模，但是由于缺乏有效的指导，这些多兴趣表示很容易崩溃成为相似的表示。本文提出了一种通用的多兴趣序列推荐方法，从理论和技术上实现了不同兴趣的分离表示学习。为了缓解多重利益的崩溃问题，我们提出在全局视角下，以多重利益共同购买的可能性为指导进行项目分割。它可以鼓励项目在每个组集中在一个歧视性的兴趣，从而实现有效的多兴趣分离学习。具体来说，我们首先证明了项目划分和 SVD 之间的理论联系，证明了它在缓解阻碍现有分离方法的项目级和面级崩溃问题方面的有效性。为了有效地优化这个问题，我们提出了一个基于马尔可夫网络(MRF)的方法，从两个不同的 MRF 中抽取小规模子图，因此它可以用交叉熵损失近似，并通过对比学习进行优化。最后，我们进行多任务学习，使项目划分学习与多兴趣模型无缝对齐，以获得更准确的推荐。在三个实际数据集上的实验表明，该方法的性能明显优于最新的方法，并且可以灵活地与现有的多兴趣模型集成，作为一个插件来提高它们的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Multi-interest+Representation+Learning+for+Sequential+Recommendation)|0|
|[Continual Collaborative Distillation for Recommender System](https://doi.org/10.1145/3637528.3671924)|Gyuseok Lee, SeongKu Kang, Wonbin Kweon, Hwanjo Yu|Pohang University of Science and Technology, Pohang, Gyeongsangbuk-do, Republic of Korea; University of Illinois Urbana-Champaign, Champaign, Illinois, USA|Knowledge distillation (KD) has emerged as a promising technique foraddressing the computational challenges associated with deploying large-scalerecommender systems. KD transfers the knowledge of a massive teacher system toa compact student model, to reduce the huge computational burdens for inferencewhile retaining high accuracy. The existing KD studies primarily focus onone-time distillation in static environments, leaving a substantial gap intheir applicability to real-world scenarios dealing with continuously incomingusers, items, and their interactions. In this work, we delve into a systematicapproach to operating the teacher-student KD in a non-stationary data stream.Our goal is to enable efficient deployment through a compact student, whichpreserves the high performance of the massive teacher, while effectivelyadapting to continuously incoming data. We propose Continual CollaborativeDistillation (CCD) framework, where both the teacher and the studentcontinually and collaboratively evolve along the data stream. CCD facilitatesthe student in effectively adapting to new data, while also enabling theteacher to fully leverage accumulated knowledge. We validate the effectivenessof CCD through extensive quantitative, ablative, and exploratory experiments ontwo real-world datasets. We expect this research direction to contribute tonarrowing the gap between existing KD studies and practical applications,thereby enhancing the applicability of KD in real-world systems.|知识精馏(KD)已经成为解决部署大规模推荐系统所面临的计算挑战的一种有前途的技术。KD 将大规模教师系统的知识转移到一个紧凑的学生模型，以减少推理的巨大计算负担，同时保持高精度。现有的 KD 研究主要集中在静态环境中的一次性提取，在处理不断进入的用户、项目及其交互的实际场景的适用性方面留下了很大的空白。在这项工作中，我们深入探讨了一个系统的方法来运行师生知识发展在一个非平稳的数据流。我们的目标是通过一个紧凑的学生实现有效的部署，这样可以保持大规模教师的高性能，同时有效地适应不断传入的数据。我们提出了持续协作蒸馏(CCD)框架，在这个框架中，教师和学生沿着数据流不断地、协作地发展。CCD 帮助学生有效地适应新的数据，同时也使教师能够充分利用积累的知识。我们通过在两个实际数据集上进行广泛的定量、消融和探索性实验，验证了 CCD 的有效性。我们期望这一研究方向能够缩小现有 KD 研究与实际应用之间的差距，从而提高 KD 在现实世界系统中的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continual+Collaborative+Distillation+for+Recommender+System)|0|
|[Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement](https://doi.org/10.1145/3637528.3671799)|Zijian Song, Wenhan Zhang, Lifang Deng, Jiandong Zhang, Zhihua Wu, Kaigui Bian, Bin Cui|; Lazada Group, Beijing, China|Cross-Domain Recommendation (CDR) is a promising technique to alleviate data sparsity by transferring knowledge across domains. However, the negative transfer issue in the presence of numerous domains has received limited attention. Most existing methods transfer all information from source domains to the target domain without distinction. This introduces harmful noise and irrelevant features, resulting in suboptimal performance. Although some methods decompose user features into domain-specific and domain-shared components, they fail to consider other causes of negative transfer. Worse still, we argue that simple feature decomposition is insufficient for multi-domain scenarios. To bridge this gap, we propose TrineCDR, the TRIple-level kNowledge transferability Enhanced model for multi-target CDR. Unlike previous methods, TrineCDR captures single domain and targeted cross-domain embeddings to serve multi-domain recommendation. For the latter, we identify three fundamental causes of negative transfer, ranging from micro to macro perspectives, and correspondingly enhance knowledge transferability at three different levels: the feature level, the interaction level, and the domain level. Through these efforts, TrineCDR effectively filters out noise and irrelevant information from source domains, leading to more comprehensive and accurate representations in the target domain. We extensively evaluate the proposed model on real-world datasets, sampled from Amazon and Douban, under both dual-target and multi-target scenarios. The experimental results demonstrate the superiority of TrineCDR over state-of-the-art cross-domain recommendation methods.|跨域推荐(CDR)是一种通过跨域传输知识来缓解数据稀疏性的有前途的技术。然而，在众多领域存在的负迁移问题受到的关注有限。大多数现有的方法不加区别地将所有信息从源域传输到目标域。这会引入有害的噪音和不相关的特征，导致性能不理想。尽管有些方法将用户特性分解为特定于领域和共享领域的组件，但它们没有考虑到负迁移的其他原因。更糟糕的是，我们认为简单的特征分解对于多领域场景是不够的。为了弥补这一差距，我们提出了 TrineCDR，一种针对多目标 CDR 的 TRIple-level 知识可转移性增强模型。与以前的方法不同，TrineCDR 捕获单个域和目标跨域嵌入，以服务于多域推荐。对于后者，我们从微观到宏观的角度找出了负迁移的三个基本原因，并相应地在三个不同的层面上提高了知识的可迁移性: 特征层面、交互层面和领域层面。通过这些努力，TrineCDR 有效地过滤掉源域中的噪声和不相关信息，从而在目标域中实现更全面、更准确的表示。在双目标和多目标情景下，我们对亚马逊和豆瓣采样的真实世界数据集上提出的模型进行了广泛的评估。实验结果表明 TrineCDR 方法优于目前最先进的跨域推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Negative+Transfer+in+Cross-Domain+Recommendation+via+Knowledge+Transferability+Enhancement)|0|
|[Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model](https://doi.org/10.1145/3637528.3671572)|Yanjie Gou, Yuanzhou Yao, Zhao Zhang, Yiqing Wu, Yi Hu, Fuzhen Zhuang, Jiangming Liu, Yongjun Xu|; Common Data Platform, Tencent, Shenzhen, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Information Science and Engineering, Yunnan University, Kunming, China; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China|Online games often house virtual shops where players can acquire character skins. Our task is centered on tailoring skin recommendations across diverse scenarios by analyzing historical interactions such as clicks, usage, and purchases. Traditional multi-behavior recommendation models employed for this task are limited. They either only predict skins based on a single type of behavior or merely recommend skins for target behavior type/task. These models lack the ability to control predictions of skins that are associated with different scenarios and behaviors. To overcome these limitations, we utilize the pretraining capabilities of Large Sequential Models (LSMs) coupled with a novel stimulus prompt mechanism and build a controllable multi-behavior recommendation (CMBR) model. In our approach, the pretraining ability is used to encapsulate users' multi-behavioral sequences into the representation of users' general interests. Subsequently, our designed stimulus prompt mechanism stimulates the model to extract scenario-related interests, thus generating potential skin purchases (or clicks and other interactions) for users. To the best of our knowledge, this is the first work to provide controlled multi-behavior recommendations, and also the first to apply the pretraining capabilities of LSMs in game domain. Through offline experiments and online A/B tests, we validate our method significantly outperforms baseline models, exhibiting about a tenfold improvement on various metrics during the offline test.|在线游戏通常会有虚拟商店，玩家可以在那里获得角色皮肤。我们的任务是通过分析点击、使用和购买等历史交互，跨不同场景裁剪皮肤推荐。传统的用于此任务的多行为推荐模型是有限的。它们要么只根据单一类型的行为预测皮肤，要么只为目标行为类型/任务推荐皮肤。这些模型缺乏控制与不同场景和行为相关联的皮肤预测的能力。为了克服这些局限性，我们利用大序列模型(LSM)的预训练能力，结合一种新颖的刺激提示机制，建立了一个可控的多行为推荐(CMBR)模型。该方法利用预训练能力将用户的多行为序列封装成用户兴趣的表示。随后，我们设计的刺激提示机制刺激模型以提取场景相关的兴趣，从而为用户产生潜在的皮肤购买(或点击和其他交互)。据我们所知，这是第一个提供可控的多行为建议的工作，也是第一个应用在游戏领域的 LSM 的预训练能力。通过离线实验和在线 A/B 测试，我们验证了我们的方法明显优于基线模型，在离线测试期间在各种指标上表现出大约10倍的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Controllable+Multi-Behavior+Recommendation+for+In-Game+Skins+with+Large+Sequential+Model)|0|
|[Multi-objective Learning to Rank by Model Distillation](https://doi.org/10.1145/3637528.3671597)|Jie Tang, Huiji Gao, Liwei He, Sanjeev Katariya|Airbnb, San Francisco, CA, USA|In online marketplaces, search ranking's objective is not only to purchase or conversion (primary objective), but to also the purchase outcomes(secondary objectives), e.g. order cancellation(or return), review rating, customer service inquiries, platform long term growth. Multi-objective learning to rank has been widely studied to balance primary and secondary objectives. But traditional approaches in industry face some challenges including expensive parameter tuning leads to sub-optimal solution, suffering from imbalanced data sparsity issue, and being not compatible with ad-hoc objective. In this paper, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives along with various considerations to optimize training and serving efficiency to meet industry standards. We found it performs much better than traditional approaches, it doesn't only significantly increases primary objective by a large margin but also meet secondary objectives constraints and improve model stability. We also demonstrated the proposed system could be further simplified by model self-distillation. Besides this, we did additional simulations to show that this approach could also help us efficiently inject ad-hoc non-differentiable business objective into the ranking system while enabling us to balance our optimization objectives.|在在线市场中，搜索排名的目标不仅仅是购买或转换(主要目标) ，还包括购买结果(次要目标) ，例如取消订单(或返回) ，评论等级，客户服务查询，平台长期增长。多目标排序学习已被广泛研究，以平衡小学和中学的目标。但是，传统的方法在工业上面临着一些挑战，包括参数调整费用昂贵、数据稀疏性不平衡、与特定目标不兼容等问题。针对多目标排序问题，提出了一种基于精馏的排序方法，该方法在不同目标的多个排序模型上对 Airbnb 的端到端排序系统进行优化，同时考虑各种因素，优化培训和服务效率，以满足行业标准。我们发现它比传统的方法有更好的性能，它不仅大幅度增加了主要目标，而且满足次要目标的约束，提高了模型的稳定性。我们还证明了模型自蒸馏可以进一步简化所提出的体系。除此之外，我们做了额外的模拟，以表明这种方法也可以帮助我们有效地注入临时不可微的业务目标到排名系统，同时使我们能够平衡我们的优化目标。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-objective+Learning+to+Rank+by+Model+Distillation)|0|
|[Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation](https://doi.org/10.1145/3637528.3671519)|Yuting Zhang, Yiqing Wu, Ruidong Han, Ying Sun, Yongchun Zhu, Xiang Li, Wei Lin, Fuzhen Zhuang, Zhulin An, Yongjun Xu|; Meituan, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China|Recommendation systems, which assist users in discovering their preferred items among numerous options, have served billions of users across various online platforms. Intuitively, users' interactions with items are highly driven by their unchanging inherent intents (e.g., always preferring high-quality items) and changing demand intents (e.g., wanting a T-shirt in summer but a down jacket in winter). However, both types of intents are implicitly expressed in recommendation scenario, posing challenges in leveraging them for accurate intent-aware recommendations. Fortunately, in search scenario, often found alongside recommendation on the same online platform, users express their demand intents explicitly through their query words. Intuitively, in both scenarios, a user shares the same inherent intent and his/her interactions may be influenced by the same demand intent. It is therefore feasible to utilize the interaction data from both scenarios to reinforce the dual intents for joint intent-aware modeling. But the joint modeling should deal with two problems: (1) accurately modeling users' implicit demand intents in recommendation; (2) modeling the relation between the dual intents and the interactive items. To address these problems, we propose a novel model named Unified Dual-Intents Translation for joint modeling of Search and Recommendation (UDITSR). To accurately simulate users' demand intents in recommendation, we utilize real queries from search data as supervision information to guide its generation. To explicitly model the relation among the triplet , we propose a dual-intent translation propagation mechanism to learn the triplet in the same semantic space via embedding translations. Extensive experiments demonstrate that UDITSR outperforms SOTA baselines both in search and recommendation tasks. Moreover, our model has been deployed online on Meituan Waimai platform, leading to an average improvement in GMV (Gross Merchandise Value) of 1.46% and CTR(Click-Through Rate) of 0.77% over one month.|推荐系统帮助用户在众多选项中发现他们喜欢的项目，已经在各种在线平台上为数十亿用户服务。直觉上，用户与物品的互动高度受到他们不变的内在意图(例如，总是喜欢高质量的物品)和不断变化的需求意图(例如，夏天想要一件 T 恤，冬天想要一件羽绒服)的驱动。然而，这两种类型的意图都隐式地在推荐场景中表达，在利用它们获得准确的意图感知建议方面提出了挑战。幸运的是，在搜索场景中，用户通过他们的查询词明确地表达他们的需求意图，这种情况经常在同一个在线平台的推荐旁边发现。直观地说，在这两种情况下，用户共享相同的内在意图，他/她的交互可能受到相同的需求意图的影响。因此，利用来自两个场景的交互数据来加强联合意图感知建模的双重意图是可行的。但是联合建模需要解决两个问题: (1)准确地建立推荐中用户隐含需求意图的模型; (2)建立双重意图与交互项目之间的关系模型。针对这些问题，本文提出了一种新的联合建模模型——统一双意图翻译模型(UDITSR)。为了准确地模拟推荐中用户的需求意图，我们利用搜索数据中的实际查询作为监督信息来指导推荐的生成。为了明确地模拟三联体之间的关系，我们提出了一种双意图翻译传播机制，通过嵌入翻译来学习同一语义空间中的三联体。大量的实验表明，UDITSR 在搜索和推荐任务中都优于 SOTA 基线。此外，我们的模型已经在 Waimai 的美团平台上使用，导致一个月内平均商品总值(GMV)提高了1.46% ，点击率(点进率)提高了0.77% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Dual-Intent+Translation+for+Joint+Modeling+of+Search+and+Recommendation)|0|
|[Shopping Trajectory Representation Learning with Pre-training for E-commerce Customer Understanding and Recommendation](https://doi.org/10.1145/3637528.3671747)|Yankai Chen, QuocTuan Truong, Xin Shen, Jin Li, Irwin King|The Chinese University of Hong Kong, Hong Kong, China; Amazon, Seattle, WA, USA|Understanding customer behavior is crucial for improving service quality in large-scale E-commerce. This paper proposes C-STAR, a new framework that learns compact representations from customer shopping journeys, with good versatility to fuel multiple downstream customer-centric tasks. We define the notion of shopping trajectory that encompasses customer interactions at the level of product categories, capturing the overall flow of their browsing and purchase activities. C-STAR excels at modeling both inter-trajectory distribution similarity-the structural similarities between different trajectories, and intra-trajectory semantic correlation-the semantic relationships within individual ones. This coarse-to-fine approach ensures informative trajectory embeddings for representing customers. To enhance embedding quality, we introduce a pre-training strategy that captures two intrinsic properties within the pre-training data. Extensive evaluation on large-scale industrial and public datasets demonstrates the effectiveness of C-STAR across three diverse customer-centric tasks. These tasks empower customer profiling and recommendation services for enhancing personalized shopping experiences on our E-commerce platform.|了解顾客行为是提高大规模电子商务服务质量的关键。本文提出了一个新的框架 C-STAR，它从顾客购物旅程中学习紧凑的表示，具有良好的通用性，可以为多个下游顾客中心任务提供支持。我们定义了购物轨迹的概念，它包含了产品类别层面的客户交互，捕捉了他们浏览和购买活动的总体流程。C-STAR 在建立轨迹间分布相似性(不同轨迹之间的结构相似性)和轨迹内语义相关性(各个轨迹之间的语义关系)两方面都表现出色。这种从粗到细的方法确保为代表客户嵌入信息轨迹。为了提高嵌入质量，我们引入了一种预训练策略，捕获预训练数据的两个内在属性。对大规模工业和公共数据集的广泛评估证明了 C-STAR 在三种不同的以客户为中心的任务中的有效性。这些任务授权客户档案和推荐服务，以提高我们的电子商务平台上的个性化购物体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shopping+Trajectory+Representation+Learning+with+Pre-training+for+E-commerce+Customer+Understanding+and+Recommendation)|0|
|[DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation](https://doi.org/10.1145/3637528.3671669)|Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, Jiwei Li|Zhejiang University & Shanghai Institute for Advanced Study of Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China|Due to the continuously improving capabilities of mobile edges, recommendersystems start to deploy models on edges to alleviate network congestion causedby frequent mobile requests. Several studies have leveraged the proximity ofedge-side to real-time data, fine-tuning them to create edge-specific models.Despite their significant progress, these methods require substantial on-edgecomputational resources and frequent network transfers to keep the model up todate. The former may disrupt other processes on the edge to acquirecomputational resources, while the latter consumes network bandwidth, leadingto a decrease in user satisfaction. In response to these challenges, we proposea customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploysthe same generic backbone (potentially incompatible for a specific edge) to alldevices. To minimize frequent bandwidth usage and storage consumption inpersonalization, DIET tailors specific subnets for each edge based on its pastinteractions, learning to generate slimming subnets(diets) within incompatiblenetworks for efficient transfer. It also takes the inter-layer relationshipsinto account, empirically reducing inference time while obtaining more suitablediets. We further explore the repeated modules within networks and propose amore storage-efficient framework, DIETING, which utilizes a single layer ofparameters to represent the entire network, achieving comparably excellentperformance. The experiments across four state-of-the-art datasets and twowidely used models demonstrate the superior accuracy in recommendation andefficiency in transmission and storage of our framework.|由于移动边缘功能的不断改进，推荐系统开始在边缘部署模型，以缓解频繁的移动请求造成的拥塞控制。一些研究已经利用边缘接近实时数据，微调它们以创建边缘特定的模型。尽管这些方法取得了显著的进展，但它们需要大量的边缘计算资源和频繁的网络传输来保持模型的最新性。前者可能破坏边缘的其他进程以获取计算资源，而后者消耗网络带宽，导致用户满意度下降。为了应对这些挑战，我们提出了针对不兼容网络(DIET)的定制减肥框架。DIET 为所有设备部署相同的通用主干网(对于特定的边缘可能不兼容)。为了尽量减少频繁的带宽使用和个性化存储消耗，DIET 根据其过去的相互作用为每个边裁剪特定的子网，学习在不兼容的网络中生成减肥子网(饮食)以便有效地传输。它还考虑到层间关系，通过实验减少推断时间，同时获得更合适的饮食。我们进一步探讨了网络中的重复模块，并提出了一种存储效率更高的框架—— DIETING，它利用一个单一的参数层来表示整个网络，取得了较好的性能。通过四个最先进的数据集和两个广泛使用的模型进行的实验表明，我们的框架在传输和存储方面具有优越的推荐准确性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIET:+Customized+Slimming+for+Incompatible+Networks+in+Sequential+Recommendation)|0|
|[Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System](https://doi.org/10.1145/3637528.3671931)|Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, MinChul Yang, Chanyoung Park|NAVER Corporation, Seongnam, Republic of Korea; KAIST, Daejeon, Republic of Korea|Collaborative filtering recommender systems (CF-RecSys) have shown successiveresults in enhancing the user experience on social media and e-commerceplatforms. However, as CF-RecSys struggles under cold scenarios with sparseuser-item interactions, recent strategies have focused on leveraging modalityinformation of user/items (e.g., text or images) based on pre-trained modalityencoders and Large Language Models (LLMs). Despite their effectiveness undercold scenarios, we observe that they underperform simple traditionalcollaborative filtering models under warm scenarios due to the lack ofcollaborative knowledge. In this work, we propose an efficient All-roundLLM-based Recommender system, called A-LLMRec, that excels not only in the coldscenario but also in the warm scenario. Our main idea is to enable an LLM todirectly leverage the collaborative knowledge contained in a pre-trainedstate-of-the-art CF-RecSys so that the emergent ability of the LLM as well asthe high-quality user/item embeddings that are already trained by thestate-of-the-art CF-RecSys can be jointly exploited. This approach yields twoadvantages: (1) model-agnostic, allowing for integration with various existingCF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typicallyrequired for LLM-based recommenders. Our extensive experiments on variousreal-world datasets demonstrate the superiority of A-LLMRec in variousscenarios, including cold/warm, few-shot, cold user, and cross-domainscenarios. Beyond the recommendation task, we also show the potential ofA-LLMRec in generating natural language outputs based on the understanding ofthe collaborative knowledge by performing a favorite genre prediction task. Ourcode is available at https://github.com/ghdtjr/A-LLMRec .|协同过滤推荐系统(CF-recsys)在增强社交媒体和电子商务平台的用户体验方面取得了成功。然而，由于 CF-RecSys 在冷场景下与稀疏用户-项目交互的斗争，最近的策略集中在基于预先训练的 modalityencoders 和 Large Language Model (LLM)利用用户/项目(例如文本或图像)的模态信息。尽管它们在低温情景下有效，但是我们观察到，由于缺乏协作知识，它们在温暖情景下表现不如传统的简单协作过滤模型。在这项工作中，我们提出了一个高效的基于全局 LLM 的推荐系统，称为 A-LLmrec，它不仅在冷场景中表现出色，而且在暖场景中也表现出色。我们的主要想法是使 LLM 能够直接利用包含在预先培训的最先进的 CF-RecSys 中的协作知识，以便 LLM 的应急能力以及已经由最先进的 CF-RecSys 培训的高质量用户/项目嵌入能够被共同利用。这种方法有两个优点: (1)模型无关，允许与各种现有的 CF-RecSys 集成; (2)效率，消除了基于 LLM 的推荐程序通常需要的广泛的微调。我们在各种真实世界数据集上的广泛实验证明了 A-LLMRec 在各种场景下的优越性，包括冷/温、少拍摄、冷用户和跨域场景。除了推荐任务，我们还展示了 A-LLMRec 的潜力，通过执行一个喜欢的体裁预测任务，基于对协作知识的理解来生成自然语言输出。我们的代码可以在 https://github.com/ghdtjr/a-llmrec 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+meet+Collaborative+Filtering:+An+Efficient+All-round+LLM-based+Recommender+System)|0|
|[Probabilistic Attention for Sequential Recommendation](https://doi.org/10.1145/3637528.3671733)|Yuli Liu, Christian Walder, Lexing Xie, Yiqun Liu|; Google Research, Brain Team, Montreal, Canada; Australian National University & Data61 CSIRO, Canberra, Australia|Sequential Recommendation (SR) navigates users' dynamic preferences through modeling their historical interactions. The incorporation of the popular Transformer framework, which captures long relationships through pairwise dot products, has notably benefited SR. However, prevailing research in this domain faces three significant challenges: (i) Existing studies directly adopt the primary component of Transformer (i.e., the self-attention mechanism), without a clear explanation or tailored definition for its specific role in SR; (ii) The predominant focus on pairwise computations overlooks the global context or relative prevalence of item pairs within the overall sequence; (iii) Transformer primarily pursues relevance-dominated relationships, neglecting another essential objective in recommendation, i.e., diversity. In response, this work introduces a fresh perspective to elucidate the attention mechanism in SR. Here, attention is defined as dependency interactions among items, quantitatively determined under a global probabilistic model by observing the probabilities of corresponding item subsets. This viewpoint offers a precise and context-specific definition of attention, leading to the design of a distinctive attention mechanism tailored for SR. Specifically, we transmute the well-formulated global, repulsive interactions in Determinantal Point Processes (DPPs) to effectively model dependency interactions. Guided by the repulsive interactions, a theoretically and practically feasible DPP kernel is designed, enabling our attention mechanism to directly consider category/topic distribution for enhancing diversity. Consequently, the Probabilistic Attention mechanism (PAtt) for sequential recommendation is developed. Experimental results demonstrate the excellent scalability and adaptability of our attention mechanism, which significantly improves recommendation performance in terms of both relevance and diversity.|顺序推荐(SR)通过建模用户的历史交互来导航用户的动态偏好。通过成对点产品捕获长关系的流行的 Transformer 框架的结合，使 SR 受益匪浅。然而，这个领域的主流研究面临三个重大挑战: (i)现有的研究直接采用 Transformer 的主要组成部分(即自我注意机制) ，没有明确的解释或量身定制的定义其在 SR 中的具体作用; (ii)成对计算的主要重点忽视了整体序列中项目对的全局上下文或相对流行程度; (iii) Transformer 主要追求相关性主导的关系，忽视了推荐中的另一个基本目标，即多样性。作为回应，本文引入了一个新的视角来阐明注意机制。这里，注意被定义为项目之间的依赖交互作用，在全局概率模型下通过观察相应项目子集的概率来定量确定。这一观点提供了一个精确的和具体的上下文关注的定义，导致了一个独特的注意机制的设计专门为 SR。具体来说，我们转换了确定性点过程(DPP)中的良好制定的全局排斥交互作用，以有效地建模依赖交互作用。在排斥交互作用的指导下，设计了一个理论上和实际上可行的 DPP 核，使我们的注意机制能够直接考虑类别/主题分布，从而增强多样性。为此，提出了序贯推荐的概率注意机制。实验结果表明，该注意机制具有良好的可扩展性和适应性，在相关性和多样性方面显著提高了推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Probabilistic+Attention+for+Sequential+Recommendation)|0|
|[Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations](https://doi.org/10.1145/3637528.3671743)|Jing Long, Guanhua Ye, Tong Chen, Yang Wang, Meng Wang, Hongzhi Yin|The University of Queensland, Brisbane, Australia; Beijing University of Posts and Telecommunications, BeiJing, China; Hefei University of Technology, Hefei, China|The rapid expansion of Location-Based Social Networks (LBSNs) has highlightedthe importance of effective next Point-of-Interest (POI) recommendations, whichleverage historical check-in data to predict users' next POIs to visit.Traditional centralized deep neural networks (DNNs) offer impressive POIrecommendation performance but face challenges due to privacy concerns andlimited timeliness. In response, on-device POI recommendations have beenintroduced, utilizing federated learning (FL) and decentralized approaches toensure privacy and recommendation timeliness. However, these methods oftensuffer from computational strain on devices and struggle to adapt to new usersand regions. This paper introduces a novel collaborative learning framework,Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POIRecommendations (DCPR), leveraging the diffusion model known for its successacross various domains. DCPR operates with a cloud-edge-device architecture tooffer region-specific and highly personalized POI recommendations whilereducing on-device computational burdens. DCPR minimizes on-devicecomputational demands through a unique blend of global and local learningprocesses. Our evaluation with two real-world datasets demonstrates DCPR'ssuperior performance in recommendation accuracy, efficiency, and adaptabilityto new users and regions, marking a significant step forward in on-device POIrecommendation technology.|基于位置的社交网络(LBSNs)的快速扩张突出了有效的下一个兴趣点(POI)建议的重要性，这些建议利用历史签入数据来预测用户下一个访问的 POI。传统的集中式深层神经网络(DNN)提供了令人印象深刻的 POI 推荐性能，但由于隐私问题和有限的时间面临挑战。作为回应，在设备上的 POI 推荐已经被引入，利用联邦学习(FL)和分散的方法来确保隐私和推荐的及时性。然而，这些方法经常受到设备计算压力的影响，难以适应新的用户和地区。本文介绍了一个新的合作学习框架，基于扩散的云端设备合作学习，用于下一个 POI 建议(DCPR) ，利用扩散模型在各个领域的成功。DCPR 采用云端设备架构，可以提供区域特定的高度个性化的 POI 建议，同时减少设备上的计算负担。DCPR 通过独特的全球和本地学习过程的混合，最大限度地减少了设备上的计算需求。我们用两个真实世界的数据集进行的评估表明，DCPR 在推荐准确性、效率以及对新用户和地区的适应性方面具有优越的性能，这标志着在设备 POI 推荐技术方面向前迈进了一大步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion-Based+Cloud-Edge-Device+Collaborative+Learning+for+Next+POI+Recommendations)|0|
|[Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range](https://doi.org/10.1145/3637528.3671852)|Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, Junchi Yan|SNMC, Tianjin University, Tianjin, China; SIAS and College of Computer Science, Zhejiang University, Hangzhou, China; Department of CSE, Shanghai Jiao Tong University, Shanghai, China; Department of CSE and MoE Key Lab of AI, Shanghai Jiao Tong University, Shanghai, China; School of AI and Department of CSE, Shanghai Jiao Tong University, Shanghai, China|Deep visual graph matching (GM) is a challenging combinatorial task that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images. Like many learning systems, empirical studies have shown that visual GM is susceptible to adversarial attacks, with reliability issues in downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge with two main difficulties: how to handle the paired inputs together with the heavily non-linear permutation output space (especially at large scale), and how to balance the trade-off between certified robustness and matching performance. Inspired by the randomized smoothing (RS) technique, we propose the Certified Robustness based on the Optimal Smoothing Range Search (CR-OSRS) technique to fulfill the robustness guarantee for deep visual GM. First, unlike conventional RS methods that use isotropic Gaussian distributions for smoothing, we build the smoothed model with paired joint Gaussian distributions, which capture the structural information among keypoints, and mitigate the performance degradation caused by smoothing. For the vast space of the permutation output, we devise a similarity-based partitioning method that can lower the computational complexity and certification difficulty. We then derive a stringent robustness guarantee that links the certified space of inputs to their corresponding fixed outputs. Second, we design a global optimization method to search for optimal joint Gaussian distributions and facilitate a larger certified space and better performance. Third, we apply data augmentation and a similarity-based regularizer in training to enhance smoothed model performance. Lastly, for the high-dimensional and multivariable nature of the certified space, we propose two methods (sampling and marginal radii) to evaluate it. Experimental results on public benchmarks show that our method achieves state-of-the-art certified robustness.|深度视觉图形匹配(GM)是一项具有挑战性的组合任务，包括从一对图像中找到一个指示关键点之间对应关系的置换矩阵。与许多学习系统一样，经验研究表明，视觉 GM 易受敌对攻击，在下游应用中存在可靠性问题。据我们所知，深度视觉 GM 的鲁棒性认证仍然是一个公开的挑战，有两个主要的困难: 如何处理配对输入和严重的非线性排列输出空间(特别是在大规模) ，以及如何平衡之间的权衡认证鲁棒性和匹配性能。受随机平滑(RS)技术的启发，我们提出了基于最优平滑范围搜索(CR-OSRS)技术的认证鲁棒性，以实现深层视觉 GM 的鲁棒性保证。首先，不同于传统的 RS 方法使用各向同性高斯分布进行平滑，我们建立了平滑模型与配对联合高斯分布，捕捉关键点之间的结构信息，并减轻平滑造成的性能下降。针对排列输出的巨大空间，提出了一种基于相似度的划分方法，降低了计算复杂度和认证难度。然后，我们推导出一个严格的鲁棒性保证，将经过验证的输入空间与它们相应的固定输出联系起来。其次，我们设计了一个全局优化方法来寻找最佳的联合高斯分布，使得更大的认证空间和更好的性能。第三，在训练中应用数据增强和基于相似性的正则化方法来提高平滑模型的性能。最后，针对证明空间的高维性和多变量性，提出了两种评价方法(抽样法和边缘半径法)。对公共基准测试的实验结果表明，该方法具有较好的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Certified+Robustness+on+Visual+Graph+Matching+via+Searching+Optimal+Smoothing+Range)|0|
|[Pre-Training with Transferable Attention for Addressing Market Shifts in Cross-Market Sequential Recommendation](https://doi.org/10.1145/3637528.3671698)|Chen Wang, Ziwei Fan, Liangwei Yang, Mingdai Yang, Xiaolong Liu, Zhiwei Liu, Philip S. Yu|Salesforce AI Research, Palo Alto, CA, USA; The University of Chicago, Chicago, IL, USA; Amazon, Santa Clara, CA, USA; Tsinghua University, Beijing, China; University of Illinois Chicago, Chicago, IL, USA|Cross-market recommendation (CMR) involves selling the same set of items across multiple nations or regions within a transfer learning framework. However, CMR's distinctive characteristics, including limited data sharing due to privacy policies, absence of user overlap, and a shared item set between markets present challenges for traditional recommendation methods. Moreover, CMR experiences market shifts, leading to differences in item popularity and user preferences among different markets. This study focuses on cross-market sequential recommendation (CMSR) and proposes the Cross-market Attention Transferring with Sequential Recommendation (CAT-SR) framework to address these challenges and market shifts. CAT-SR incorporates a pre-training strategy emphasizing item-item correlation, selective self-attention transferring for effective transfer learning, and query and key adapters for market-specific user preferences. Experimental results on real-world cross-market datasets demonstrate the superiority of CAT-SR, and ablation studies validate the benefits of its components across different geographical continents. CAT-SR offers a robust and adaptable solution for cross-market sequential recommendation. The code is available at https://github.com/ChenMetanoia/CATSR-KDD/.|跨市场推荐(CMR)涉及在一个迁移学习框架内在多个国家或地区销售同一套产品。然而，CMR 的显著特点，包括由于隐私政策导致的有限数据共享、用户重叠的缺失以及市场之间的共享项目集，对传统的推荐方法提出了挑战。此外，CMR 经历了市场变化，导致不同市场之间的产品流行度和用户偏好的差异。本研究以跨市场序贯推荐(CMSR)为研究对象，提出了基于序贯推荐的跨市场注意力转移(CAT-SR)框架，以解决这些挑战和市场转移问题。CAT-SR 包括一个强调项目-项目相关性的预训练策略，选择性自我注意转移以有效转移学习，以及针对特定市场用户偏好的查询和关键适配器。在现实世界跨市场数据集上的实验结果证明了 CAT-SR 的优越性，并且消融研究验证了其组件在不同地理大陆上的优势。CAT-SR 为跨市场连续推荐提供了一个健壮的、适应性强的解决方案。密码可在 https://github.com/chenmetanoia/catsr-kdd/查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+with+Transferable+Attention+for+Addressing+Market+Shifts+in+Cross-Market+Sequential+Recommendation)|0|
|[Dataset Regeneration for Sequential Recommendation](https://doi.org/10.1145/3637528.3671841)|Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, Enhong Chen|; Huawei Singapore Research Center, Singapore, Singapore|The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at https://github.com/USTC-StarTeam/DR4SR.|顺序推荐(SR)系统是现代推荐系统的重要组成部分，因为它旨在捕获用户不断变化的偏好。为提高 SR 系统的性能已经做出了重大努力。这些方法通常遵循以模型为中心的范式，其中包括基于固定数据集开发有效的模型。然而，这种方法常常忽略数据中潜在的质量问题和固有缺陷。在以数据为中心的人工智能的潜力驱动下，我们提出了一种新的以数据为中心的范式，用于开发一个理想的训练数据集，使用一个模型无关的数据集再生框架 DR4SR。这个框架使得数据集的再生具有异常的跨架构通用性。此外，我们还介绍了 DR4SR + 框架，该框架结合了一个模型感知的数据集个性化工具，可以专门为目标模型定制重新生成的数据集。为了证明以数据为中心的范式的有效性，我们将我们的框架与各种以模型为中心的方法集成在一起，并观察四个广泛采用的数据集的显著性能改进。此外，我们进行深入的分析，以探索潜在的数据为中心的范式，并提供有价值的见解。密码可以在 https://github.com/ustc-starteam/dr4sr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+Regeneration+for+Sequential+Recommendation)|0|
|[GPFedRec: Graph-Guided Personalization for Federated Recommendation](https://doi.org/10.1145/3637528.3671702)|Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang|; Computer Science and UMIACS, University of Maryland, Maryland, USA|The federated recommendation system is an emerging AI service architecture that provides recommendation services in a privacy-preserving manner. Using user-relation graphs to enhance federated recommendations is a promising topic. However, it is still an open challenge to construct the user-relation graph while preserving data locality-based privacy protection in federated settings. Inspired by a simple motivation, similar users share a similar vision (embeddings) to the same item set, this paper proposes a novel Graph-guided Personalization for Federated Recommendation (GPFedRec). The proposed method constructs a user-relation graph from user-specific personalized item embeddings at the server without accessing the users' interaction records. The personalized item embedding is locally fine-tuned on each device, and then a user-relation graph will be constructed by measuring the similarity among client-specific item embeddings. Without accessing users' historical interactions, we embody the data locality-based privacy protection of vanilla federated learning. Furthermore, a graph-guided aggregation mechanism is designed to leverage the user-relation graph and federated optimization framework simultaneously. Extensive experiments on five benchmark datasets demonstrate GPFedRec's superior performance. The in-depth study validates that GPFedRec can generally improve existing federated recommendation methods as a plugin while keeping user privacy safe. Code is available https://github.com/Zhangcx19/GPFedRec|联邦推荐系统是一种新兴的人工智能服务架构，它以保护隐私的方式提供推荐服务。使用用户关系图来增强联邦推荐是一个很有前途的课题。然而，在联邦环境中保护基于数据位置的隐私保护的同时构建用户关系图仍然是一个公开的挑战。受简单动机的启发，相似用户对同一条目集有着相似的愿景(嵌入) ，本文提出了一种新的基于图的联邦推荐个性化(GPFedRec)方法。该方法在不访问用户交互记录的情况下，通过在服务器上嵌入用户特定的个性化项目来构造用户关系图。在每个设备上对个性化项目嵌入进行局部微调，然后通过测量客户特定项目嵌入之间的相似度来构造用户关系图。在不访问用户历史交互的情况下，体现了基于数据位置的普通联邦学习的隐私保护。此外，设计了一种图引导的聚合机制，同时利用用户关系图和联邦优化框架。在五个基准数据集上的大量实验证明了 GPFedRec 的优越性能。深入的研究证实，GPFedRec 可以作为一个插件改进现有的联邦推荐方法，同时保护用户隐私安全。密码 https://github.com/zhangcx19/gpfedrec|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPFedRec:+Graph-Guided+Personalization+for+Federated+Recommendation)|0|
|[GradCraft: Elevating Multi-task Recommendations through Holistic Gradient Crafting](https://doi.org/10.1145/3637528.3671585)|Yimeng Bai, Yang Zhang, Fuli Feng, Jing Lu, Xiaoxue Zang, Chenyi Lei, Yang Song|Kuaishou Technology, Beijing, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China & USTC Beijing Research Institute, Hefei, China|Recommender systems require the simultaneous optimization of multiple objectives to accurately model user interests, necessitating the application of multi-task learning methods. However, existing multi-task learning methods in recommendations overlook the specific characteristics of recommendation scenarios, falling short in achieving proper gradient balance. To address this challenge, we set the target of multi-task learning as attaining the appropriate magnitude balance and the global direction balance, and propose an innovative methodology named GradCraft in response. GradCraft dynamically adjusts gradient magnitudes to align with the maximum gradient norm, mitigating interference from gradient magnitudes for subsequent manipulation. It then employs projections to eliminate gradient conflicts in directions while considering all conflicting tasks simultaneously, theoretically guaranteeing the global resolution of direction conflicts. GradCraft ensures the concurrent achievement of appropriate magnitude balance and global direction balance, aligning with the inherent characteristics of recommendation scenarios. Both offline and online experiments attest to the efficacy of GradCraft in enhancing multi-task performance in recommendations. The source code for GradCraft can be accessed at https://github.com/baiyimeng/GradCraft.|推荐系统需要同时对多个目标进行优化，以准确地建立用户兴趣模型，这就需要应用多任务学习方法。然而，现有的多任务推荐学习方法忽视了推荐场景的特殊性，未能实现适当的梯度平衡。为了应对这一挑战，我们将多任务学习的目标设定为实现适当的量级平衡和全局方向平衡，并提出了一种名为“毕业设计”的创新方法。梯度工艺动态调整梯度大小，以符合最大梯度范数，减少干扰梯度大小，以便随后的操作。然后利用预测消除方向梯度冲突，同时考虑所有冲突任务，从理论上保证了方向冲突的全局解决。Gracraft 确保同时实现适当的规模平衡和全球方向平衡，与建议方案的固有特点保持一致。线下和线上的实验都证明了 GradCraft 在提高推荐中的多任务表现方面的有效性。你可透过 https://github.com/baiyimeng/GradCraft 查阅葛拉夫特的源代码。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GradCraft:+Elevating+Multi-task+Recommendations+through+Holistic+Gradient+Crafting)|0|
|[NudgeRank: Digital Algorithmic Nudging for Personalized Health](https://doi.org/10.1145/3637528.3671562)|Jodi Chiam, Aloysius Lim, Ankur Teredesai|CueZen, Inc. & University of Washington, Seattle, WA, USA; CueZen, Inc., Singapore, Singapore|In this paper we describe NudgeRankTM, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems' 4%. Demonstrating scalability and reliability, NudgeRankTM operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems.|在本文中，我们描述了 NudgeRankTM，一个创新的数字算法推动系统，旨在培养积极的健康行为在全人口范围内。利用图形神经网络与可扩展的知识图表相结合的新颖组合，这个推荐系统在生产中运作，每天向超过110万名护理接受者提供个性化和上下文感知的推动。这个企业部署标志着一个最大的人工智能驱动的健康行为改变倡议，适应不同的健康条件和可穿戴设备。严格的评估显示，健康结果在统计学上有显著的改善，包括每日步数增加6.17% ，运动时间增加7.61% 。此外，用户参与度和程序注册率也大幅上升，开放率为13.1% ，而基准系统的开放率为4% 。NudgeRankTM 展示了可伸缩性和可靠性，它可以有效地运行商品计算资源，同时维护生产系统必不可少的自动化和可观测性标准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NudgeRank:+Digital+Algorithmic+Nudging+for+Personalized+Health)|0|
|[Achieving a Better Tradeoff in Multi-stage Recommender Systems through Personalization](https://doi.org/10.1145/3637528.3671593)|Ariel Evnine, Stratis Ioannidis, Dimitris Kalimeris, Shankar Kalyanaraman, Weiwei Li, Israel Nir, Wei Sun, Udi Weinsberg|Meta, Menlo Park, CA, USA; Northeastern University, Boston, MA, USA|Recommender systems in social media websites provide value to their communities by recommending engaging content and meaningful connections. Scaling high-quality recommendations to billions of users in real-time requires sophisticated ranking models operating on a vast number of potential items to recommend, becoming prohibitively expensive computationally. A common technique "funnels'' these items through progressively complex models ("multi-stage''), each ranking fewer items but at higher computational cost for greater accuracy. This architecture introduces a trade-off between the cost of ranking items and providing users with the best recommendations. A key observation we make in this paper is that, all else equal, ranking more items indeed improves the overall objective but has diminishing returns. Following this observation, we provide a rigorous formulation through the framework of DR-submodularity, and argue that for a certain class of objectives (reward functions), it is possible to improve the trade-off between performance and computational cost in multi-stage ranking systems with strong theoretical guarantees. We show that this class of reward functions that provide this guarantee is large and robust to various noise models. Finally, we describe extensive experimentation of our method on three real-world recommender systems in Facebook, achieving 8.8% reduction in overall compute resources with no significant impact on recommendation quality, compared to a 0.8% quality loss in a non-personalized budget allocation.|社交媒体网站的推荐系统通过推荐参与内容和有意义的联系，为社区提供价值。实时将高质量推荐扩展到数十亿用户需要对大量潜在项目进行复杂的排名模型，计算成本高得令人望而却步。一种常见的技术是通过逐步复杂的模型(“多阶段”)“漏斗”这些项目，每个项目的排名较少，但计算成本较高，以获得更高的准确性。这种体系结构在对项目排序的成本和为用户提供最佳建议之间进行了权衡。我们在本文中的一个关键观察结果是，在其他条件相同的情况下，对更多项目进行排序确实会提高整体目标，但具有报酬递减。在此基础上，我们通过 DR- 子模块化的框架提供了一个严格的公式，并认为对于一定类型的目标(奖励函数) ，在具有强理论保证的多阶段排序系统中，可以改善性能和计算成本之间的权衡。我们证明了这类提供这种保证的奖励函数对各种噪声模型是大的和鲁棒的。最后，我们描述了我们的方法在 Facebook 的三个现实世界推荐系统上的广泛实验，实现了总体计算资源减少8.8% ，对推荐质量没有显着影响，而非个性化预算分配中的质量损失为0.8% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Achieving+a+Better+Tradeoff+in+Multi-stage+Recommender+Systems+through+Personalization)|0|
|[Residual Multi-Task Learner for Applied Ranking](https://doi.org/10.1145/3637528.3671523)|Cong Fu, Kun Wang, Jiahua Wu, Yizhou Chen, Guangda Huzhang, Yabo Ni, Anxiang Zeng, Zhiming Zhou|Nanyang Technological University, Singapore, Singapore; ECONCS, Shanghai University of Finance and Economics, Shanghai, China; Shopee Pte. Ltd., Singapore, Singapore; SCSE, Nanyang Technological University, Singapore, Singapore; Shopee Pte. Ltd., Shanghai, China|Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability due to being computationally intensive, having scalability issues, or relying on strong assumptions. To address these limitations and better fit our real-world scenario, pre-rank in Shopee Search, we introduce in this paper ResFlow, a lightweight multi-task learning framework that enables efficient cross-task information sharing via residual connections between corresponding layers of task networks. Extensive experiments on datasets from various scenarios and modalities demonstrate its superior performance and adaptability over state-of-the-art methods. The online A/B tests in Shopee Search showcase its practical value in large-scale industrial applications, evidenced by a 1.29% increase in OPU (order-per-user) without additional system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. To facilitate efficient online deployment, we propose a novel offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding online-offline metric misalignment issue. Besides, we propose to fuse scores from the multiple tasks additively when ranking items, which outperforms traditional multiplicative fusion.|现代电子商务平台在很大程度上依赖于建模不同的用户反馈来提供个性化服务。因此，多任务学习已成为其排名系统的一个组成部分。然而，现有的多任务学习方法遇到了两个主要的挑战: 一些方法缺乏对任务关系的明确建模，导致性能较差，而另一些方法由于计算密集、存在可伸缩性问题或依赖于强大的假设而适用性有限。为了解决这些局限性，并更好地适应我们的现实世界场景，在 Shopee 搜索中的预排序，本文介绍了 ResFlow，一个轻量级的多任务学习框架，使得有效的跨任务信息共享通过相应的任务网络层之间的剩余连接。对来自不同场景和模式的数据集进行的大量实验表明，它比最先进的方法具有更好的性能和适应性。Shopee Search 的在线 A/B 测试展示了其在大规模工业应用中的实用价值，在不增加系统延迟的情况下，OPU (每用户订单)增长了1.29% 。ResFlow 现在完全部署在 Shopee Search 的 pre-rank 模块中。为了促进有效的在线部署，我们提出了一种新的离线度量加权召回@K，它与我们的在线度量 OPU 很好地一致，解决了长期存在的在线-离线度量失调问题。此外，本文还提出了在项目排序时对多个任务的得分进行累加融合，这种融合方法的性能优于传统的乘法融合方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Residual+Multi-Task+Learner+for+Applied+Ranking)|0|
|[Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising](https://doi.org/10.1145/3637528.3671549)|Baoshen Guo, Xining Song, Shuai Wang, Wei Gong, Tian He, Xue Liu|McGill University, Montréal, Canada; Southeast University, Nanjing, China; JD Logistics, Beijing, China; University of Science and Technology of China, Hefei, China; Southeast University & JD Logistics, Nanjing, China|Logistics advertising is an emerging task in online-to-offline logistics systems, where logistics companies expand parcel shipping services to new users through advertisements on shopping websites. Compared to existing online e-commerce advertising, logistics advertising has two significant new characteristics: (i) the complex factors in logistics advertising considering both users' offline logistics preference and online purchasing profiles; and (ii) data sparsity and mutual relations among multiple steps due to longer advertising conversion processes. To address these challenges, we design MCAC, a Multi-task Conditional Attention network-based logistics advertising Conversion prediction framework, which consists of (i) an offline shipping preference extraction model to extract the user's offline logistics preference from historical shipping records, and (ii) a multi-task conditional attention-based conversion rate prediction module to model mutual relations among multiple steps in logistics advertising conversion processes. We evaluate and deploy MCAC on one of the largest e-commerce platforms in China for logistics advertising. Extensive offline experiments show that our method outperforms state-of-the-art baselines in various metrics. Moreover, the conversion rate prediction results of large-scale online A/B testing show that MCAC achieves a 15.22% improvement compared to existing industrial practices, which demonstrates the effectiveness of the proposed framework.|物流广告是线上到线下物流系统中的一项新兴任务，物流公司通过在购物网站上投放广告，向新用户提供包裹运输服务。与现有的在线电子商务广告相比，物流广告具有两个重要的新特点: (1)考虑用户线下物流偏好和线上购买概况的物流广告复杂因素; (2)由于广告转换过程较长，数据稀疏和多步骤之间的相互关系。针对这些挑战，我们设计了基于多任务条件注意网络的物流广告转化预测框架 MCAC，该框架包括: (1)离线运输偏好提取模型，从历史运输记录中提取用户的离线物流偏好; (2)基于多任务条件注意的转化率预测模型，模拟物流广告转化过程中多个步骤之间的相互关系。我们评估和部署 MCAC 在中国最大的电子商务平台之一的物流广告。大量的离线实验表明，我们的方法在各种指标上都优于最先进的基线。此外，大规模在线 A/B 测试的转换率预测结果表明，与现有工业实践相比，MCAC 的转换率提高了15.22% ，证明了该框架的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-task+Conditional+Attention+Network+for+Conversion+Prediction+in+Logistics+Advertising)|0|
|[Learning to Rank for Maps at Airbnb](https://doi.org/10.1145/3637528.3671648)|Malay Haldar, Hongwei Zhang, Kedar Bellare, Sherry Chen, Soumyadip Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei He, Sanjeev Katariya|Airbnb, Inc., San Francisco, CA, USA; Airbnb, Inc., San Francisco, WA, USA|As a two-sided marketplace, Airbnb brings together hosts who own listings for rent with prospective guests from around the globe. Results from a guest's search for listings are displayed primarily through two interfaces: (1) as a list of rectangular cards that contain on them the listing image, price, rating, and other details, referred to as list-results (2) as oval pins on a map showing the listing price, called map-results. Both these interfaces, since their inception, have used the same ranking algorithm that orders listings by their booking probabilities and selects the top listings for display. But some of the basic assumptions underlying ranking, built for a world where search results are presented as lists, simply break down for maps. This paper describes how we rebuilt ranking for maps by revising the mathematical foundations of how users interact with search results. Our iterative and experiment-driven approach led us through a path full of twists and turns, ending in a unified theory for the two interfaces. Our journey shows how assumptions taken for granted when designing machine learning algorithms may not apply equally across all user interfaces, and how they can be adapted. The net impact was one of the largest improvements in user experience for Airbnb which we discuss as a series of experimental validations.|作为一个双向的市场，Airbnb 将拥有房源的房东和来自世界各地的潜在客人聚集在一起。客人对列表的搜索结果主要通过两个界面显示: (1)作为一个矩形卡片列表，其中包含列表图像、价格、评级和其他细节，称为列表结果(2)作为椭圆形针在地图上显示列表价格，称为地图结果。这两个界面从一开始就使用相同的排名算法，根据预订概率对列表进行排序，并选择最上面的列表进行显示。但是，一些基本的排名假设，建立在一个世界里，搜索结果显示为列表，只是分解为地图。本文描述了我们如何通过修改用户与搜索结果交互的数学基础来重建地图的排名。我们的迭代和实验驱动的方法带领我们走过了一条充满曲折的道路，最终形成了两个接口的统一理论。我们的旅程表明，当设计机器学习算法时，假设是理所当然的，可能不会平等地适用于所有用户界面，以及它们是如何适应的。净影响是 Airbnb 用户体验的最大改进之一，我们将其作为一系列实验验证进行讨论。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Rank+for+Maps+at+Airbnb)|0|
|[Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce](https://doi.org/10.1145/3637528.3671559)|Zhe Lin, Jiwei Tan, Dan Ou, Xi Chen, Shaowei Yao, Bo Zheng|Alibaba Group, HangZhou, China|Text relevance or text matching of query and product is an essential technique for the e-commerce search system to ensure that the displayed products can match the intent of the query. Many studies focus on improving the performance of the relevance model in search system. Recently, pre-trained language models like BERT have achieved promising performance on the text relevance task. While these models perform well on the offline test dataset, there are still obstacles to deploy the pre-trained language model to the online system as their high latency. The two-tower model is extensively employed in industrial scenarios, owing to its ability to harmonize performance with computational efficiency. Regrettably, such models present an opaque ''black box'' nature, which prevents developers from making special optimizations. In this paper, we raise deep Bag-o f-Words (DeepBoW) model, an efficient and interpretable relevance architecture for Chinese e-commerce. Our approach proposes to encode the query and the product into the sparse BoW representation, which is a set of word-weight pairs. The weight means the important or the relevant score between the corresponding word and the raw text. The relevance score is measured by the accumulation of the matched word between the sparse BoW representation of the query and the product. Compared to popular dense distributed representation that usually suffers from the drawback of black-box, the most advantage of the proposed representation model is highly explainable and interventionable, which is a superior advantage to the deployment and operation of online search engines. Moreover, the online efficiency of the proposed model is even better than the most efficient inner product form of dense representation. The proposed model is experimented on three different datasets for learning the sparse BoW representations, including the human-annotation set, the search-log set and the click-through set. Then the models are evaluated by experienced human annotators. Both the auto metrics and the online evaluations show our DeepBoW model achieves competitive performance while the online inference is much more efficient than the other models. Our DeepBoW model has already deployed to the biggest Chinese e-commerce search engine Taobao and served the entire search traffic for over 6 months.|查询与产品的文本相关性或文本匹配是电子商务搜索系统中保证所显示的产品与查询意图相匹配的关键技术。许多研究集中在提高搜索系统中相关性模型的性能。近年来，像 BERT 这样的预训练语言模型在文本相关性任务中取得了良好的效果。尽管这些模型在离线测试数据集上表现良好，但仍然存在将预先训练的语言模型部署到在线系统的障碍，因为它们具有较高的延迟。由于双塔模型能够协调性能和计算效率，因此在工业场景中得到了广泛的应用。遗憾的是，这样的模型呈现出一种不透明的“黑盒”特性，这阻止了开发人员进行特殊的优化。本文提出了一种面向中国电子商务的高效、易解释的关联结构——深层 Bag-o-Words 模型。我们的方法建议将查询和产品编码成稀疏的 BW 表示，这是一组字权重对。加权表示相应单词和原始文本之间的重要或相关得分。相关性得分是通过查询的稀疏弓形表示和产品之间匹配词的累积来衡量的。该模型的最大优点是可解释性强、可干预性强，优于在线搜索引擎的部署和运行。此外，该模型的在线效率甚至优于最有效的密集表示内积形式。该模型在三个不同的数据集上进行了实验，包括人工注释集、搜索日志集和点击通过集。然后由经验丰富的人工注释者对模型进行评估。自动度量和在线评估都表明，我们的 DeepBW 模型达到了竞争性能，而在线推理是更有效的比其他模型。我们的 DeepBow 模式已经部署到中国最大的电子商务搜索引擎淘宝，并服务于整个搜索流量超过6个月。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Bag-of-Words+Model:+An+Efficient+and+Interpretable+Relevance+Architecture+for+Chinese+E-Commerce)|0|
|[GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security](https://doi.org/10.1145/3637528.3671602)|Xuanqing Liu, Runhui Wang, Yang Song, Luyang Kong|Amazon Web Services, Seattle, WA, USA|Schema matching constitutes a pivotal phase in the data ingestion process forcontemporary database systems. Its objective is to discern pairwisesimilarities between two sets of attributes, each associated with a distinctdata table. This challenge emerges at the initial stages of data analytics,such as when incorporating a third-party table into existing databases toinform business insights. Given its significance in the realm of databasesystems, schema matching has been under investigation since the 2000s. Thisstudy revisits this foundational problem within the context of large languagemodels. Adhering to increasingly stringent data security policies, our focuslies on the zero-shot and few-shot scenarios: the model should analyze only aminimal amount of customer data to execute the matching task, contrasting withthe conventional approach of scrutinizing the entire data table. We emphasizethat the zero-shot or few-shot assumption is imperative to safeguard theidentity and privacy of customer data, even at the potential cost of accuracy.The capability to accurately match attributes under such stringent requirementsdistinguishes our work from previous literature in this domain.|模式匹配是现代数据库系统数据摄取过程中的一个关键阶段。它的目标是识别两组属性之间的成对相似性，每组属性都与一个不同的数据表相关联。这一挑战出现在数据分析的初始阶段，例如将第三方表合并到现有数据库中以提供业务见解。鉴于模式匹配在数据库领域的重要性，自2000年以来，模式匹配一直在研究之中。本研究在大型语言模型的背景下重新审视这个基本问题。坚持越来越严格的数据安全策略，我们的重点是零射击和少射击场景: 模型应该只分析最小数量的客户数据来执行匹配任务，与审查整个数据表的传统方法形成对比。我们强调，零拍摄或少拍摄假设是必要的，以保护客户数据的身份和隐私，即使在潜在的准确性成本。在如此严格的要求下精确匹配属性的能力使我们的工作区别于这个领域以前的文献。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRAM:+Generative+Retrieval+Augmented+Matching+of+Data+Schemas+in+the+Context+of+Data+Security)|0|
|[Non-autoregressive Generative Models for Reranking Recommendation](https://doi.org/10.1145/3637528.3671645)|Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang|Kuaishou Technology, Beijing, China; Tsinghua University, Beijing, China; Peking University, Beijing, China|Contemporary recommendation systems are designed to meet users' needs by delivering tailored lists of items that align with their specific demands or interests. In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items. The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. The generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Firstly, the generator can only generate the target items one by one and hence suffers from slow inference. Secondly, the discrepancy between training and inference brings an error accumulation. Lastly, the left-to-right generation overlooks information from succeeding items, leading to suboptimal performance. To address these issues, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To tackle challenges such as sparse training samples and dynamic candidates, we introduce a matching model. Considering the diverse nature of user feedback, we employ a sequence-level unlikelihood training objective to differentiate feasible sequences from unfeasible ones. Additionally, to overcome the lack of dependency modeling in non-autoregressive models regarding target items, we introduce contrastive decoding to capture correlations among these items. Extensive offline experiments validate the superior performance of NAR4Rec over state-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec significantly enhances the user experience. Furthermore, NAR4Rec has been fully deployed in a popular video app Kuaishou with over 300 million daily active users.|当代的推荐系统通过提供符合用户特定需求或兴趣的量身定制的项目列表来满足用户的需求。在多阶段推荐系统中，重新排序通过建立项目之间的列表内相关性起着至关重要的作用。重新排序的关键挑战在于在排列的组合空间中探索最优序列。最近的研究提出了生成器-评估器学习范式，其中生成器生成多个可行序列，评估器根据估计的列表分数挑选出最佳序列。生成器是至关重要的，生成模型非常适合于生成器函数。当前的生成模型采用自回归策略进行序列生成。然而，在实时工业系统中部署自回归模型是具有挑战性的。首先，生成器只能生成一个个目标项，因此存在推理速度慢的问题。其次，训练与推理的差异带来了错误的积累。最后，从左到右的生成会忽略来自后续项的信息，从而导致性能不理想。为了解决这些问题，我们提出了一个非自动回归的重新排名建议(NAR4rec)生成模型，旨在提高效率和效力。为了解决稀疏训练样本和动态候选人等问题，我们引入了一个匹配模型。考虑到用户反馈的多样性，我们采用序列级不似然训练目标来区分可行序列和不可行序列。此外，为了克服非自回归模型中缺乏对目标项的依赖建模，我们引入对比解码来捕获这些项之间的相关性。大量的离线实验验证了 NAR4Rec 优于最先进的重新排序方法的性能。在线 A/B 测试显示 NAR4Rec 显著提高了用户体验。此外，NAR4Rec 已经完全部署在一个流行的视频应用快手中，每天有超过3亿的活跃用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Non-autoregressive+Generative+Models+for+Reranking+Recommendation)|0|
|[Chaining Text-to-Image and Large Language Model: A Novel Approach for Generating Personalized e-commerce Banners](https://doi.org/10.1145/3637528.3671636)|Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag, Yokila Arora, Sushant Kumar, Kannan Achan|Walmart Global Tech, Sunnyvale, CA, USA|Text-to-image models such as stable diffusion have opened a plethora ofopportunities for generating art. Recent literature has surveyed the use oftext-to-image models for enhancing the work of many creative artists. Manye-commerce platforms employ a manual process to generate the banners, which istime-consuming and has limitations of scalability. In this work, we demonstratethe use of text-to-image models for generating personalized web banners withdynamic content for online shoppers based on their interactions. The novelty inthis approach lies in converting users' interaction data to meaningful promptswithout human intervention. To this end, we utilize a large language model(LLM) to systematically extract a tuple of attributes from itemmeta-information. The attributes are then passed to a text-to-image model viaprompt engineering to generate images for the banner. Our results show that theproposed approach can create high-quality personalized banners for users.|文本到图像的模型，例如稳定的扩散，已经为艺术的生成提供了大量的机会。最近的文献调查了文本到图像模型的使用，以提高许多创造性的艺术家的工作。许多电子商务平台使用手工过程来生成横幅，这非常耗时并且具有可伸缩性的限制。在这项工作中，我们演示了使用文本到图像的模型来生成个性化的网络横幅与动态内容的在线购物者基于他们的交互。这种方法的新颖之处在于无需人工干预就能将用户的交互数据转换为有意义的提示。为此，我们利用大型语言模型(LLM)系统地从 itemmeta 信息中提取属性元组。然后通过提示工程将属性传递给文本到图像模型，以生成横幅的图像。结果表明，该方法可以为用户创建高质量的个性化横幅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Chaining+Text-to-Image+and+Large+Language+Model:+A+Novel+Approach+for+Generating+Personalized+e-commerce+Banners)|0|
|[LiMAML: Personalization of Deep Recommender Models via Meta Learning](https://doi.org/10.1145/3637528.3671599)|Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang, Zeinab S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya Venugopalan, Aman Gupta, Fedor Borisyuk, S. Sathiya Keerthi, Ajith Muralidharan|LinkedIn Corporation, Sunnyvale, CA, USA; Aliveo AI Corp, Sunnyvale, CA, USA|In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in production, which involves transforming them into fixed-sized vectors, termed meta embeddings, thereby enabling the seamless deployment of models with hundreds of billions of parameters for online serving. Through extensive experimentation on production data drawn from various applications at LinkedIn, we demonstrate that the proposed solution consistently outperforms the best performing baseline models of those applications, including strong baselines such as using wide-and-deep ID based personalization approach. Our approach has enabled the deployment of a range of highly personalized AI models across diverse LinkedIn applications, leading to substantial improvements in business metrics as well as refreshed experience for our members.|在推荐系统领域，深层神经网络的普遍采用已经成为建模不同业务目标的主要范例。随着用户基础的不断扩大，个性化和频繁更新模型的必要性对于确保向各类成员提供相关的最新经验具有至关重要的意义。在这项工作中，我们介绍了一个创新的元学习解决方案，专为个人成员和其他实体的模型个性化，加上基于最新用户交互信号的频繁更新。具体来说，我们利用模型不可知元学习(MAML)算法来使用最近的用户交互数据来适应每个任务的子网络。鉴于在在线推荐系统中生产原始的基于 MAML 的模型几乎是不可行的，我们提出了一个有效的策略来操作生产中的元学习子网络，包括将它们转换成固定大小的向量，称为元嵌入，从而能够无缝部署具有数千亿参数的在线服务模型。通过对来自 LinkedIn 各种应用程序的生产数据进行广泛的实验，我们证明了所提出的解决方案始终优于这些应用程序的最佳性能基线模型，包括强大的基线，例如使用基于广泛和深度 ID 的个性化方法。我们的方法使得一系列高度个性化的人工智能模型能够在不同的 LinkedIn 应用程序中部署，从而大大改善了业务指标，并为我们的会员带来了全新的体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiMAML:+Personalization+of+Deep+Recommender+Models+via+Meta+Learning)|0|
|[Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking](https://doi.org/10.1145/3637528.3671517)|Dezhi Ye, Jie Liu, Jiabin Fan, Bowen Tian, Tianhua Zhou, Xiang Chen, Jin Ma|Tencent PCG, Beijing, China|This paper addresses the challenge of the semantic gap between user queries and web content, commonly referred to as asymmetric text matching, within the domain of web search. By leveraging BERT for reading comprehension, current algorithms enable significant advancements in query understanding, but still encounter limitations in effectively resolving the asymmetrical ranking problem due to model comprehension and summarization constraints. To tackle this issue, we propose the QAGR (Question-Answer Generation and Ranking) method, comprising an offline module called QAGeneration and an online module called QARanking. The QAGeneration module utilizes large language models (LLMs) to generate high-quality question-answering pairs for each web page. This process involves two steps: generating question-answer pairs and performing verification to eliminate irrelevant questions, resulting in high-quality questions associated with their respective documents. The QARanking module combines and ranks the generated questions and web page content. To ensure efficient online inference, we design the QARanking model as a homogeneous dual-tower model, incorporating query intent to drive score fusion while balancing keyword matching and asymmetric matching. Additionally, we conduct a preliminary screening of questions for each document, selecting only the top-N relevant questions for further relevance calculation. Empirical results demonstrate the substantial performance improvement of our proposed method in web search. We achieve over 8.7% relative offline relevance improvement and over 8.5% online engagement gain compared to the state-of-the-art web search system. Furthermore, we deploy QAGR to online web search engines and share our deployment experience, including production considerations and ablation experiments. This research contributes to advancing the field of asymmetric web search and provides valuable insights for enhancing search engine performance.|本文讨论了在网络搜索领域中，用户查询和网页内容之间的语义差异(通常称为非对称文本匹配)所带来的挑战。通过利用 BERT 的阅读理解，当前的算法在查询理解方面取得了显著的进步，但由于模型理解和摘要约束，在有效解决非对称排序问题方面仍然存在局限性。为了解决这个问题，我们提出了 QAGR (问答生成和排序)方法，包括一个称为 QAGeneration 的离线模块和一个称为 QARanking 的在线模块。QAGeneration 模块利用大语言模型(LLM)为每个网页生成高质量的问答对。这个过程包括两个步骤: 生成问题-答案对和进行验证，以消除不相关的问题，从而产生与各自文件相关的高质量问题。QARanking 模块将生成的问题和网页内容进行组合和排序。为了保证在线推理的有效性，我们将 QARanking 模型设计成一个均匀的双塔模型，在平衡关键字匹配和非对称匹配的同时，结合查询意图驱动得分融合。此外，我们对每个文档的问题进行初步筛选，只选择排名前 N 位的相关问题进行进一步的相关性计算。实验结果表明，本文提出的方法在网络搜索中性能得到了显著提高。与最先进的网络搜索系统相比，我们实现了超过8.7% 的相对离线相关性改善和超过8.5% 的在线参与收益。此外，我们部署 QAGR 到在线网络搜索引擎和分享我们的部署经验，包括生产考虑和烧蚀实验。本文的研究有助于推进非对称网络搜索领域的发展，为提高搜索引擎性能提供了有价值的见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Asymmetric+Web+Search+through+Question-Answer+Generation+and+Ranking)|0|
|[Unsupervised Ranking Ensemble Model for Recommendation](https://doi.org/10.1145/3637528.3671598)|Wenhui Yu, Bingqi Liu, Bin Xia, Xiaoxiao Xu, Ying Chen, Yongchang Li, Lantao Hu|Kuaishou Technology, Beijing, China|When visiting an online platform, a user generates various actions, such as clicks, long views, likes, comments, etc. To capture user preferences in these aspects, we learn these objectives and return multiple rankings of candidate items for each user. We need to aggregate them into one to truncate the candidate set, and ranking ensemble model is proposed for this task. However, there is a critical issue: though we input abundant information, what model learns depends on the supervision. Unfortunately, the existing supervision is poorly designed, leading to serious information loss issue. To address this issue, we designed an unsupervised loss to compel the ranking ensemble model to learn all information of input rankings, including sequential and numerical information. (1) For sequential information, we design a distance measure between two rankings, and train the ensemble ranking to have similar order with all input rankings by minimizing the distance. (2) For numerical information, we design a decoder to reconstruct values of original rankings from the hidden layer of the model, to guarantee that the model captures as much input information as possible. Our unsupervised loss is compatible with all ranking ensemble models. We optimize several widely-used structures to propose unsupervised ranking ensemble models. We devise comprehensive experiments on two real-world datasets to demonstrate the effectiveness of the proposed models. We also apply our model in a short video platform with billions of users, and achieve significant improvement.|当访问在线平台时，用户生成各种各样的动作，如点击、长视图、喜欢、评论等。为了捕获这些方面的用户偏好，我们学习这些目标，并返回每个用户的候选项的多个排名。为了截断候选集，我们需要将它们聚合成一个集合，并提出了排序集成模型。然而，有一个关键的问题: 虽然我们输入了大量的信息，但是模型学到了什么取决于监督。然而，现有的监管体系设计不当，导致了严重的信息流失问题。为了解决这个问题，我们设计了一个无监督的损失，以迫使排名集成模型学习所有的输入排名信息，包括顺序和数字信息。(1)对于序列信息，我们设计了两个排名之间的距离度量，并通过最小化距离训练集合排名与所有输入排名具有相似的顺序。(2)对于数值信息，我们设计了一个解码器，从模型的隐层重建原始排名值，以保证模型捕获尽可能多的输入信息。我们的无监督损失与所有等级集合模型兼容。我们优化了几个广泛使用的结构，提出了无监督排序集成模型。我们设计了两个实际数据集的综合实验来验证所提出模型的有效性。在一个拥有数十亿用户的短视频平台上应用了该模型，并取得了显著的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Ranking+Ensemble+Model+for+Recommendation)|0|
|[Adapting Job Recommendations to User Preference Drift with Behavioral-Semantic Fusion Learning](https://doi.org/10.1145/3637528.3671759)|Xiao Han, Chen Zhu, Xiao Hu, Chuan Qin, Xiangyu Zhao, Hengshu Zhu|Career Science Lab, BOSS Zhipin, Beijing, China; City University of Hong Kong, Hong Kong, China; Career Science Lab, BOSS Zhipin, University of Science and Technology of China, Beijing, China|Job recommender systems are crucial for aligning job opportunities with job-seekers in online job-seeking. However, users tend to adjust their job preferences to secure employment opportunities continually, which limits the performance of job recommendations. The inherent frequency of preference drift poses a challenge to promptly and precisely capture user preferences. To address this issue, we propose a novel session-based framework, BISTRO, to timely model user preference through fusion learning of semantic and behavioral information. Specifically, BISTRO is composed of three stages: 1) coarse-grained semantic clustering, 2) fine-grained job preference extraction, and 3) personalized top-k job recommendation. Initially, BISTRO segments the user interaction sequence into sessions and leverages session-based semantic clustering to achieve broad identification of person-job matching. Subsequently, we design a hypergraph wavelet learning method to capture the nuanced job preference drift. To mitigate the effect of noise in interactions caused by frequent preference drift, we innovatively propose an adaptive wavelet filtering technique to remove noisy interaction. Finally, a recurrent neural network is utilized to analyze session-based interaction for inferring personalized preferences. Extensive experiments on three real-world offline recruitment datasets demonstrate the significant performances of our framework. Significantly, BISTRO also excels in online experiments, affirming its effectiveness in live recruitment settings. This dual success underscores the robustness and adaptability of BISTRO. The source code is available at https://github.com/Applied-Machine-Learning-Lab/BISTRO.|在网上求职中，职位推荐系统对于将求职机会与求职者联系起来至关重要。然而，用户倾向于不断调整自己的工作偏好以获得就业机会，这就限制了工作推荐的效果。偏好漂移的固有频率对及时、准确地捕捉用户偏好提出了挑战。为了解决这个问题，我们提出了一个新的基于会话的框架 BISTRO，通过语义和行为信息的融合学习来及时建模用户偏好。具体来说，BISTRO 由三个阶段组成: 1)粗粒度语义聚类，2)细粒度工作偏好提取，3)个性化的 top-k 工作推荐。最初，BISTRO 将用户交互序列分割成会话，并利用基于会话的语义聚类实现人-工匹配的广泛识别。随后，我们设计了一种超图小波学习方法来捕捉细微的工作偏好漂移。为了消除频繁偏好漂移引起的相互作用中的噪声影响，本文创新性地提出了一种自适应小波滤波技术来去除噪声相互作用。最后，一个递归神经网络被用来分析基于会话的交互来推断个性化的偏好。在三个真实世界的离线招聘数据集上的大量实验证明了我们的框架的显著性能。值得注意的是，BISTRO 还擅长在线实验，确认其在现场招聘设置的有效性。这种双重成功突出了 BISTRO 的稳健性和适应性。源代码可在 https://github.com/applied-machine-learning-lab/bistro 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adapting+Job+Recommendations+to+User+Preference+Drift+with+Behavioral-Semantic+Fusion+Learning)|0|
|[Learning to Bid the Interest Rate in Online Unsecured Personal Loans](https://doi.org/10.1145/3637528.3671584)|Dong Jun Jee, Seung Jung Jin, JiHoon Yoo, Byunggyu Ahn|PFC Technologies, Seoul, Republic of Korea|The unsecured personal loan (UPL) market is a multi-billion dollar market where numerous financial institutions compete. Due to the development of online banking, loan applicants start to compare numerous loan products. They aim for high loan limits and low interest rates. Since loan applicants have a desired loan amount, institutions instead focus on adjusting interest rates. Despite the importance of determining optimal interest strategies, institutions have traditionally relied on heuristic methods by human experts to set interest rates. This is done by adding a target return on assets (ROA) to the applicant's expected default probability predicted by a credit scoring system (CSS) such as the FICO score. We conceptualize the UPL market dynamics as a repeated auction scenario, where loan applicants (akin to sellers) seek the lowest interest rates, while financial institutions (akin to bidders) aim to maximize profits through higher interest rates. To the best of our knowledge, this is the first time anyone has approached the UPL market through the viewpoint of a repeated auction. While there are several research done in learning to bid in repeated auctions, those works cannot be directly applied to the UPL market due to the lack of any feedback about other bidders' strategies and the need to satisfy the bidder's target loan volume and profit variance. We present an algorithm named AutoInterest, which is a modification of the dual gradient descent algorithm. In addition, we provide a framework to evaluate interest rate bidding strategies on a benchmark dataset and the credit bureau dataset of actual loan applicants in South Korea. We evaluate AutoInterest on this framework and show higher cumulative profit compared to other common online algorithms and the current fixed strategy used by real institutions.|无担保个人贷款(UPL)市场是一个数十亿美元的市场，众多金融机构在其中展开竞争。由于网上银行的发展，贷款申请者开始比较众多的贷款产品。他们的目标是高贷款限额和低利率。由于贷款申请人有一个理想的贷款数额，机构反而把重点放在调整利率。尽管确定最优利率策略很重要，但机构历来依赖人类专家的启发式方法来设定利率。这是通过将目标资产收益率(ROA)添加到由信用评分系统(CSS)(例如 FICO 评分)预测的申请者的预期违约概率来完成的。我们将 UPL 市场动态概念化为一个重复的拍卖场景，其中贷款申请者(类似于卖方)寻求最低的利率，而金融机构(类似于投标者)旨在通过更高的利率实现利润最大化。据我们所知，这是第一次有人通过重复拍卖的观点进入 UPL 市场。虽然在重复拍卖中学习投标已经有了一些研究，但是由于缺乏对其他投标者策略的反馈，以及需要满足投标者的目标贷款量和利润差异，这些工作不能直接应用于 UPL 市场。我们提出了一个名为“自动兴趣”的算法，它是对双梯度下降法算法的修改。此外，我们还提供了一个基于基准数据集和韩国实际贷款申请者信用局数据集评估利率投标策略的框架。我们在这个框架下评估了自动收益，并显示了比其他常见的在线算法和当前实际机构使用的固定策略更高的累积利润。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+to+Bid+the+Interest+Rate+in+Online+Unsecured+Personal+Loans)|0|
|[A Hierarchical and Disentangling Interest Learning Framework for Unbiased and True News Recommendation](https://doi.org/10.1145/3637528.3671944)|Shoujin Wang, Wentao Wang, Xiuzhen Zhang, Yan Wang, Huan Liu, Fang Chen|University of Technology Sydney, Sydney, Australia; Arizona State University, Tempe, USA; Macquarie University, Sydney, Australia; RMIT University, Melbourne, Australia|In the era of information explosion, news recommender systems are crucial for users to effectively and efficiently discover their interested news. However, most of the existing news recommender systems face two major issues, hampering recommendation quality. Firstly, they often oversimplify users' reading interests, neglecting their hierarchical nature, spanning from high-level event (e.g., US Election) related interests to low-level news article-specifc interests. Secondly, existing work often assumes a simplistic context, disregarding the prevalence of fake news and political bias under the real-world context. This oversight leads to recommendations of biased or fake news, posing risks to individuals and society. To this end, this paper addresses these gaps by introducing a novel framework, the Hierarchical and Disentangling Interest learning framework (HDInt). HDInt incorporates a hierarchical interest learning module and a disentangling interest learning module. The former captures users' high- and low-level interests, enhancing next-news recommendation accuracy. The latter effectively separates polarity and veracity information from news contents and model them more specifcally, promoting fairness- and truth-aware reading interest learning for unbiased and true news recommendations. Extensive experiments on two real-world datasets demonstrate HDInt's superiority over state-of-the-art news recommender systems in delivering accurate, unbiased, and true news recommendations.|在信息爆炸时代，新闻推荐系统对于用户有效发现感兴趣的新闻至关重要。然而，大多数现有的新闻推荐系统面临两个主要问题，阻碍了推荐质量。首先，他们往往过分简化用户的阅读兴趣，忽视了他们的等级性质，从高水平的事件(如美国大选)相关兴趣低水平的新闻文章的具体兴趣。其次，现有作品往往假设一个简单化的语境，忽视了现实语境下假新闻和政治偏见的盛行。这种疏忽导致了有偏见或假新闻的建议，给个人和社会带来风险。为此，本文提出了一种新的兴趣学习框架——分层分离式兴趣学习框架(HDInt)。HDInt 集成了分层兴趣学习模块和分离兴趣学习模块。前者捕捉用户的高层次和低层次兴趣，提高下一新闻推荐的准确性。后者有效地将极性和准确性信息从新闻内容中分离出来，并对其进行更具体的建模，促进公平和真实意识的阅读兴趣学习，以获得无偏见和真实的新闻推荐。在两个真实世界数据集上的大量实验表明，HDInt 在提供准确、公正和真实的新闻推荐方面优于最先进的新闻推荐系统。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Hierarchical+and+Disentangling+Interest+Learning+Framework+for+Unbiased+and+True+News+Recommendation)|0|
|[Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature Interactions](https://doi.org/10.1145/3637528.3671784)|Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, Jingbo Zhou|Baidu AI Cloud, Baidu Inc., Beijing, China; Baidu Research, Baidu Inc., Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong|In recommendation systems, new items are continuously introduced, initially lacking interaction records but gradually accumulating them over time. Accurately predicting the click-through rate (CTR) for these items is crucial for enhancing both revenue and user experience. While existing methods focus on enhancing item ID embeddings for new items within general CTR models, they tend to adopt a global feature interaction approach, often overshadowing new items with sparse data by those with abundant interactions. Addressing this, our work introduces EmerG, a novel approach that warms up cold-start CTR prediction by learning item-specific feature interaction patterns. EmerG utilizes hypernetworks to generate an item-specific feature graph based on item characteristics, which is then processed by a Graph Neural Network (GNN). This GNN is specially tailored to provably capture feature interactions at any order through a customized message passing mechanism. We further design a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a minimal set of item-specific parameters within each task. This strategy effectively reduces the risk of overfitting when dealing with limited data. Extensive experiments on benchmark datasets validate that EmerG consistently performs the best given no, a few and sufficient instances of new items.|在推荐系统中，不断引入新的项目，最初缺乏交互记录，但随着时间的推移逐渐积累。准确地预测这些项目的点进率对于提高收入和用户体验至关重要。虽然现有的方法侧重于在一般的 CTR 模型中增强新项目的项目 ID 嵌入，但它们倾向于采用全局特征交互方法，往往使那些具有丰富交互的数据稀疏的新项目黯然失色。为了解决这个问题，我们的工作介绍了 EmerG，这是一种通过学习项目特定的特征交互模式来预测冷启动 CTR 的新方法。EmerG 利用超网络生成基于项目特征的项目特征图，然后通过图神经网络(GNN)对其进行处理。这个 GNN 是专门定制的，可以通过定制的消息传递机制以任何顺序捕获特性交互。我们进一步设计了一个元学习策略，在不同的项目 CTR 预测任务中优化超级网络和 GNN 的参数，同时在每个任务中只调整最小的项目特定参数集。这种策略有效地降低了处理有限数据时过度拟合的风险。在基准数据集上的大量实验验证了 EmerG 始终如一地表现出最好的特性——没有、少数和充分的新项目实例。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Warming+Up+Cold-Start+CTR+Prediction+by+Learning+Item-Specific+Feature+Interactions)|0|
|[Improving Multi-modal Recommender Systems by Denoising and Aligning Multi-modal Content and User Feedback](https://doi.org/10.1145/3637528.3671703)|Guipeng Xv, Xinyu Li, Ruobing Xie, Chen Lin, Chong Liu, Feng Xia, Zhanhui Kang, Leyu Lin|Tencent, Beijing, China; School of Informatics, Xiamen University, Xiamen, Fujian, China|Multi-modal recommender systems (MRSs) are pivotal in diverse online web platforms and have garnered considerable attention in recent years. However, previous studies overlook the challenges of (1)noisy multi-modal content, (2) noisy user feedback, and (3) aligning multi-modal content and user feedback. To tackle these challenges, we propose Denoising and Aligning Multi-modal Recommender System (DA-MRS). To mitigate noise in multi-modal content, DA-MRS first constructs item-item graphs determined by consistent content similarity across modalities. To denoise user feedback, DA-MRS associates the probability of observed feedback with multi-modal content and devises a denoised BPR loss. Furthermore, DA-MRS implements Alignment guided by User preference to enhance task-specific item representation and Alignment guided by graded Item relations to provide finer-grained alignment. Extensive experiments verify that DA-MRS is a plug-and-play framework and achieves significant and consistent improvements across various datasets, backbone models, and noisy scenarios.|多模态推荐系统(MRS)是各种在线网络平台中的关键技术，近年来得到了广泛的关注。然而，以往的研究忽略了以下挑战: (1)噪声多模态内容，(2)噪声用户反馈，和(3)调整多模态内容和用户反馈。为了应对这些挑战，我们提出了去噪和对齐多模态推荐系统(DA-MRS)。为了减轻多模态内容中的噪声，DA-MRS 首先构建由不同模态间一致的内容相似性确定的项目-项目图。为了去除用户反馈的噪声，DA-MRS 将观测反馈的概率与多模态内容联系起来，设计了一种去除 BPR 损失的方法。此外，DA-MRS 还实现了用户偏好引导的对齐，增强了任务特定项目的表示和分级项目关系引导的对齐，提供了更细粒度的对齐。大量的实验验证了 DA-MRS 是一个即插即用的框架，并在各种数据集、骨干模型和噪声场景中实现了显著和一致的改进。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Multi-modal+Recommender+Systems+by+Denoising+and+Aligning+Multi-modal+Content+and+User+Feedback)|0|
|[DDCDR: A Disentangle-based Distillation Framework for Cross-Domain Recommendation](https://doi.org/10.1145/3637528.3671605)|Zhicheng An, Zhexu Gu, Li Yu, Ke Tu, Zhengwei Wu, Binbin Hu, Zhiqiang Zhang, Lihong Gu, Jinjie Gu|Ant Group, Hangzhou, Zhejiang, China|Modern recommendation platforms frequently encompass multiple domains to cater to the varied preferences of users. Recently, cross-domain learning has gained traction as a significant paradigm within the context of recommendation systems, enabling the leveraging of rich information from a well-endowed source domain to enhance a target domain, often limited by inadequate data resources. A primary concern in cross-domain recommendation is the mitigation of negative transfer-ensuring the selective transference of pertinent knowledge from the source (domain-shared knowledge) while maintaining the integrity of domain-unique insights within the target domain (domain-specific knowledge). In this paper, we propose a novel Disentangle-based Distillation Framework for Cross-Domain Recommendation (DDCDR), designed to operate at the representational level and rooted in the established teacher-student knowledge distillation paradigm. Our methodology begins with the development of a cross-domain teacher model, trained adversarially alongside a domain discriminator. This is followed by the creation of a target domain-specific student model. By employing the trained domain discriminator, we successfully segregate domain-shared from domain-specific representations. The teacher model guides the learning of domain-shared features, while domain-specific features are enhanced via contrastive learning methods. Experiments conducted on both public datasets and an industrial dataset demonstrate DDCDR achieves a new state-of-the-art performance. The implementation within Ant Group's platform further confirms its online efficacy, manifesting relative improvements of 0.33% and 0.45% in Unique Visitor Click-Through Rate (UVCTR) across two distinct recommendation scenarios, compared to baseline performances.|现代推荐平台经常包含多个域，以满足用户的不同偏好。最近，跨领域学习作为推荐系统范围内的一个重要范式已经获得了广泛的关注，使得能够利用来自资源丰富的源领域的丰富信息来增强目标领域，而这往往受到数据资源不足的限制。跨领域推荐的主要关注点是减轻负面转移-确保从源(领域共享知识)选择性转移相关知识，同时保持目标领域(领域特定知识)内领域独特见解的完整性。本文提出了一种新的基于分离角度的跨域推荐精馏框架(DDCDR) ，该框架基于已建立的师生知识精馏范式，设计在表示层次上进行操作。我们的方法开始于开发一个跨领域的教师模型，与领域鉴别器一起进行对抗性的培训。然后创建特定于目标领域的学生模型。通过使用训练有素的领域鉴别器，我们成功地将领域共享与领域特定的表示隔离开来。教师模型指导领域共享特征的学习，而领域特定特征通过对比学习方法得到增强。在公共数据集和工业数据集上进行的实验表明，DDCDR 实现了一种新的最先进的性能。蚂蚁集团平台的实施进一步证实了其在线功效，与基线表现相比，在两个不同的推荐场景中，Unique Visitor 点进率(UVCTR)的相对改善率分别为0.33% 和0.45% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DDCDR:+A+Disentangle-based+Distillation+Framework+for+Cross-Domain+Recommendation)|0|
|[Rankability-enhanced Revenue Uplift Modeling Framework for Online Marketing](https://doi.org/10.1145/3637528.3671516)|Bowei He, Yunpeng Weng, Xing Tang, Ziqiang Cui, Zexu Sun, Liang Chen, Xiuqiang He, Chen Ma|Renmin University of China, Beijing, China; City University of Hong Kong, Hong Kong, Hong Kong; FiT, Tencent, Shenzhen, China|Uplift modeling has been widely employed in online marketing by predicting the response difference between the treatment and control groups, so as to identify the sensitive individuals toward interventions like coupons or discounts. Compared with traditional conversion uplift modeling,revenue uplift modeling exhibits higher potential due to its direct connection with the corporate income. However, previous works can hardly handle the continuous long-tail response distribution in revenue uplift modeling. Moreover, they have neglected to optimize the uplift ranking among different individuals, which is actually the core of uplift modeling. To address such issues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss to regress the responses and customize the corresponding modeling network, which can be adapted to different existing uplift models. Then, we study the ranking-related uplift modeling error from the theoretical perspective and propose two tighter error bounds as the additional loss terms to the conventional response regression loss. Finally, we directly model the uplift ranking error for the entire population with a listwise uplift ranking loss. The experiment results on offline public and industrial datasets validate the effectiveness of our method for revenue uplift modeling. Furthermore, we conduct large-scale experiments on a prominent online fintech marketing platform, Tencent FiT, which further demonstrates the superiority of our method in real-world applications.|提升模型通过预测治疗组与对照组之间的反应差异，以识别对优惠券或折扣等干预措施敏感的个体，在网络营销中得到了广泛的应用。与传统的转换提升模型相比，收入提升模型由于与企业收入直接相关，因此具有更大的潜力。然而，以往的工作难以处理连续的长尾响应分布的收入提升模型。而且，他们忽略了优化不同个体之间的提升排序，这实际上是提升模型的核心。为了解决这些问题，本文首先利用零膨胀对数正态(ZILN)损失对响应进行回归，并定制相应的模型网络，以适应不同的现有抬升模型。然后，从理论角度研究了与排名相关的提升模型误差，提出了两个更严格的误差界作为常规响应回归损失的附加损失项。最后，我们直接模型的提升排名误差的整个人口与列表提升排名损失。在离线公共数据集和工业数据集上的实验结果验证了该方法对收入提升模型的有效性。此外，我们在一个著名的网上金融科技营销平台腾讯 FiT 上进行了大规模的实验，这进一步证明了我们的方法在实际应用中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rankability-enhanced+Revenue+Uplift+Modeling+Framework+for+Online+Marketing)|0|
|[Personalized Product Assortment with Real-time 3D Perception and Bayesian Payoff Estimation](https://doi.org/10.1145/3637528.3671518)|Porter Jenkins, Michael Selander, J. Stockton Jenkins, Andrew Merrill, Kyle Armstrong|Delicious AI, Lehi, UT, USA; Department of Computer Science, Brigham Young University, Provo, UT, USA|Product assortment selection is a critical challenge facing physical retailers. Effectively aligning inventory with the preferences of shoppers can increase sales and decrease out-of-stocks. However, in real-world settings the problem is challenging due to the combinatorial explosion of product assortment possibilities. Consumer preferences are typically heterogeneous across space and time, making inventory-preference alignment challenging. Additionally, existing strategies rely on syndicated data, which tends to be aggregated, low resolution, and suffer from high latency. To solve these challenges, we introduce a real-time recommendation system, which we call EdgeRec3D. Our system utilizes recent advances in 3D computer vision for perception and automatic, fine grained sales estimation. These perceptual components run on the edge of the network and facilitate real-time reward signals. Additionally, we develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR data. We rely on spatial clustering to allow the system to adapt to heterogeneous consumer preferences, and a graph-based candidate generation algorithm to address the combinatorial search problem. We test our system in real-world stores across two, 6-8 week A/B tests with beverage products and demonstrate a 35% and 27% increase in sales respectively. Finally, we monitor the deployed system for a period of 28 weeks with an observational study and show a 9.4% increase in sales.|产品分类选择是实体零售商面临的一个关键挑战。有效地调整库存与购物者的偏好可以增加销售和减少缺货。然而，在现实世界中，这个问题是具有挑战性的，因为产品分类的可能性是组合爆炸的。消费者的偏好在空间和时间上具有典型的异质性，这使得库存偏好的调整具有挑战性。此外，现有的策略依赖于聚合数据，这些数据往往是聚合的、低分辨率的，并且存在高延迟。为了解决这些问题，我们引入了一个实时推荐系统，我们称之为 EdgeRec3D。我们的系统利用三维计算机视觉的最新进展来进行感知和自动、细粒度的销售估算。这些感知成分运行在网络的边缘，便于实时奖励信号。此外，我们开发了贝叶斯支付模型，以考虑噪声估计从三维激光雷达数据。我们依靠空间聚类来使系统能够适应不同的消费者偏好，以及一个基于图的候选人生成算法来解决组合搜索问题。我们测试我们的系统在现实世界的商店两个，6-8周的 A/B 测试与饮料产品，并证明了35% 和27% 的销售分别增长。最后，我们对已部署的系统进行了为期28周的观察性研究监控，结果显示销售额增长了9.4% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Product+Assortment+with+Real-time+3D+Perception+and+Bayesian+Payoff+Estimation)|0|
|[Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation](https://doi.org/10.1145/3637528.3671758)|Kunlin Cai, Jinghuai Zhang, Zhiqing Hong, William Shand, Guang Wang, Desheng Zhang, Jianfeng Chi, Yuan Tian|Meta, New York, NY, USA; University of California, Los Angeles, Los Angeles, CA, USA; Rutgers University, New Brunswick, NJ, USA; Florida State University, Tallahassee, FL, USA|As location-based services (LBS) have grown in popularity, more human mobility data has been collected. The collected data can be used to build machine learning (ML) models for LBS to enhance their performance and improve overall experience for users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges.|随着基于位置的服务(LBS)越来越流行，越来越多的移动性数据被收集。所收集的数据可以用来为 LBS 建立机器学习(ML)模型，以提高它们的性能和改善用户的整体体验。然而，这种便利伴随着隐私泄露的风险，因为这类数据可能包含与用户身份有关的敏感信息，例如家庭/工作地点。此前的工作重点是保护移动数据在传输过程中或发布之前的隐私，缺乏基于移动数据的机器学习模型的隐私风险评估。为了更好地理解和量化基于移动性数据的机器学习模型中的隐私泄漏，我们设计了一个隐私攻击套件，其中包含针对感兴趣点(POI)推荐模型的数据提取和成员推断攻击，这是最广泛使用的基于移动性数据的机器学习模型之一。我们的攻击套件中的这些攻击假设不同的敌人知识，目的是从移动数据中提取不同类型的敏感信息，为 POI 推荐模型提供一个全面的隐私风险评估。我们的实验评估使用两个真实世界的移动性数据集表明，目前的 POI 推荐模型是脆弱的，我们的攻击。我们还提出了独特的发现，以了解哪些类型的移动数据更容易受到隐私攻击。最后，我们将评估针对这些攻击的防御措施，并强调未来的方向和挑战。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Where+Have+You+Been?+A+Study+of+Privacy+Risk+for+Point-of-Interest+Recommendation)|0|
|[Neural Retrievers are Biased Towards LLM-Generated Content](https://doi.org/10.1145/3637528.3671882)|Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, Jun Xu|; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Noah's Ark Lab, Huawei, Shenzhen, China|Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias.|最近，大语言模型(LLMs)的出现彻底改变了信息检索(IR)应用的范式，特别是在网络搜索中，通过在互联网上生成大量类似人类的文本。因此，LLM 时代的信息检索系统面临着新的挑战: 索引文档不仅由人工编写，而且由 LLM 自动生成。这些 LLM 生成的文档如何影响 IR 系统是一个紧迫的、尚未探索的问题。在这项工作中，我们进行了定量评估的情况下，国际关系模型的人写和 LLM 生成的文本都涉及。令人惊讶的是，我们的研究结果表明，神经检索模型往往排名 LLM 生成的文档更高。我们将神经检索器对 LLM 生成的内容的这类偏差称为源偏差。此外，我们发现这种偏差并不局限于第一阶段的神经检索，而是延伸到第二阶段的神经重新排序。然后，从文本压缩的角度进行深入分析，结果表明 LLM 生成的文本具有更集中的语义和更少的噪声，使得神经检索模型更容易进行语义匹配。为了减小源偏差，我们还提出了一个即插即用的去偏约束优化目标，实验结果表明其有效性。最后，我们讨论了由观察到的来源偏差引起的潜在的严重关切，并希望我们的发现可以作为对 IR 社区和其他方面的一个关键的警告。为了促进 LLM 时代对信息检索的未来探索，构建了两个新的基准 https://github.com/kid-22/source-bias。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Retrievers+are+Biased+Towards+LLM-Generated+Content)|0|
|[DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation](https://doi.org/10.1145/3637528.3672008)|Kounianhua Du, Jizheng Chen, Jianghao Lin, Yunjia Xi, Hangyu Wang, Xinyi Dai, Bo Chen, Ruiming Tang, Weinan Zhang|Huawei Noah's Ark Lab, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Noah's Ark Lab, Shanghai, China|Recommender systems play important roles in various applications such ase-commerce, social media, etc. Conventional recommendation methods usuallymodel the collaborative signals within the tabular representation space.Despite the personalization modeling and the efficiency, the latent semanticdependencies are omitted. Methods that introduce semantics into recommendationthen emerge, injecting knowledge from the semantic representation space wherethe general language understanding are compressed. However, existingsemantic-enhanced recommendation methods focus on aligning the two spaces,during which the representations of the two spaces tend to get close while theunique patterns are discarded and not well explored. In this paper, we proposeDisCo to Disentangle the unique patterns from the two representation spaces andCollaborate the two spaces for recommendation enhancement, where both thespecificity and the consistency of the two spaces are captured. Concretely, wepropose 1) a dual-side attentive network to capture the intra-domain patternsand the inter-domain patterns, 2) a sufficiency constraint to preserve thetask-relevant information of each representation space and filter out thenoise, and 3) a disentanglement constraint to avoid the model from discardingthe unique information. These modules strike a balance between disentanglementand collaboration of the two representation spaces to produce informativepattern vectors, which could serve as extra features and be appended toarbitrary recommendation backbones for enhancement. Experiment results validatethe superiority of our method against different models and the compatibility ofDisCo over different backbones. Various ablation studies and efficiencyanalysis are also conducted to justify each model component.|推荐系统在电子商务、社交媒体等各种应用中发挥着重要作用。传统的推荐方法通常是在表格表示空间中对协作信号进行建模。尽管个性化建模和效率，潜在的语义依赖性被忽略。然后出现将语义引入推荐的方法，从语义表示空间注入知识，在这个空间中一般语言理解被压缩。然而，现有的语义增强推荐方法侧重于对齐这两个空间，在此期间，两个空间的表示趋于接近，而唯一的模式被丢弃，没有得到很好的探索。在本文中，我们提出了 DisCo 从两个表示空间中分离出唯一的模式，并协作两个空间进行推荐增强，同时捕获两个空间的特殊性和一致性。具体来说，我们提出了两种方案: 1)双侧注意网络捕获域内模式和域间模式; 2)充分约束保留每个表示空间的任务相关信息并过滤掉噪声; 3)解缠约束避免模型丢弃唯一信息。这些模块在两个表示空间的分离和协作之间取得了平衡，从而产生了信息模式向量，这些向量可以作为额外的特征，并附加到任意的推荐主干上进行增强。实验结果验证了该方法对不同模型的优越性以及 DisCo 在不同骨架上的兼容性。各种消融研究和效率分析也进行了验证每个模型组件。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DisCo:+Towards+Harmonious+Disentanglement+and+Collaboration+between+Tabular+and+Semantic+Space+for+Recommendation)|0|
|[Label Shift Correction via Bidirectional Marginal Distribution Matching](https://doi.org/10.1145/3637528.3671867)|Ruidong Fan, Xiao Ouyang, Hong Tao, Chenping Hou|National University of Defense Technology, Changsha, Hunan, China|Due to the timeliness and uncertainty of data acquisition, label shift, which assumes that the source (training) and target (test) label distributions differ, occurs with the changing environment and reduces the generalization ability of traditional models. To correct the label shift, existing methods estimate the true label distribution by prediction of target data from a source classifier, which results in high variance, especially with large label shift. In this paper, we tackle this problem by proposing a novel approach termed as Label Shift Correction via Bidirectional Marginal Distribution Matching (BMDM). Our approach matchs the label and feature marginal distributions simultaneously to ensure the stability of estimated class proportions. We prove theoretically that there is a unique optimal solution, i.e., true target label distribution, for our approach under mild conditions, and an efficient optimization strategy is also proposed. On this basis, in multi-shot scenario where label distribution changes continuously, we extend BMDM by designing a new distribution matching mechanism and constructing a regularization term that constrains the direction of label distribution change. Extensive experimental results validate the effectiveness of our approach over existing state-of-the-arts methods.|由于数据采集的及时性和不确定性，假设源(训练)和目标(测试)标签分布不同的标签偏移随着环境的变化而发生，降低了传统模型的泛化能力。为了校正标签偏移，现有的方法通过预测源分类器的目标数据来估计真实的标签分布，这导致了很大的方差，尤其是标签偏移。在本文中，我们提出了一种新的方法来解决这个问题，这种方法被称为双向边缘分布匹配的标签偏移校正(bMDM)。我们的方法同时匹配标签和特征的边际分布，以确保估计的类比例的稳定性。从理论上证明了该方法在温和条件下存在唯一的最优解，即真实目标标签分布，并提出了一种有效的优化策略。在此基础上，在标签分布不断变化的多镜头场景下，通过设计一种新的分布匹配机制，构造一个约束标签分布变化方向的正则项，对 BMDM 进行了扩展。大量的实验结果验证了我们的方法比现有的最先进的方法更有效。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Shift+Correction+via+Bidirectional+Marginal+Distribution+Matching)|0|
|[On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-n Recommendation](https://doi.org/10.1145/3637528.3671687)|Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko|ShareChat, London, United Kingdom; ShareChat, Edinburgh, United Kingdom|Approaches to recommendation are typically evaluated in one of two ways: (1)via a (simulated) online experiment, often seen as the gold standard, or (2)via some offline evaluation procedure, where the goal is to approximate theoutcome of an online experiment. Several offline evaluation metrics have beenadopted in the literature, inspired by ranking metrics prevalent in the fieldof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is onesuch metric that has seen widespread adoption in empirical studies, and higher(n)DCG values have been used to present new methods as the state-of-the-art intop-n recommendation for many years. Our work takes a critical look at this approach, and investigates when we canexpect such metrics to approximate the gold standard outcome of an onlineexperiment. We formally present the assumptions that are necessary to considerDCG an unbiased estimator of online reward and provide a derivation for thismetric from first principles, highlighting where we deviate from itstraditional uses in IR. Importantly, we show that normalising the metricrenders it inconsistent, in that even when DCG is unbiased, ranking competingmethods by their normalised DCG can invert their relative order. Through acorrelation analysis between off- and on-line experiments conducted on alarge-scale recommendation platform, we show that our unbiased DCG estimatesstrongly correlate with online reward, even when some of the metric's inherentassumptions are violated. This statement no longer holds for its normalisedvariant, suggesting that nDCG's practical utility may be limited.|推荐方法通常以两种方式之一进行评估: (1)通过(模拟)在线实验，通常被视为黄金标准，或者(2)通过一些离线评估程序，其目标是近似在线实验的结果。文献中已经采用了一些线下评估指标，这些指标的灵感来自于信息检索领域中流行的排名指标。(标准化)贴现累积增益(nDCG)是在实证研究中得到广泛采用的一种度量标准，较高的(n) DCG 值已被用于表示新方法作为最先进的推荐方法多年。我们的工作对这种方法进行了批判性的研究，并且调查了我们什么时候可以期望这些指标接近在线实验的黄金标准结果。我们正式提出的假设是必要的，认为 DCG 是一个公正的估计在线奖励，并提供了从第一原则这一度量的推导，突出了我们偏离其传统用途在 IR。重要的是，我们表明，规范化的度量呈现它不一致，即使当 DCG 是无偏见的，排名竞争方法的规范化 DCG 可以颠倒他们的相对顺序。通过在大规模推荐平台上进行的离线和在线实验之间的相关性分析，我们表明我们的无偏 DCG 估计与在线奖励强烈相关，即使一些度量的固有假设被违背。这种说法不再适用于它的正常化变体，表明 nDCG 的实际用途可能是有限的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+(Normalised)+Discounted+Cumulative+Gain+as+an+Off-Policy+Evaluation+Metric+for+Top-n+Recommendation)|0|
|[FairMatch: Promoting Partial Label Learning by Unlabeled Samples](https://doi.org/10.1145/3637528.3671685)|Jiahao Jiang, Yuheng Jia, Hui Liu, Junhui Hou|School of Computing & Information Sciences, Saint Francis University, HongKong, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Department of Computer Science, City University of Hong Kong, HongKong, China; College of Software Engineering, Southeast University, Nanjing, China|This paper studies the semi-supervised partial label learning (SSPLL) problem, which aims to improve the partial label learning (PLL) by leveraging unlabeled samples. Both the existing SSPLL methods and the semi-supervised learning methods exploit the information in unlabeled samples by selecting high-confidence unlabeled samples as the pseudo labels based on the maximum value of the model output. However, the scarcity of labeled samples and the ambiguity from partial labels skew this strategy towards an unfair selection of high-confidence samples on each class, most notably during the initial phases of training, resulting in slower training and performance degradation. In this paper, we propose a novel method FairMatch, which adopts a learning state aware self-adaptive threshold for selecting the same number of high-confidence samples on each class, and uses augmentation consistency to incorporate the unlabeled samples to promote PLL. In addition, we adopt the candidate label disambiguation to utilize the partial labeled samples and mix up the partial labeled samples and the selected high-confidence unlabeled samples to prevent the model from overfitting on partial label samples. FairMatch can achieve maximum accuracy improvements of 9.53%, 4.9%, and 16.45% on CIFAR-10, CIFAR-100, and CIFAR-100H, respectively. The codes can be found at https://github.com/jhjiangSEU/FairMatch.|本文研究了半监督部分标记学习(SSPLL)问题，旨在利用未标记样本改进部分标记学习(PLL)。现有的 SSPLL 方法和半监督学习方法都是根据模型输出的最大值，选择高置信度的未标记样本作为伪标签，从而利用未标记样本的信息。然而，标记样本的稀缺性和部分标记的模糊性使该策略倾向于在每个类别上不公平地选择高置信度样本，最显着的是在训练的初始阶段，导致训练较慢和性能下降。本文提出了一种新的 FairMatch 方法，该方法采用一种学习状态感知的自适应阈值来选择每类中相同数量的高置信度样本，并使用增强一致性来合并未标记的样本来提升锁相环。此外，我们采用候选标签消歧的方法，利用部分标签样本，混合部分标签样本和选择的高置信度未标签样本，以防止模型对部分标签样本的过度拟合。FairMatch 可以分别在 CIFAR-10，CIFAR-100和 CIFAR-100H 上实现9.53% ，4.9% 和16.45% 的最大准确性改进。密码可以在 https://github.com/jhjiangseu/fairmatch 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairMatch:+Promoting+Partial+Label+Learning+by+Unlabeled+Samples)|0|
|[Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation](https://doi.org/10.1145/3637528.3671872)|Qingyao Li, Wei Xia, Li'ang Yin, Jiarui Jin, Yong Yu|Huawei Noah's Ark Lab, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China|Educational recommendation seeks to suggest knowledge concepts that match a learner's ability, thus facilitating a personalized learning experience. In recent years, reinforcement learning (RL) methods have achieved considerable results by taking the encoding of the learner's exercise log as the state and employing an RL-based agent to make suitable recommendations. However, these approaches suffer from handling the diverse and dynamic learner's knowledge states. In this paper, we introduce the privileged feature distillation technique and propose the P rivileged K nowledge S tate D istillation (PKSD ) framework, allowing the RL agent to leverage the "actual'' knowledge state as privileged information in the state encoding to help tailor recommendations to meet individual needs. Concretely, our PKSD takes the privileged knowledge states together with the representations of the exercise log for the state representations during training. And through distillation, we transfer the ability to adapt to learners to aknowledge state adapter. During inference, theknowledge state adapter would serve as the estimated privileged knowledge states instead of the real one since it is not accessible. Considering that there are strong connections among the knowledge concepts in education, we further propose to collaborate the graph structure learning for concepts into our PKSD framework. This new approach is termed GEPKSD (Graph-Enhanced PKSD). As our method is model-agnostic, we evaluate PKSD and GEPKSD by integrating them with five different RL bases on four public simulators, respectively. Our results verify that PKSD can consistently improve the recommendation performance with various RL methods, and our GEPKSD could further enhance the effectiveness of PKSD in all the simulations.|教育推荐旨在建议符合学习者能力的知识概念，从而促进个性化的学习体验。近年来，以学习者运动日志的编码为状态，并使用基于强化学习的代理来提出合适的建议，这些方法已经取得了相当大的成果。然而，这些方法在处理多样化和动态学习者的知识状态时存在缺陷。本文介绍了特权特征提取技术，提出了 P 特权 K 知识 S 状态 D 提取(PKSD)框架，允许 RL 代理利用“实际”知识状态作为状态编码中的特权信息，帮助定制推荐以满足个体需求。具体地说，我们的 PKSD 将特权知识状态与训练日志的表示一起提取，用于训练过程中的状态表示。并通过升华，将学习者的适应能力转化为知识状态适应能力。在推理过程中，知识状态适配器将作为估计的特权知识状态，而不是实际知识状态，因为它是不可访问的。考虑到教育中知识概念之间的紧密联系，我们进一步提出将概念的图形结构学习协同到 PKSD 框架中。这种新的方法被称为 GEPKSD (图形增强 PKSD)。由于我们的方法是模型无关的，所以我们分别在四个公共模拟器上将 PKSD 和 GEPKSD 与五个不同的 RL 基地集成在一起进行评估。实验结果表明，PKSD 能够持续改善各种 RL 方法的推荐性能，而 GEPKSD 能够进一步提高 PKSD 在所有仿真中的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privileged+Knowledge+State+Distillation+for+Reinforcement+Learning-based+Educational+Path+Recommendation)|0|
|[Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach](https://doi.org/10.1145/3637528.3671848)|Yicong Li, Yu Yang, Jiannong Cao, Shuaiqi Liu, Haoran Tang, Guandong Xu|The Education University of Hong Kong & University of Technology Sydney, Hong Kong, Hong Kong; The Hong Kong Polytechnic University, Hong Kong, Hong Kong|Recent studies successfully learned static graph embeddings that arestructurally fair by preventing the effectiveness disparity of high- andlow-degree vertex groups in downstream graph mining tasks. However, achievingstructure fairness in dynamic graph embedding remains an open problem.Neglecting degree changes in dynamic graphs will significantly impair embeddingeffectiveness without notably improving structure fairness. This is because theembedding performance of high-degree and low-to-high-degree vertices willsignificantly drop close to the generally poorer embedding performance of mostslightly changed vertices in the long-tail part of the power-law distribution.We first identify biased structural evolutions in a dynamic graph based on theevolving trend of vertex degree and then propose FairDGE, the firststructurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biasedstructural evolutions by jointly embedding the connection changes amongvertices and the long-short-term evolutionary trend of vertex degrees.Furthermore, a novel dual debiasing approach is devised to encode fairembeddings contrastively, customizing debiasing strategies for different biasedstructural evolutions. This innovative debiasing strategy breaks theeffectiveness bottleneck of embeddings without notable fairness loss. Extensiveexperiments demonstrate that FairDGE achieves simultaneous improvement in theeffectiveness and fairness of embeddings.|最近的研究成功地学习了静态图嵌入，通过防止有效性差异的高度和低度顶点组在下游图挖掘任务。然而，在动态图嵌入中实现结构公平性仍然是一个悬而未决的问题。忽略动态图的度变化会显著降低嵌入效率，而不能显著提高结构的公平性。这是因为在幂律分布的长尾部分，高度顶点和低到高度顶点的嵌入性能会明显下降，接近于最微小变化顶点的嵌入性能普遍较差。我们首先根据顶点度的演化趋势在动态图中识别有偏的结构演化，然后提出 FairDGE，第一个结构公平的动态图嵌入算法。FairDGE 通过联合嵌入顶点之间的联系变化和顶点度的长期短期演化趋势来学习有偏的结构演化。此外，设计了一种新的双重消偏方法来对比编码公平层合，定制消偏策略以适应不同的有偏结构演化。这种创新的去偏策略打破了嵌入的有效性瓶颈，没有明显的公平性损失。大量实验表明，FairDGE 算法同时提高了嵌入的有效性和公平性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Toward+Structure+Fairness+in+Dynamic+Graph+Embedding:+A+Trend-aware+Dual+Debiasing+Approach)|0|
|[Bridging Items and Language: A Transition Paradigm for Large Language Model-Based Recommendation](https://doi.org/10.1145/3637528.3671884)|Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, SeeKiong Ng, TatSeng Chua|The Hong Kong Polytechnic University, Hong Kong SAR, China; University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, Singapore|Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging, which relies on two fundamental steps to bridge the recommendation item space and the language space: 1) item indexing utilizes identifiers to represent items in the language space, and 2) generation grounding associates LLMs' generated token sequences to in-corpus items. However, previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers (e.g., titles) either lose semantics or lack adequate distinctiveness. Moreover, prior generation grounding methods might generate invalid identifiers, thus misaligning with in-corpus items. To address these issues, we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically, TransRec presents multi-facet identifiers, which simultaneously incorporate ID, title, and attribute for item indexing to pursue both distinctiveness and semantics. Additionally, we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly, TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models, BART-large and LLaMA-7B. Extensive results on three real-world datasets under diverse settings validate the superiority of TransRec.|利用大型语言模型(LLM)进行推荐正在迅速兴起，这依赖于两个基本步骤来连接推荐项空间和语言空间: 1)项索引利用标识符来表示语言空间中的项目，2)生成基础将 LLM 生成的令牌序列与语料库中的项目相关联。然而，以前的方法在这两个步骤中表现出固有的局限性。现有的基于 ID 的标识符(例如，数字 ID)和基于描述的标识符(例如，标题)要么失去语义，要么缺乏足够的区别性。此外，上一代接地方法可能会产生无效的标识符，从而与语料库中的项目不一致。为了解决这些问题，我们提出了一种新的基于 LLM 的传输范式(称为 TransRec) ，以连接项目和语言。具体来说，TransRec 提供了多方面标识符，这些标识符同时合并 ID、 title 和属性用于项目索引，以实现独特性和语义。此外，我们还为 TransRec 引入了专门的数据结构，以确保只生成有效的标识符，并利用子字符串索引鼓励 LLM 从标识符的任何位置生成标识符。最后，TransRec 提出了一个聚合的接地模块，利用生成的多方面标识符对语料库中的项目进行有效排序。我们在两个骨干模型上实例化 TransRec，BART-large 和 LLaMA-7B。在三个不同设置的真实世界数据集上的广泛结果验证了 TransRec 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+Items+and+Language:+A+Transition+Paradigm+for+Large+Language+Model-Based+Recommendation)|0|
|[BadSampler:  Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning](https://doi.org/10.1145/3637528.3671879)|Yi Liu, Cong Wang, Xingliang Yuan|The University of Melbourne, Melbourne, Australia; City University of Hong Kong, Hong Kong, China|Federated Learning (FL) is susceptible to poisoning attacks, wherein compromised clients manipulate the global model by modifying local datasets or sending manipulated model updates. Experienced defenders can readily detect and mitigate the poisoning effects of malicious behaviors using Byzantine-robust aggregation rules. However, the exploration of poisoning attacks in scenarios where such behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poisoning Byzantine-robust FL by introducing catastrophic forgetting. To fill this gap, we first formally define generalization error and establish its connection to catastrophic forgetting, paving the way for the development of a clean-label data poisoning attack named BadSampler. This attack leverages only clean-label data (i.e., without poisoned data) to poison Byzantine-robust FL and requires the adversary to selectively sample training data with high loss to feed model training and maximize the model's generalization error. We formulate the attack as an optimization problem and present two elegant adversarial sampling strategies, Top-k sampling, and meta-sampling, to approximately solve it. Additionally, our formal error upper bound and time complexity analysis demonstrate that our design can preserve attack utility with high efficiency. Extensive evaluations on two real-world datasets illustrate the effectiveness and performance of our proposed attacks.|联邦学习(FL)容易受到中毒攻击，其中受损的客户端通过修改本地数据集或发送受控模型更新来操纵全局模型。经验丰富的防御者可以很容易地使用拜占庭稳健的聚合规则检测和减轻恶意行为的毒害效应。然而，在没有这种行为的情况下，对中毒攻击的探索对于拜占庭-鲁棒 FL 来说仍然很大程度上是未知的。本文通过引入灾难遗忘，解决了对拜占庭-鲁棒 FL 中毒的挑战性问题。为了填补这一空白，我们首先正式定义了泛化误差，并建立了它与灾难性遗忘之间的联系，为名为 BadSampler 的清洁标签数据中毒攻击的开发铺平了道路。这种攻击只利用干净的标签数据(即，没有中毒的数据)来毒害拜占庭-鲁棒的 FL，并要求对手有选择地采样高损失的训练数据来提供模型训练，并最大限度地提高模型的泛化误差。我们把这种攻击作为一种最佳化问题，并提出了两种优雅的对抗性抽样策略: Top-k 抽样和 meta 抽样，来近似地解决这个问题。此外，我们的形式误差上限和时间复杂度分析表明，我们的设计可以保持攻击效用的高效率。对两个真实世界数据集的广泛评估说明了我们提出的攻击的有效性和性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BadSampler:++Harnessing+the+Power+of+Catastrophic+Forgetting+to+Poison+Byzantine-robust+Federated+Learning)|0|
|[Dataset Condensation for Time Series Classification via Dual Domain Matching](https://doi.org/10.1145/3637528.3671675)|Zhanyu Liu, Ke Hao, Guanjie Zheng, Yanwei Yu|Shanghai Jiao Tong University, Shanghai, China; Ocean University of China, Qingdao, China|Time series data has been demonstrated to be crucial in various researchfields. The management of large quantities of time series data presentschallenges in terms of deep learning tasks, particularly for training a deepneural network. Recently, a technique named Dataset Condensation hasemerged as a solution to this problem. This technique generates a smallersynthetic dataset that has comparable performance to the full real dataset indownstream tasks such as classification. However, previous methods areprimarily designed for image and graph datasets, and directly adapting them tothe time series dataset leads to suboptimal performance due to their inabilityto effectively leverage the rich information inherent in time series data,particularly in the frequency domain. In this paper, we propose a novelframework named Dataset Condensation forTime SeriesClassification via Dual Domain Matching (CondTSC)which focuses on the time series classification dataset condensation task.Different from previous methods, our proposed framework aims to generate acondensed dataset that matches the surrogate objectives in both the time andfrequency domains. Specifically, CondTSC incorporates multi-view dataaugmentation, dual domain training, and dual surrogate objectives to enhancethe dataset condensation process in the time and frequency domains. Throughextensive experiments, we demonstrate the effectiveness of our proposedframework, which outperforms other baselines and learns a condensed syntheticdataset that exhibits desirable characteristics such as conforming to thedistribution of the original data.|时间序列数据已被证明在各个研究领域都是至关重要的。大量时间序列数据的管理在深度学习任务方面面临挑战，特别是在训练深度神经网络方面。最近，一种名为“数据集压缩”的技术被用来解决这个问题。该技术生成一个较小的合成数据集，其性能与完整的实际数据集的下游任务(如分类)具有可比性。然而，以前的方法主要是为图像和图形数据集设计的，直接将它们适应于时间序列数据集会导致次优性能，因为它们无法有效地利用时间序列数据中固有的丰富信息，特别是在频率域。本文提出了一种基于双域匹配的时间序列分类数据集压缩框架(CondTSC) ，该框架主要针对时间序列分类数据集压缩任务。与以往的方法不同，我们提出的框架旨在生成在时间和频率领域匹配替代目标的浓缩数据集。具体来说，CondTSC 结合了多视图数据增强、双域训练和双代理目标来增强数据集在时间和频率域的缩聚过程。通过大量的实验，我们证明了我们提出的框架的有效性，它的性能优于其他基线，并学习了一个浓缩的合成数据集，这个数据集展示了令人满意的特征，例如符合原始数据的分布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+Condensation+for+Time+Series+Classification+via+Dual+Domain+Matching)|0|
|[Self-Supervised Denoising through Independent Cascade Graph Augmentation for Robust Social Recommendation](https://doi.org/10.1145/3637528.3671958)|Youchen Sun, Zhu Sun, Yingpeng Du, Jie Zhang, Yew Soon Ong|; Nanyang Technological University, Singapore, Singapore; ASTAR Centre for Frontier AI Research & Nanyang Technological University, Singapore, Singapore|Social Recommendation (SR) typically exploits neighborhood influence in the social network to enhance user preference modeling. However, users' intricate social behaviors may introduce noisy social connections for user modeling and harm the models' robustness. Existing solutions to alleviate social noise either filter out the noisy connections or generate new potential social connections. Due to the absence of labels, the former approaches may retain uncertain connections for user preference modeling while the latter methods may introduce additional social noise. Through data analysis, we discover that (1) social noise likely comes from the connected users with low preference similarity; and (2) Opinion Leaders (OLs) play a pivotal role in influence dissemination, surpassing high-similarity neighbors, regardless of their preference similarity with trusting peers. Guided by these observations, we propose a novel Self-Supervised Denoising approach through Independent Cascade Graph Augmentation, for more robust SR. Specifically, we employ the independent cascade diffusion model to generate an augmented graph view, which traverses the social graph and activates the edges in sequence to simulate the cascading influence spread. To steer the augmentation towards a denoised social graph, we (1) introduce a hierarchical contrastive loss to prioritize the activation of OLs first, followed by high-similarity neighbors, while weakening the low-similarity neighbors; and (2) integrate an information bottleneck based contrastive loss, aiming to minimize mutual information between original and augmented graphs yet preserve sufficient information for improved SR. Experiments conducted on two public datasets demonstrate that our model outperforms the state-of-the-art while also exhibiting higher robustness to different extents of social noise.|社交推荐(SR)通常利用社交网络中的邻域影响来增强用户偏好建模。然而，用户错综复杂的社会行为可能为用户建模引入噪声社会关系，损害模型的鲁棒性。现有的减轻社会噪音的解决方案要么过滤掉噪音连接，要么产生新的潜在社会连接。由于标签的缺失，前一种方法在用户偏好建模时可能会保留不确定的联系，而后一种方法可能会引入额外的社会噪声。通过数据分析，我们发现: (1)社交噪声可能来自偏好相似度较低的关联用户; (2)意见领袖(OLs)在影响力传播中发挥着关键作用，超过了高相似度的邻居，而不管他们与信任同伴的偏好相似度如何。在这些观测结果的指导下，我们提出了一种新的自我监督去噪方法，通过独立级联图增强，更健壮的 SR。具体来说，我们采用独立的级联扩散模型来生成一个扩展图视图，该视图横穿社会图并依次激活边界，以模拟级联影响的传播。为了将扩展引向去噪的社会图，我们(1)引入分层对比损失来优先激活 OLs，然后是高相似性邻居，同时弱化低相似性邻居; (2)整合基于对比损失的信息瓶颈，旨在最小化原始图和扩展图之间的相互信息，同时保留足够的信息以改善 SR。在两个公共数据集上进行的实验表明，我们的模型优于最先进的水平，同时也表现出对不同程度的社会噪声更高的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Denoising+through+Independent+Cascade+Graph+Augmentation+for+Robust+Social+Recommendation)|0|
|[Revisiting Local PageRank Estimation on Undirected Graphs: Simple and Optimal](https://doi.org/10.1145/3637528.3671820)|Hanzhi Wang|Renmin University of China, Beijing, China|We propose a simple and optimal algorithm, BackMC, for local PageRank estimation in undirected graphs: given an arbitrary target node t in an undirected graph G comprising n nodes and m edges, BackMC accurately estimates the PageRank score of node t while assuring a small relative error and a high success probability. The worst-case computational complexity of BackMC is upper bounded by O(1/dmin ⋅ min(dt, m1/2)), where dmin denotes the minimum degree of G, and dt denotes the degree of t, respectively. Compared to the previously best upper bound of O(log n ⋅ min(dt, m1/2)) (VLDB '23), which is derived from a significantly more complex algorithm and analysis, our BackMC improves the computational complexity for this problem by a factor of Θ(log n/dmin) with a much simpler algorithm. Furthermore, we establish a matching lower bound of Ω(1/dmin ⋅ min(dt, m1/2)) for any algorithm that attempts to solve the problem of local PageRank estimation, demonstrating the theoretical optimality of our BackMC. We conduct extensive experiments on various large-scale real-world and synthetic graphs, where BackMC consistently shows superior performance.|针对无向图的局部 PageRank 估计问题，提出了一种简单而优化的 BackMC 算法: 在包含 n 个节点和 m 条边的无向图 G 中，给定一个任意目标节点 t，BackMC 在保证较小的相对误差和较高的成功概率的情况下，精确地估计节点 t 的 PageRank 得分。最坏情况下 BackMC 的计算复杂度上界为 O (1/dmin ≥ min (dt，m1/2)) ，其中 dmin 表示 G 的最小度，dt 表示 t 的最小度。相比之前的最佳上界 O (log n  (dt，m1/2))(VLDB’23) ，它是由一个更加复杂的算法和分析得到的，我们的 BackMC 用一个更加简单的算法提高了 Θ (log n/dmin)的一个因子，从而提高了这个问题的计算复杂度。进一步，我们建立了任何试图解决局部 PageRank 估计问题的算法的匹配下界 Ω (1/dmin min (dt，m1/2)) ，证明了我们的 BackMC 算法的理论最优性。我们在各种大规模的真实世界和合成图上进行广泛的实验，其中 BackMC 始终显示出优越的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Local+PageRank+Estimation+on+Undirected+Graphs:+Simple+and+Optimal)|0|
|[Performative Debias with Fair-exposure Optimization Driven by Strategic Agents in Recommender Systems](https://doi.org/10.1145/3637528.3671786)|Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, Jianping Fan|; The Hong Kong University of Science and Technology, Hong Kong, China; AI Lab at Lenovo Research, Beijing, China|Data bias, e.g., popularity impairs the dynamics of two-sided markets within recommender systems. This overshadows the less visible but potentially intriguing long-tail items that could capture user interest. Despite the abundance of research surrounding this issue, it still poses challenges and remains a hot topic in academic circles. Along this line, in this paper, we developed a re-ranking approach in dynamic settings with fair-exposure optimization driven by strategic agents. Designed for the producer side, the execution of agents assumes content creators can modify item features based on strategic incentives to maximize their exposure. This iterative process entails an end-to-end optimization, employing differentiable ranking operators that simultaneously target accuracy and fairness. Joint objectives ensure the performance of recommendations while enhancing the visibility of tail items. We also leveraged the performativity nature of predictions to illustrate how strategic learning influences content creators to shift towards fairness efficiently, thereby incentivizing features of tail items. Through comprehensive experiments on both public and industrial datasets, we have substantiated the effectiveness and dominance of the proposed method especially on unveiling the potential of tail items.|数据偏差，例如，受欢迎程度会损害推荐系统中双边市场的动态性。这掩盖了不太可见但可能引起用户兴趣的长尾项目。尽管围绕这一问题进行了大量的研究，但它仍然是学术界面临的挑战和热点问题。沿着这条路线，本文开发了一种在动态环境下的重新排序方法，其中公平曝光优化是由战略代理驱动的。代理的执行是为生产者设计的，它假定内容创造者可以基于战略激励来修改项目特征，以最大限度地提高他们的曝光率。这个迭代过程需要一个端到端的优化，使用可微排名运算符，同时目标的准确性和公平性。联合目标确保建议的执行，同时提高尾部项目的可见性。我们还利用预测的执行性质来说明战略学习如何影响内容创建者有效地转向公平，从而激励尾部项目的特性。通过对公共数据集和工业数据集的综合实验，验证了该方法的有效性和优越性，特别是在揭示尾部项目的潜力方面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Performative+Debias+with+Fair-exposure+Optimization+Driven+by+Strategic+Agents+in+Recommender+Systems)|0|
|[Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning](https://doi.org/10.1145/3637528.3671663)|Yidan Xing, Zhenzhe Zheng, Fan Wu|Shanghai Jiao Tong University, Shanghai, China|Vertical federated learning (VFL) is an emerging collaborative machine learning paradigm to facilitate the utilization of private features distributed across multiple parties. During the inference process of VFL, the involved parties need to upload their local embeddings to be aggregated for the final prediction. Despite its remarkable performances, the inference process of the current VFL system is vulnerable to the strategic behavior of involved parties, as they could easily change the uploaded local embeddings to exert direct influences on the prediction result. In a representative case study of federated recommendation, we find the allocation of display opportunities to be severely disrupted due to the parties' preferences in display content. In order to elicit the true local embeddings for VFL system, we propose a distribution-based penalty mechanism to detect and penalize the strategic behaviors in collaborative inference. As the key motivation of our design, we theoretically prove the power of constraining the distribution of uploaded embeddings in preventing the dishonest parties from achieving higher utility. Our mechanism leverages statistical two-sample tests to distinguish whether the distribution of uploaded embeddings is reasonable, and penalize the dishonest party through deactivating her uploaded embeddings. The resulted mechanism could be shown to admit truth-telling to converge to a Bayesian Nash equilibrium asymptotically under mild conditions. The experimental results further demonstrate the effectiveness of the proposed mechanism to reduce the dishonest utility increase of strategic behaviors and promote the truthful uploading of local embeddings in inferences.|垂直联邦学习(VFL)是一种新兴的协作机器学习范式，它有助于利用分布在多个方面的私有特性。在 VFL 的推理过程中，各参与方需要上传自己的局部嵌入信息进行聚合，才能得到最终的预测结果。当前 VFL 系统的推理过程虽然具有显著的性能，但容易受到相关各方策略行为的影响，因为它们很容易改变上传的局部嵌入，从而直接影响预测结果。在联邦推荐的一个典型案例中，我们发现由于各方对显示内容的偏好，显示机会的分配会受到严重干扰。为了在 VFL 系统中实现真正的局部嵌入，我们提出了一种基于分布的惩罚机制来检测和惩罚协同推理中的策略行为。作为我们设计的关键动机，我们从理论上证明了约束上传嵌入分布的力量，防止不诚实的当事人获得更高的效用。我们的机制利用统计双样本检验来判断上传嵌入的分布是否合理，并通过停用不诚实方的上传嵌入来惩罚不诚实方。结果显示，在温和的条件下，该机制能够承认说实话，并渐近地收敛到贝叶斯纳什均衡点。实验结果进一步证明了该机制在减少策略行为的不诚实效用增加和促进推理中局部嵌入的真实上传方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preventing+Strategic+Behaviors+in+Collaborative+Inference+for+Vertical+Federated+Learning)|0|
|[Extreme Meta-Classification for Large-Scale Zero-Shot Retrieval](https://doi.org/10.1145/3637528.3672046)|Sachin Yadav, Deepak Saini, Anirudh Buvanesh, Bhawna Paliwal, Kunal Dahiya, Siddarth Asokan, Yashoteja Prabhu, Jian Jiao, Manik Varma|Microsoft Research, Bangalore, India; Indian Institute of Technology, Delhi, India; Microsoft, Redmond, WA, USA|We develop accurate and efficient solutions for large-scale retrieval tasks where novel (zero-shot) items can arrive continuously at a rapid pace. Conventional Siamese-style approaches embed both queries and items through a small encoder and retrieve the items lying closest to the query. While this approach allows efficient addition and retrieval of novel items, the small encoder lacks sufficient capacity for the necessary world knowledge in complex retrieval tasks. The extreme classification approaches have addressed this by learning a separate classifier for each item observed in the training set which significantly increases the representation capacity of the model. Such classifiers outperform Siamese approaches on observed items, but cannot be trained for novel items due to data and latency constraints. To bridge these gaps, this paper develops: (1) A new algorithmic framework, EMMETT, which efficiently synthesizes classifiers on-the-fly for novel items, by relying on the readily available classifiers for observed items; (2) A new algorithm, IRENE, which is a simple and effective instance of EMMETT that is specifically suited for large-scale deployments, and (3) A new theoretical framework for analyzing the generalization performance in large-scale zero-shot retrieval which guides our algorithm and training related design decisions. Comprehensive experiments are conducted on a wide range of retrieval tasks which demonstrate that IRENE improves the zero-shot retrieval accuracy by up to 15% points in Recall@10 when added on top of leading encoders. Additionally, on an online A/B test in a large-scale ad retrieval task in a major search engine, IRENE improved the ad click-through rate by 4.2%. Lastly, we validate our design choices through extensive ablative experiments. The source code for IRENE is available at https://aka.ms/irene.|我们开发准确和有效的解决方案，大规模的检索任务，新的(零射击)项目可以连续到达快速的步伐。传统的暹罗式方法通过一个小编码器嵌入查询和项，并检索与查询最接近的项。虽然这种方法可以有效地增加和检索新的项目，小编码器缺乏足够的能力，必要的世界知识在复杂的检索任务。极端分类方法通过为训练集中观察到的每个项目学习一个单独的分类器来解决这个问题，这大大提高了模型的表示能力。这种分类器在观察项目上的表现优于暹罗方法，但是由于数据和延迟限制，不能对新项目进行训练。为了弥补这些差距，本文开发了: (1)一种新的算法框架—— EMMETT，该算法依靠现有的分类器对观察到的项目进行高效的动态综合分类; (2)一种新的算法—— IRENE，它是 EMMETT 的一个简单而有效的实例，特别适合于大规模部署; (3)一种新的理论框架，用于分析大规模零拍检索中的泛化性能，指导我们的算法和训练相关的设计决策。实验结果表明，在前置编码器的基础上加入 IRENE 后，Recall@10的零镜头检索精度提高了15% 。此外，在一个主要搜索引擎的大规模广告检索任务的在线 A/B 测试中，iRENE 将广告点进率提高了4.2% 。最后，我们通过广泛的烧蚀实验验证了我们的设计选择。IRENe 的源代码可在 https://aka.ms/IRENE 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Extreme+Meta-Classification+for+Large-Scale+Zero-Shot+Retrieval)|0|
|[Conversational Dueling Bandits in Generalized Linear Models](https://doi.org/10.1145/3637528.3671892)|Shuhua Yang, Hui Yuan, Xiaoying Zhang, Mengdi Wang, Hong Zhang, Huazheng Wang|Princeton University, Princeton, NJ, USA; University of Science and Technology of China, Hefei, China; ByteDance, Beijing, China; Oregon State University, Corvallis, OR, USA|Conversational recommendation systems elicit user preferences by interacting with users to obtain their feedback on recommended commodities. Such systems utilize a multi-armed bandit framework to learn user preferences in an online manner and have received great success in recent years. However, existing conversational bandit methods have several limitations. First, they only enable users to provide explicit binary feedback on the recommended items or categories, leading to ambiguity in interpretation. In practice, users are usually faced with more than one choice. Relative feedback, known for its informativeness, has gained increasing popularity in recommendation system design. Moreover, current contextual bandit methods mainly work under linear reward assumptions, ignoring practical non-linear reward structures in generalized linear models. Therefore, in this paper, we introduce relative feedback-based conversations into conversational recommendation systems through the integration of dueling bandits in generalized linear models (GLM) and propose a novel conversational dueling bandit algorithm called ConDuel. Theoretical analyses of regret upper bounds and empirical validations on synthetic and real-world data underscore ConDuel's efficacy. We also demonstrate the potential to extend our algorithm to multinomial logit bandits with theoretical and experimental guarantees, which further proves the applicability of the proposed framework.|对话式推荐系统通过与用户进行交互以获得他们对推荐商品的反馈，从而引起用户的偏好。这类系统利用多臂老虎机框架，以在线方式学习用户偏好，近年来取得了巨大成功。然而，现有的会话强盗方法有一些局限性。首先，它们只允许用户对推荐的项目或类别提供明确的二进制反馈，从而导致解释上的歧义。实际上，用户通常面临不止一种选择。相对反馈以信息量大而闻名，在推荐系统设计中越来越受欢迎。此外，目前的情境强盗方法主要在线性报酬假设下工作，忽略了广义线性模型中实际的非线性报酬结构。因此，本文通过广义线性模型(GLM)中对决斗强盗的集成，将基于相对反馈的会话引入到会话推荐系统中，提出了一种新的会话决斗强盗算法 ConDuel。对遗憾上限的理论分析以及对合成数据和现实数据的经验验证强调了 ConDuel 的有效性。在理论和实验的基础上，证明了该算法在多项式 Logit 强盗问题上的可行性，进一步证明了该算法的适用性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conversational+Dueling+Bandits+in+Generalized+Linear+Models)|0|
|[User Welfare Optimization in Recommender Systems with Competing Content Creators](https://doi.org/10.1145/3637528.3672021)|Fan Yao, Yiming Liao, Mingzhe Wu, Chuanhao Li, Yan Zhu, James Yang, Jingzhou Liu, Qifan Wang, Haifeng Xu, Hongning Wang|Meta Platforms, Inc., New York, USA; Yale University, New Haven, USA; Meta Platforms, Inc., Menlo Park, USA; University of Virginia, Charlottesville, USA; University of Chicago, Chicago, USA; Google, Mountain View, USA; University of Southern California, Los Angeles, USA|Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms. This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform. However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare. To encourage creators to best serve a broad user population with relevant content, it becomes the platform's responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators. In this study, we perform system-side user welfare optimization under a competitive game setting among content creators. We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content. These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators' content production strategies. To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1. a proof-of-concept negative example illustrating how creators' strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on Instagram Reels short-video recommendation platform.|在创作者经济带来的新经济机遇的驱动下，越来越多的内容创作者依赖并竞争在线内容推荐平台产生的收入。这种蓬勃发展的竞争重塑了内容分发的动态，并深刻影响了平台上的长期用户福利。然而，缺乏全球用户偏好分布的全面图像，往往会使竞争，尤其是创造者陷入产生次优用户福利的状态。为了鼓励创作者用相关内容最好地服务于广泛的用户群体，平台有责任利用其在用户偏好分布方面的信息优势来准确地向创作者发出信号。在这项研究中，我们在内容创作者之间的竞争博弈环境下进行系统端用户福利优化。我们提出了一个平台的算法解决方案，该方案根据每个用户对推荐内容的满意度动态计算每个用户的权重序列。然后利用这些权重来设计调整推荐策略或推荐后奖励的机制，从而影响创作者的内容生产策略。为了验证我们提出的方法的有效性，我们报告了一系列的实验结果，包括: 1。一个概念证明的否定例子，说明创造者的策略如何在没有平台干预的情况下收敛到次优状态;。在不同的数据集上使用我们提出的干预机制的离线实验;。在 Instagram Reels 短视频推荐平台上进行了为期三周的在线实验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Welfare+Optimization+in+Recommender+Systems+with+Competing+Content+Creators)|0|
|[Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity](https://doi.org/10.1145/3637528.3671941)|Kyuhwan Yeom, Hyeongjun Yang, Gayeon Park, Myeongheon Jeon, Yunjeong Ko, Byungkook Oh, KyongHo Lee|Computer Science, Yonsei University, Seoul, Republic of Korea; Artificial Intelligence, Yonsei University, Seoul, Republic of Korea; Computer Science and Engineering, Konkuk University, Seoul, Republic of Korea|Numerous large-scale knowledge graphs (KGs) fundamentally represent two-view KGs: an ontology-view KG with abstract classes in ontology and an instance-view KG with specific collections of entities instantiated from ontology classes. Two-view KG embedding aims to jointly learn continuous vector representations of entities and relations in the aforementioned two-view KGs. In essence, an ontology schema exhibits a tree-like structure guided by class hierarchies, which leads classes to form inheritance hierarchies. However, existing two-view KG embedding models neglect those hierarchies, which provides the necessity to reflect class inheritance. On the other hand, KG is constructed based on a pre-defined ontology schema that includes heterogeneous relations between classes. Furthermore, these relations are defined within the scope of those among classes since instances inherit all the properties of their corresponding classes, which reveals structural similarity between two multi-relational networks. Despite the consideration to bridge the gap among two-view KG representations, existing methods ignore the existence of structural similarity between two-view KGs. To address these issues, we propose a novel two-view KG embedding model, CISS, considering Class Inheritance and Structural Similarity between two-view KGs. To deal with class inheritance, we utilize class sets, each of which is composed of sibling classes, to learn fine-grained class representations. In addition, we configure virtual instance-view KG from clustered instances and compare subgraph representations of two-view KGs to enhance structural similarity between them. Experimental results show our superior performance compared to existing models.|许多大规模的知识图(KG)从根本上表示两个视图 KG: 一个本体视图 KG 具有本体中的抽象类，一个实例视图 KG 具有从本体类实例化的实体的特定集合。双视图幼儿园嵌入的目的是联合学习上述两视图幼儿园中实体和关系的连续向量表示。从本质上讲，本体模式表现出一种由类层次结构引导的树状结构，这种结构引导类形成继承层次结构。然而，现有的双视图 KG 嵌入模型忽略了这些层次结构，这就需要反映类继承。另一方面，KG 是基于一个预定义的本体模式构建的，该模式包含类之间的异构关系。此外，这些关系是在类之间的范围内定义的，因为实例继承了相应类的所有属性，这揭示了两个多关系网络之间的结构相似性。尽管现行方法已考虑填补双视角幼稚园表现形式之间的差距，但却忽略了双视角幼稚园之间是否存在结构相似性。为了解决这些问题，我们提出了一个新的双视图幼儿园嵌入模型 CISS，该模型考虑了类继承和双视图幼儿园之间的结构相似性。为了处理类继承，我们利用类集(每个类集都由兄弟类组成)来学习细粒度的类表示。此外，我们从集群实例配置虚拟实例视图幼稚园，并比较两个视图幼稚园的子图表示，以增强它们之间的结构相似性。实验结果表明，我们的性能优于现有的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Two-View+Knowledge+Graphs+with+Class+Inheritance+and+Structural+Similarity)|0|
|[Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective](https://doi.org/10.1145/3637528.3671947)|Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing Sha, Shijin Wang|; State Key Laboratory of Cognitive Intelligence & iFLYTEK AI Research, Hefei, China; iFLYTEK AI Research, Hefei, China|Learning path recommendation aims to provide learners with a reasonable order of items to achieve their learning goals. Intuitively, the learning process on the learning path can be metaphorically likened to walking. Despite extensive efforts in this area, most previous methods mainly focus on the relationship among items but overlook the difficulty of items, which may raise two issues from a real walking perspective: (1) The path may be rough: When learners tread the path without considering item difficulty, it's akin to walking a dark, uneven road, making learning harder and dampening interest. (2) The path may be inefficient: Allowing learners only a few attempts on very challenging items before switching, or persisting with a difficult item despite numerous attempts without mastery, can result in inefficiencies in the learning journey. To conquer the above limitations, we propose a novel method named Difficulty-constrained Learning Path Recommendation (DLPR), which is aware of item difficulty. Specifically, we first explicitly categorize items into learning items and practice items, then construct a hierarchical graph to model and leverage item difficulty adequately. Then we design a Difficulty-driven Hierarchical Reinforcement Learning (DHRL) framework to facilitate learning paths with efficiency and smoothness. Finally, extensive experiments on three different simulators demonstrate our framework achieves state-of-the-art performance.|学习路径推荐的目的是为学习者提供一个合理的项目顺序，以实现他们的学习目标。直观地说，学习过程中的学习路径可以比喻为行走。尽管在这个领域做了大量的努力，以前的大多数方法主要关注项目之间的关系，但是忽略了项目的难度，这可能会从一个真正的行走的角度提出两个问题: (1)路径可能是粗糙的: 当学习者在不考虑项目难度的情况下行走在路径上，这就像走在一条黑暗的、不平坦的路上，使学习更加困难和抑制兴趣。(2)路径可能是低效的: 允许学习者在转换之前只尝试几次非常具有挑战性的项目，或者尽管尝试了很多次但没有掌握，仍然坚持一个困难的项目，可能会导致学习过程中的低效。为了克服上述限制，本文提出了一种新的学习路径推荐方法——难度约束学习路径推荐(DLPR) ，该方法能够识别项目的难度。具体来说，我们首先明确地将项目分类为学习项目和实践项目，然后构建一个层次图来充分地建模和利用项目难度。然后，我们设计了一个难度驱动的层次强化学习(dHRL)框架，以促进学习路径的有效性和顺畅性。最后，在三个不同的模拟器上进行了广泛的实验，证明了我们的框架实现了最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Item-Difficulty-Aware+Learning+Path+Recommendation:+From+a+Real+Walking+Perspective)|0|
|[Optimized Cost Per Click in Online Advertising: A Theoretical Analysis](https://doi.org/10.1145/3637528.3671767)|Kaichen Zhang, Zixuan Yuan, Hui Xiong||In recent years, Optimized Cost Per Click (OCPC) and Optimized Cost Per Mille(OCPM) have emerged as the most widely adopted pricing models in the onlineadvertising industry. However, the existing literature has yet to identify thespecific conditions under which these models outperform traditional pricingmodels like Cost Per Click (CPC) and Cost Per Action (CPA). To fill the gap,this paper builds an economic model that compares OCPC with CPC and CPAtheoretically, which incorporates out-site scenarios and outside options as twokey factors. Our analysis reveals that OCPC can effectively replace CPA bytackling the problem of advertisers strategically manipulating conversionreporting in out-site scenarios where conversions occur outside the advertisingplatform. Furthermore, OCPC exhibits the potential to surpass CPC in platformpayoffs by providing higher advertiser payoffs and consequently attracting moreadvertisers. However, if advertisers have less competitive outside options andconsistently stay in the focal platform, the platform may achieve higherpayoffs using CPC. Our findings deliver valuable insights for onlineadvertising platforms in selecting optimal pricing models, and providerecommendations for further enhancing their payoffs. To the best of ourknowledge, this is the first study to analyze OCPC from an economicperspective. Moreover, our analysis can be applied to the OCPM model as well.|近年来，优化每点击成本(OCPC)和优化每公里成本(OCPM)已经成为在线广告行业最广泛采用的定价模型。然而，现有文献尚未确定这些模型优于传统定价模型如每次点击成本(CPC)和每次行动成本(CPA)的具体条件。为了填补这一空白，本文从理论上建立了一个比较 OCPC 与 CPC 和 CPA 的经济模型，该模型将场外情景和场外选择作为两个关键因素。我们的分析表明，OCPC 可以有效地替代 CPA，解决广告商在转化发生在广告平台之外的外部场景中策略性地操纵转化报告的问题。此外，OCPC 通过提供更高的广告客户收益，从而吸引更多的广告客户，在平台收益方面显示出超过 CPC 的潜力。然而，如果广告商没有那么多竞争性的外部选择，并且一直呆在焦点平台上，那么该平台可能会利用 CPC 获得更高的回报。我们的研究结果为在线广告平台选择最佳定价模型提供了有价值的见解，并为进一步提高其收益提供了建议。据我们所知，这是第一个从经济学角度分析 OCPC 的研究。此外，我们的分析也适用于 OCPM 模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimized+Cost+Per+Click+in+Online+Advertising:+A+Theoretical+Analysis)|0|
|[Counteracting Duration Bias in Video Recommendation via Counterfactual Watch Time](https://doi.org/10.1145/3637528.3671817)|Haiyuan Zhao, Guohao Cai, Jieming Zhu, Zhenhua Dong, Jun Xu, JiRong Wen|Noah's Ark Lab, Huawei, Shenzhen, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China|In video recommendation, an ongoing effort is to satisfy users' personalizedinformation needs by leveraging their logged watch time. However, watch timeprediction suffers from duration bias, hindering its ability to reflect users'interests accurately. Existing label-correction approaches attempt to uncoveruser interests through grouping and normalizing observed watch time accordingto video duration. Although effective to some extent, we found that theseapproaches regard completely played records (i.e., a user watches the entirevideo) as equally high interest, which deviates from what we observed on realdatasets: users have varied explicit feedback proportion when completelyplaying videos. In this paper, we introduce the counterfactual watch time(CWT),the potential watch time a user would spend on the video if its duration issufficiently long. Analysis shows that the duration bias is caused by thetruncation of CWT due to the video duration limitation, which usually occurs onthose completely played records. Besides, a Counterfactual Watch Model (CWM) isproposed, revealing that CWT equals the time users get the maximum benefit fromvideo recommender systems. Moreover, a cost-based transform function is definedto transform the CWT into the estimation of user interest, and the model can belearned by optimizing a counterfactual likelihood function defined overobserved user watch times. Extensive experiments on three real videorecommendation datasets and online A/B testing demonstrated that CWMeffectively enhanced video recommendation accuracy and counteracted theduration bias.|在视频推荐中，一个持续的努力是通过利用用户的观看时间来满足用户的个性化信息需求。然而，手表时间预测存在持续时间偏差，影响了其准确反映用户兴趣的能力。现有的标签校正方法试图通过根据视频持续时间对观看时间进行分组和标准化来揭示用户的兴趣。虽然在某种程度上有效，但是我们发现这些方法把完全播放的记录(例如，用户观看整个视频)视为同样高的兴趣，这偏离了我们在真实数据集上观察到的: 当完全播放视频时，用户有不同的显式反馈比例。本文介绍了反事实观看时间(CWT) ，即当视频持续时间过长时，用户可能花在视频上的观看时间。分析表明，持续时间偏差是由于视频持续时间受到限制而导致的连续小波变换(CWT)截断所引起的，这种情况通常发生在完全播放的记录上。此外，提出了一种反事实观察模型(CWM) ，揭示了 CWT 等于用户从视频推荐系统中获得最大收益的时间。此外，定义了一个基于代价的转换函数，将连续小波变换转换为用户兴趣的估计，该模型可以通过优化一个反事实似然函数来定义过度观察的用户观察时间。在三个真实视频推荐数据集上的大量实验和在线 A/B 测试表明，CWM 有效地提高了视频推荐的准确性，抵消了持续时间偏差。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counteracting+Duration+Bias+in+Video+Recommendation+via+Counterfactual+Watch+Time)|0|
|[MMBee: Live Streaming Gift-Sending Recommendations via Multi-Modal Fusion and Behaviour Expansion](https://doi.org/10.1145/3637528.3671511)|Jiaxin Deng, Shiyao Wang, Yuchen Wang, Jiansong Qi, Liqin Zhao, Guorui Zhou, Gaofeng Meng|; KuaiShou Inc., Beijing, China; Institute of Automation, Beijing, China|Live streaming services are becoming increasingly popular due to real-time interactions and entertainment. Viewers can chat and send comments or virtual gifts to express their preferences for the streamers. Accurately modeling the gifting interaction not only enhances users' experience but also increases streamers' revenue. Previous studies on live streaming gifting prediction treat this task as a conventional recommendation problem, and model users' preferences using categorical data and observed historical behaviors. However, it is challenging to precisely describe the real-time content changes in live streaming using limited categorical information. Moreover, due to the sparsity of gifting behaviors, capturing the preferences and intentions of users is quite difficult. In this work, we propose MMBee based on real-time Multi-Modal Fusion and Behaviour Expansion to address these issues. Specifically, we first present a Multi-modal Fusion Module with Learnable Query (MFQ) to perceive the dynamic content of streaming segments and process complex multi-modal interactions, including images, text comments and speech. To alleviate the sparsity issue of gifting behaviors, we present a novel Graph-guided Interest Expansion (GIE) approach that learns both user and streamer representations on large-scale gifting graphs with multi-modal attributes. It consists of two main parts: graph node representations pre-training and metapath-based behavior expansion, all of which help model jump out of the specific historical gifting behaviors for exploration and largely enrich the behavior representations. Comprehensive experiment results show that MMBee achieves significant performance improvements on both public datasets and Kuaishou real-world streaming datasets and the effectiveness has been further validated through online A/B experiments. MMBee has been deployed and is serving hundreds of millions of users at Kuaishou.|由于实时交互和娱乐，流媒体直播服务变得越来越流行。观众可以聊天和发送评论或虚拟礼物来表达他们对主播的喜好。精确建模礼物互动不仅提高了用户的体验，而且增加了主播的收入。以往的流媒体直播礼物预测研究将这一任务视为一个传统的推荐问题，并利用分类数据和观察到的历史行为对用户的偏好进行建模。然而，使用有限的分类信息来精确描述直播流中的实时内容变化是一个挑战。此外，由于送礼行为的稀少性，要捕捉用户的喜好和意图是相当困难的。在这项工作中，我们提出了基于实时多模态融合和行为扩展的 MMBee 来解决这些问题。具体来说，我们首先提出了一种基于可学习查询(Learnable Query，MFQ)的多模态融合模块来感知流媒体片段的动态内容，并处理复杂的多模态交互，包括图像、文本注释和语音。为了缓解送礼行为的稀疏性问题，我们提出了一种新的图引导兴趣扩展(GIE)方法，该方法同时学习用户和流媒体在具有多模态属性的大规模送礼图上的表示。它包括两个主要部分: 图节点表示预训练和基于元路径的行为扩展，所有这些都有助于模型跳出具体的历史馈赠行为去探索，并大大丰富了行为表示。综合实验结果表明，mMBee 在公共数据集和 Kuaishou 真实世界流数据集上都取得了显著的性能改善，并通过在线 A/B 实验进一步验证了其有效性。MMBee 已经部署完毕，目前正在 Kuaishou 为数亿用户服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MMBee:+Live+Streaming+Gift-Sending+Recommendations+via+Multi-Modal+Fusion+and+Behaviour+Expansion)|0|
|[Contextual Distillation Model for Diversified Recommendation](https://doi.org/10.1145/3637528.3671514)|Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui Zhou, Yang Song, Hechang Chen|Jilin University, Changchun, China; Kuaishou Inc. & Tsinghua University, Beijing, China; Kuaishou Inc., Beijing, China; University of Science and Technology of China, Hefei, China; Tsinghua University, Beijing, China; University of Oxford, Oxford, United Kingdom|The diversity of recommendation is equally crucial as accuracy in improvinguser experience. Existing studies, e.g., Determinantal Point Process (DPP) andMaximal Marginal Relevance (MMR), employ a greedy paradigm to iterativelyselect items that optimize both accuracy and diversity. However, prior methodstypically exhibit quadratic complexity, limiting their applications to there-ranking stage and are not applicable to other recommendation stages with alarger pool of candidate items, such as the pre-ranking and ranking stages. Inthis paper, we propose Contextual Distillation Model (CDM), an efficientrecommendation model that addresses diversification, suitable for thedeployment in all stages of industrial recommendation pipelines. Specifically,CDM utilizes the candidate items in the same user request as context to enhancethe diversification of the results. We propose a contrastive context encoderthat employs attention mechanisms to model both positive and negative contexts.For the training of CDM, we compare each target item with its context embeddingand utilize the knowledge distillation framework to learn the win probabilityof each target item under the MMR algorithm, where the teacher is derived fromMMR outputs. During inference, ranking is performed through a linearcombination of the recommendation and student model scores, ensuring bothdiversity and efficiency. We perform offline evaluations on two industrialdatasets and conduct online A/B test of CDM on the short-video platformKuaiShou. The considerable enhancements observed in both recommendation qualityand diversity, as shown by metrics, provide strong superiority for theeffectiveness of CDM.|推荐的多样性与提高用户体验的准确性同样重要。现有的研究，例如行列式点过程(DPP)和最大边际相关性(MMR) ，采用贪婪的范式迭代选择项目，优化准确性和多样性。然而，先验方法通常表现出二次复杂性，将其应用限制在三阶段排名阶段，不适用于其他候选项目较多的推荐阶段，如预先排名阶段和排名阶段。本文提出了一种适用于行业推荐流程各个阶段的高效多样化推荐模型——上下文精馏模型(CDM)。具体来说，CDM 利用同一用户请求中的候选项作为上下文，以增强结果的多样化。我们提出了一个对比语境编码器，它使用注意机制来模拟积极和消极的语境。对于 CDM 的培训，我们将每个目标项目与其上下文嵌入进行比较，并利用知识提取框架在 MMR 算法下学习每个目标项目的获胜概率，其中教师是从 MMR 输出中获得的。在推理过程中，排名是通过推荐和学生模型得分的线性组合来完成的，从而保证了多样性和效率。我们对两个工业数据集进行离线评估，并在快手短视频平台上对 CDM 进行在线 A/B 测试。指标表明，在推荐质量和多样性方面观察到的显著增强为 CDM 的有效性提供了强大的优势。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+Distillation+Model+for+Diversified+Recommendation)|0|
|[MGMatch: Fast Matchmaking with Nonlinear Objective and Constraints via Multimodal Deep Graph Learning](https://doi.org/10.1145/3637528.3671553)|Yu Sun, Kai Wang, Zhipeng Hu, Runze Wu, Yaoxin Wu, Wen Song, Xudong Shen, Tangjie Lv, Changjie Fan|Eindhoven University of Technology, Eindhoven, Netherlands; Shandong University, Qingdao, Shandong, China; Fuxi AI Lab, NetEase Inc., Hangzhou, Zhejiang, China|As a core problem of online games, matchmaking is to assign players into multiple teams to maximize their gaming experience. With the rapid development of game industry, it is increasingly difficulty to explicitly model players' experiences as linear functions. Instead, it is often modeled in a data-driven way by training a neural network. Meanwhile, complex rules must be satisfied to ensure the robustness of matchmaking, which are often described using logical operators. Therefore, matchmaking in practical scenarios is a challenging combinatorial optimization problem with nonlinear objective, linear constraints and logical constraints, which receives much less attention in previous research. In this paper, we propose a novel deep learning method for high-quality matchmaking in real-time. We first cast the problem as standard mixed-integer programming (MIP) by linearizing ReLU networks and logical constraints. Then, based on supervised learning, we design and train a multi-modal graph learning architecture to predict optimal solutions end-to-end from instance data, and solve a surrogate problem to efficiently obtain feasible solutions. Evaluation results on real industry datasets show that our method can deliver near-optimal solutions within 100ms.|作为网络游戏的一个核心问题，匹配是将玩家分配到多个团队中，以最大限度地提高他们的游戏体验。随着游戏产业的快速发展，将玩家的体验明确地建模为线性函数变得越来越困难。相反，它通常是通过训练神经网络以数据驱动的方式建模的。同时，为了保证匹配的鲁棒性，必须满足复杂的规则，这些规则通常用逻辑运算符来描述。因此，实际场景中的匹配问题是一个具有非线性目标、线性约束和逻辑约束的组合优化问题，在以往的研究中受到的关注较少。本文提出了一种新的高质量实时匹配的深度学习方法。首先通过线性化 ReLU 网络和逻辑约束将问题转化为标准的混合整数规划(MIP)问题。然后，在监督式学习的基础上，我们设计并训练了一个多模态图学习架构来从实例数据中预测端到端的最优解，并解决一个代理问题来有效地获得可行的解。对实际工业数据集的评估结果表明，该方法可以在100ms 内提供接近最优的解决方案。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MGMatch:+Fast+Matchmaking+with+Nonlinear+Objective+and+Constraints+via+Multimodal+Deep+Graph+Learning)|0|
|[R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models](https://doi.org/10.1145/3637528.3671564)|Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li|SIOE, Beihang University, Beijing, China; DCST, Tsinghua University, Beijing, China; SoI, Renmin University of China, Beijing, China; BNRist, DCST, Tsinghua University, Beijing, China|Large language models have achieved remarkable success on general NLP tasks,but they may fall short for domain-specific problems. Recently, variousRetrieval-Augmented Large Language Models (RALLMs) are proposed to address thisshortcoming. However, existing evaluation tools only provide a few baselinesand evaluate them on various domains without mining the depth of domainknowledge. In this paper, we address the challenges of evaluating RALLMs byintroducing the R-Eval toolkit, a Python toolkit designed to streamline theevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,which supports popular built-in RAG workflows and allows for the incorporationof customized testing data on the specific domain, is designed to beuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMsacross three task levels and two representative domains, revealing significantvariations in the effectiveness of RALLMs across different tasks and domains.Our analysis emphasizes the importance of considering both task and domainrequirements when choosing a RAG workflow and LLM combination. We are committedto continuously maintaining our platform at https://github.com/THU-KEG/R-Evalto facilitate both the industry and the researchers.|大型语言模型已经在一般的 NLP 任务上取得了显著的成功，但是它们可能在特定领域的问题上有所不足。最近，各种检索增强大型语言模型(RALLMs)被提出来解决这个问题。然而，现有的评估工具只提供了一些基线，并在不同的领域进行评估，而没有挖掘领域知识的深度。在本文中，我们通过引入 R-Eval 工具包来解决评估 RAG 工作流的挑战，这是一个 Python 工具包，旨在与 LLM 一起简化不同 RAG 工作流的评估。我们的工具包，支持流行的内置 RAG 工作流程，并允许在特定领域集成定制的测试数据，被设计成用户友好的、模块化的和可扩展的。我们在三个任务级别和两个代表性领域对21个 RALLM 进行了评估，揭示了 RALLM 在不同任务和领域的有效性的显著差异。我们的分析强调了在选择 RAG 工作流和 LLM 组合时同时考虑任务和领域需求的重要性。我们致力于不断维护我们的平台， https://github.com/thu-keg/r-evalto 为业界和研究人员提供便利。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R-Eval:+A+Unified+Toolkit+for+Evaluating+Domain+Knowledge+of+Retrieval+Augmented+Large+Language+Models)|0|
|[ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising](https://doi.org/10.1145/3637528.3671612)|Ruize Wang, Hui Xu, Ying Cheng, Qi He, Xing Zhou, Rui Feng, Wei Xu, Lei Huang, Jie Jiang|Tencent Inc., Shanghai, China; Tencent Inc., Shenzhen, China; School of Computer Science, Fudan University, Shanghai, China|Advertising platforms have evolved in estimating Lifetime Value (LTV) to better align with advertisers' true performance metric which considers cumulative sum of purchases a customer contributes over a period. Accurate LTV estimation is crucial for the precision of the advertising system and the effectiveness of advertisements. However, the sparsity of real-world LTV data presents a significant challenge to LTV predictive model(i.e., pLTV), severely limiting the their capabilities. Therefore, we propose to utilize external data, in addition to the internal data of advertising platform, to expand the size of purchase samples and enhance the LTV prediction model of the advertising platform. To tackle the issue of data distribution shift between internal and external platforms, we introduce an Adaptive Difference Siamese Network (ADSNet), which employs cross-domain transfer learning to prevent negative transfer. Specifically, ADSNet is designed to learn information that is beneficial to the target domain. We introduce a gain evaluation strategy to calculate information gain, aiding the model in learning helpful information for the target domain and providing the ability to reject noisy samples, thus avoiding negative transfer. Additionally, we also design a Domain Adaptation Module as a bridge to connect different domains, reduce the distribution distance between them, and enhance the consistency of representation space distribution. We conduct extensive offline experiments and online A/B tests on a real advertising platform. Our proposed ADSNet method outperforms other methods, improving GINI by 2%. The ablation study highlights the importance of the gain evaluation strategy in negative gain sample rejection and improving model performance. Additionally, ADSNet significantly improves long-tail prediction. The online A/B tests confirm ADSNet's efficacy, increasing online LTV by 3.47% and GMV by 3.89%.|广告平台已经发展到估算终身价值(LTV) ，以便更好地与广告商的真实业绩指标保持一致，后者考虑的是消费者在一段时间内贡献的累计购买总额。准确的 LTV 估计对于广告系统的准确性和广告的有效性至关重要。然而，实际 LTV 数据的稀疏性对 LTV 预测模型(即 pLTV)提出了严峻的挑战，严重限制了它们的能力。因此，我们建议利用外部数据，除了广告平台的内部数据外，扩大购买样本的规模，提高广告平台的 LTV 预测模型。为了解决内部平台和外部平台之间数据分布转移的问题，我们引入了自适应差分暹罗网(ADSNet) ，该网络采用跨域传输学习来防止负向传输。具体来说，ADSNet 是为了学习对目标域有益的信息而设计的。我们引入一个增益评估策略来计算信息增益，帮助模型学习目标域的有用信息，并提供拒绝噪声样本的能力，从而避免负迁移。此外，我们还设计了一个领域适应模块作为连接不同领域的桥梁，减少它们之间的分布距离，提高表示空间分布的一致性。我们在一个真实的广告平台上进行广泛的离线实验和在线 A/B 测试。我们提出的 ADSNet 方法优于其他方法，GINI 提高了2% 。烧蚀研究突出了增益评价策略在抑制负增益样本和改善模型性能中的重要性。此外，ADSNet 显著改善了长尾预测。在线 A/B 测试证实了 ADSNet 的有效性，在线 LTV 增加了3.47% ，GMV 增加了3.89% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ADSNet:+Cross-Domain+LTV+Prediction+with+an+Adaptive+Siamese+Network+in+Advertising)|0|
|[Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks](https://doi.org/10.1145/3637528.3671569)|Yijie Zhang, Yuanchen Bei, Hao Chen, Qijie Shen, Zheng Yuan, Huan Gong, Senzhang Wang, Feiran Huang, Xiao Huang|Central South University, Hunan, Changsha, China; Jinan University, Guangzhou, China; National University of Defense Technology, Changsha, China; Alibaba Group, Zhejiang, Hangzhou, China; The Hong Kong Polytechnic University, Hong Kong, China; Zhejiang University, Zhejiang, Hangzhou, China|Representing information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior graphs, we propose the concept of Partial Order Recommendation Graphs (POG). POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored Partial Order Graph Convolutional Networks (POGCN) that convolute neighbors' information while considering the behavior relations between users and items. POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training. POGCN has been successfully deployed on the homepage of Alibaba for two months, providing recommendation services for over one billion users. Extensive offline experiments conducted on three public benchmark datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors. Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale recommender systems.|在单一图形协同过滤(CF)向量中表示多种行为的信息一直是一个长期的挑战。这是因为不同的行为自然地形成独立的行为图，并学习独立的 CF 嵌入。现有的模型通过指定一些行为的 CF 嵌入作为主嵌入，并利用其他辅助来增强主嵌入，从而将分离的嵌入进行合并。然而，这种方法往往导致联合嵌入在主要任务上表现良好，但在辅助任务上表现不佳。为了解决分离的行为图所带来的问题，我们提出了偏序推荐图(POG)的概念。POG 将多个行为和模型行为组合的偏序关系定义为加权边，以便将单独的行为图合并到一个联合 POG 中。理论证明了 POG 可以推广到任意给定的一组多行为。在 POG 的基础上，提出了一种考虑用户与项目之间行为关系的卷积邻居信息的剪裁偏序图卷积网络(POGCN)。POGCN 还引入了一种偏序 BPR 抽样策略，用于有效的多行为 CF 训练。POgcn 已成功登入阿里巴巴网页两个月，为超过十亿用户提供推荐服务。在三个公共基准数据集上进行的大量离线实验表明，POGCN 在所有类型的行为上都优于最先进的多行为基准。此外，在线 A/B 测试证实了 POGCN 在亿万规模推荐系统中的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Behavior+Collaborative+Filtering+with+Partial+Order+Graph+Convolutional+Networks)|0|
|[Inductive Modeling for Realtime Cold Start Recommendations](https://doi.org/10.1145/3637528.3671588)|Chandler Zuo, Jonathan Castaldo, Hanqing Zhu, Haoyu Zhang, Ji Liu, Yangpeng Ou, Xiao Kong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inductive+Modeling+for+Realtime+Cold+Start+Recommendations)|0|
|[Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era](https://doi.org/10.1145/3637528.3671458)|Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bias+and+Unfairness+in+Information+Retrieval+Systems:+New+Challenges+in+the+LLM+Era)|0|
|[Approximating Memorization Using Loss Surface Geometry for Dataset Pruning and Summarization](https://doi.org/10.1145/3637528.3671985)|Andrea Agiollo, Young In Kim, Rajiv Khanna||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximating+Memorization+Using+Loss+Surface+Geometry+for+Dataset+Pruning+and+Summarization)|0|
|[Evading Community Detection via Counterfactual Neighborhood Search](https://doi.org/10.1145/3637528.3671896)|Andrea Bernini, Fabrizio Silvestri, Gabriele Tolomei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evading+Community+Detection+via+Counterfactual+Neighborhood+Search)|0|
|[FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering](https://doi.org/10.1145/3637528.3672065)|Tianchi Cai, Zhiwen Tan, Xierui Song, Tao Sun, Jiyan Jiang, Yunqi Xu, Yinger Zhang, Jinjie Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FoRAG:+Factuality-optimized+Retrieval+Augmented+Generation+for+Web-enhanced+Long-form+Question+Answering)|0|
|[A Hierarchical Context Augmentation Method to Improve Retrieval-Augmented LLMs on Scientific Papers](https://doi.org/10.1145/3637528.3671847)|TianYi Che, XianLing Mao, Tian Lan, Heyan Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Hierarchical+Context+Augmentation+Method+to+Improve+Retrieval-Augmented+LLMs+on+Scientific+Papers)|0|
|[Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation](https://doi.org/10.1145/3637528.3671750)|Xiaocong Chen, Siyu Wang, Lina Yao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maximum-Entropy+Regularized+Decision+Transformer+with+Reward+Relabelling+for+Dynamic+Recommendation)|0|
|[Retrieval-Augmented Hypergraph for Multimodal Social Media Popularity Prediction](https://doi.org/10.1145/3637528.3672041)|Zhangtao Cheng, Jienan Zhang, Xovee Xu, Goce Trajcevski, Ting Zhong, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Hypergraph+for+Multimodal+Social+Media+Popularity+Prediction)|0|
|[ROTAN: A Rotation-based Temporal Attention Network for Time-Specific Next POI Recommendation](https://doi.org/10.1145/3637528.3671809)|Shanshan Feng, Feiyu Meng, Lisi Chen, Shuo Shang, Yew Soon Ong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ROTAN:+A+Rotation-based+Temporal+Attention+Network+for+Time-Specific+Next+POI+Recommendation)|0|
|[AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting](https://doi.org/10.1145/3637528.3672057)|Raphael Fischer, Amal Saadallah||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoXPCR:+Automated+Multi-Objective+Model+Selection+for+Time+Series+Forecasting)|0|
|[Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization](https://doi.org/10.1145/3637528.3672070)|Zhibin Gu, Zhendong Li, Songhe Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topology-Driven+Multi-View+Clustering+via+Tensorial+Refined+Sigmoid+Rank+Minimization)|0|
|[Ranking with Slot Constraints](https://doi.org/10.1145/3637528.3672000)|Wentao Guo, Andrew Wang, Bradon Thymes, Thorsten Joachims||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+with+Slot+Constraints)|0|
|[Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations](https://doi.org/10.1145/3637528.3672056)|Linxin Guo, Yaochen Zhu, Min Gao, Yinghui Tao, Junliang Yu, Chen Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Consistency+and+Discrepancy-Based+Contrastive+Tripartite+Graph+Learning+for+Recommendations)|0|
|[Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors](https://doi.org/10.1145/3637528.3671793)|Croix Gyurek, Niloy Talukder, Mohammad Al Hasan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Binder:+Hierarchical+Concept+Representation+through+Order+Embedding+of+Binary+Vectors)|0|
|[An Efficient Local Search Algorithm for Large GD Advertising Inventory Allocation with Multilinear Constraints](https://doi.org/10.1145/3637528.3671811)|Xiang He, Wuyang Mao, Zhenghang Xu, Yuanzhe Gu, Yundu Huang, Zhonglin Zu, Liang Wang, Mengyu Zhao, Mengchuan Zou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Local+Search+Algorithm+for+Large+GD+Advertising+Inventory+Allocation+with+Multilinear+Constraints)|0|
|[Double Correction Framework for Denoising Recommendation](https://doi.org/10.1145/3637528.3671692)|Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai, Jinqi Gong, Richang Hong, Min Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Double+Correction+Framework+for+Denoising+Recommendation)|0|
|[Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models](https://doi.org/10.1145/3637528.3671932)|Zhibo Hu, Chen Wang, Yanfeng Shu, HyeYoung Paik, Liming Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prompt+Perturbation+in+Retrieval-Augmented+Generation+based+Large+Language+Models)|0|
|[Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs](https://doi.org/10.1145/3637528.3671980)|Hao Li, Hao Jiang, Jiajun Fan, Dongsheng Ye, Liang Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Neural+Dowker+Network:+Approximating+Persistent+Homology+in+Dynamic+Directed+Graphs)|0|
|[RecExplainer: Aligning Large Language Models for Explaining Recommendation Models](https://doi.org/10.1145/3637528.3671802)|Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecExplainer:+Aligning+Large+Language+Models+for+Explaining+Recommendation+Models)|0|
|[Customizing Graph Neural Network for CAD Assembly Recommendation](https://doi.org/10.1145/3637528.3671788)|Fengqi Liang, Huan Zhao, Yuhan Quan, Wei Fang, Chuan Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Customizing+Graph+Neural+Network+for+CAD+Assembly+Recommendation)|0|
|[When Box Meets Graph Neural Network in Tag-aware Recommendation](https://doi.org/10.1145/3637528.3671973)|Fake Lin, Ziwei Zhao, Xi Zhu, Da Zhang, Shitian Shen, Xueying Li, Tong Xu, Suojuan Zhang, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+Box+Meets+Graph+Neural+Network+in+Tag-aware+Recommendation)|0|
|[Fast Query of Biharmonic Distance in Networks](https://doi.org/10.1145/3637528.3671856)|Changan Liu, Ahad N. Zehmakan, Zhongzhi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Query+of+Biharmonic+Distance+in+Networks)|0|
|[Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization](https://doi.org/10.1145/3637528.3672040)|Fei Liu, Xi Lin, Zhenkun Wang, Qingfu Zhang, Tong Xialiang, Mingxuan Yuan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Task+Learning+for+Routing+Problem+with+Cross-Problem+Zero-Shot+Generalization)|0|
|[Low Rank Multi-Dictionary Selection at Scale](https://doi.org/10.1145/3637528.3671723)|Boya Ma, Maxwell McNeil, Abram Magner, Petko Bogdanov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Low+Rank+Multi-Dictionary+Selection+at+Scale)|0|
|[ImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation](https://doi.org/10.1145/3637528.3671751)|Tong Nie, Guoyang Qin, Wei Ma, Yuewen Mei, Jian Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ImputeFormer:+Low+Rankness-Induced+Transformers+for+Generalizable+Spatiotemporal+Imputation)|0|
|[Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning](https://doi.org/10.1145/3637528.3671787)|Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, Xudong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Consistency+in+Cross-Lingual+Cross-Modal+Retrieval+with+1-to-K+Contrastive+Learning)|0|
|[CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://doi.org/10.1145/3637528.3671837)|LiangBo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CheatAgent:+Attacking+LLM-Empowered+Recommender+Systems+via+LLM+Agent)|0|
|[Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I](https://doi.org/10.1145/3637528.3671883)|Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reliable+Confidence+Intervals+for+Information+Retrieval+Evaluation+Using+Generative+A.I)|0|
|[How Powerful is Graph Filtering for Recommendation](https://doi.org/10.1145/3637528.3671789)|Shaowen Peng, Xin Liu, Kazunari Sugiyama, Tsunenori Mine||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+Powerful+is+Graph+Filtering+for+Recommendation)|0|
|[STEMO: Early Spatio-temporal Forecasting with Multi-Objective Reinforcement Learning](https://doi.org/10.1145/3637528.3671922)|Wei Shao, Yufan Kang, Ziyan Peng, Xiao Xiao, Lei Wang, Yuhui Yang, Flora D. Salim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STEMO:+Early+Spatio-temporal+Forecasting+with+Multi-Objective+Reinforcement+Learning)|0|
|[Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations](https://doi.org/10.1145/3637528.3671935)|Yuanfeng Song, Xuefang Zhao, Raymond ChiWing Wong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Marrying+Dialogue+Systems+with+Data+Visualization:+Interactive+Data+Visualization+Generation+from+Natural+Language+Conversations)|0|
|[Towards Robust Recommendation via Decision Boundary-aware Graph Contrastive Learning](https://doi.org/10.1145/3637528.3671661)|Jiakai Tang, Sunhao Dai, Zexu Sun, Xu Chen, Jun Xu, Wenhui Yu, Lantao Hu, Peng Jiang, Han Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+Recommendation+via+Decision+Boundary-aware+Graph+Contrastive+Learning)|0|
|[Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness](https://doi.org/10.1145/3637528.3672009)|Dingrong Wang, Hitesh Sapkota, Zhiqiang Tao, Qi Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforced+Compressive+Neural+Architecture+Search+for+Versatile+Adversarial+Robustness)|0|
|[Routing Evidence for Unseen Actions in Video Moment Retrieval](https://doi.org/10.1145/3637528.3671693)|Guolong Wang, Xun Wu, Zheng Qin, Liangliang Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Routing+Evidence+for+Unseen+Actions+in+Video+Moment+Retrieval)|0|
|[Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks](https://doi.org/10.1145/3637528.3671795)|Zongwei Wang, Junliang Yu, Min Gao, Hongzhi Yin, Bin Cui, Shazia W. Sadiq||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Vulnerabilities+of+Contrastive+Recommender+Systems+to+Poisoning+Attacks)|0|
|[FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning](https://doi.org/10.1145/3637528.3671748)|Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang, Chenglu Wen, Rongshan Yu, Cheng Wang, Xiaoliang Fan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedSAC:+Dynamic+Submodel+Allocation+for+Collaborative+Fairness+in+Federated+Learning)|0|
|[Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering](https://doi.org/10.1145/3637528.3671840)|Yihong Wu, Le Zhang, Fengran Mo, Tianyu Zhu, Weizhi Ma, JianYun Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Graph+Convolution+and+Contrastive+Learning+in+Collaborative+Filtering)|0|
|[Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification](https://doi.org/10.1145/3637528.3671706)|Beini Xie, Heng Chang, Ziwei Zhang, Zeyang Zhang, Simin Wu, Xin Wang, Yuan Meng, Wenwu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Lightweight+Graph+Neural+Network+Search+with+Curriculum+Graph+Sparsification)|0|
|[Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method](https://doi.org/10.1145/3637528.3671734)|Chen Yang, Sunhao Dai, Yupeng Hou, Wayne Xin Zhao, Jun Xu, Yang Song, Hengshu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Reciprocal+Recommender+Systems:+Metrics,+Formulation,+and+Method)|0|
|[Graph Bottlenecked Social Recommendation](https://doi.org/10.1145/3637528.3671807)|Yonghui Yang, Le Wu, Zihan Wang, Zhuangzhuang He, Richang Hong, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Bottlenecked+Social+Recommendation)|0|
|[Efficient and Effective Anchored Densest Subgraph Search: A Convex-programming based Approach](https://doi.org/10.1145/3637528.3671727)|Xiaowei Ye, RongHua Li, Lei Liang, Zhizhen Liu, Longlong Lin, Guoren Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Anchored+Densest+Subgraph+Search:+A+Convex-programming+based+Approach)|0|
|[Approximate Matrix Multiplication over Sliding Windows](https://doi.org/10.1145/3637528.3671819)|Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei, Cheng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Approximate+Matrix+Multiplication+over+Sliding+Windows)|0|
|[Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning](https://doi.org/10.1145/3637528.3672015)|Wangyang Ying, Dongjie Wang, Xuanming Hu, Yuanchun Zhou, Charu C. Aggarwal, Yanjie Fu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Generative+Feature+Transformation+via+Graph+Contrastive+Pre-training+and+Multi-objective+Fine-tuning)|0|
|[Personalized Federated Continual Learning via Multi-Granularity Prompt](https://doi.org/10.1145/3637528.3671948)|Hao Yu, Xin Yang, Xin Gao, Yan Kang, Hao Wang, Junbo Zhang, Tianrui Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Federated+Continual+Learning+via+Multi-Granularity+Prompt)|0|
|[DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning](https://doi.org/10.1145/3637528.3672036)|Jingyi Yuan, Yang Weng, Erik Blasch||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DipDNN:+Preserving+Inverse+Consistency+and+Approximation+Efficiency+for+Invertible+Learning)|0|
|[Conditional Logical Message Passing Transformer for Complex Query Answering](https://doi.org/10.1145/3637528.3671869)|Chongzhi Zhang, Zhiping Peng, Junhao Zheng, Qianli Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conditional+Logical+Message+Passing+Transformer+for+Complex+Query+Answering)|0|
|[Natural Language Explainable Recommendation with Robustness Enhancement](https://doi.org/10.1145/3637528.3671781)|Jingsen Zhang, Jiakai Tang, Xu Chen, Wenhui Yu, Lantao Hu, Peng Jiang, Han Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Natural+Language+Explainable+Recommendation+with+Robustness+Enhancement)|0|
|[Enabling Collaborative Test-Time Adaptation in Dynamic Environment via Federated Learning](https://doi.org/10.1145/3637528.3671908)|Jiayuan Zhang, Xuefeng Liu, Yukang Zhang, Guogang Zhu, Jianwei Niu, Shaojie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enabling+Collaborative+Test-Time+Adaptation+in+Dynamic+Environment+via+Federated+Learning)|0|
|[Topology-aware Embedding Memory for Continual Learning on Expanding Networks](https://doi.org/10.1145/3637528.3671732)|Xikun Zhang, Dongjin Song, Yixin Chen, Dacheng Tao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topology-aware+Embedding+Memory+for+Continual+Learning+on+Expanding+Networks)|0|
|[Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing](https://doi.org/10.1145/3637528.3671823)|Xinbo Zhao, Yingxue Zhang, Xin Zhang, Yu Yang, Yiqun Xie, Yanhua Li, Jun Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Urban-Focused+Multi-Task+Offline+Reinforcement+Learning+with+Contrastive+Data+Sharing)|0|
|[Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models](https://doi.org/10.1145/3637528.3671836)|Yuan Zhong, Xiaochen Wang, Jiaqi Wang, Xiaokun Zhang, Yaqing Wang, Mengdi Huai, Cao Xiao, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthesizing+Multimodal+Electronic+Health+Records+via+Predictive+Diffusion+Models)|0|
|[Generative AI in E-Commerce: What Can We Expect?](https://doi.org/10.1145/3637528.3672503)|Haixun Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+AI+in+E-Commerce:+What+Can+We+Expect?)|0|
|[LiRank: Industrial Large Scale Ranking Models at LinkedIn](https://doi.org/10.1145/3637528.3671561)|Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Charles Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiRank:+Industrial+Large+Scale+Ranking+Models+at+LinkedIn)|0|
|[Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training](https://doi.org/10.1145/3637528.3671513)|Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, Zhenli Sheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Multi-field+B2B+Cloud+Solution+Matching+via+Contrastive+Pre-training)|0|
|[GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants](https://doi.org/10.1145/3637528.3671622)|Sophie Fischer, Carlos Gemmell, Niklas Tecklenburg, Iain Mackie, Federico Rossetto, Jeffrey Dalton||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GRILLBot+In+Practice:+Lessons+and+Tradeoffs+Deploying+Large+Language+Models+for+Adaptable+Conversational+Task+Assistants)|0|
|[Enhancing E-commerce Spelling Correction with Fine-Tuned Transformer Models](https://doi.org/10.1145/3637528.3671625)|Arnab Dutta, Gleb Polushin, Xiaoshuang Zhang, Daniel Stein||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+E-commerce+Spelling+Correction+with+Fine-Tuned+Transformer+Models)|0|
|[Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information](https://doi.org/10.1145/3637528.3671652)|Aishwarya Jayagopal, Hansheng Xue, Ziyang He, Robert J. Walsh, Krishna Kumar Hariprasannan, David Shao Peng Tan, Tuan Zea Tan, Jason J. Pitt, Anand D. Jeyasekharan, Vaibhav Rajan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalised+Drug+Identifier+for+Cancer+Treatment+with+Transformers+using+Auxiliary+Information)|0|
|[ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems](https://doi.org/10.1145/3637528.3671571)|Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ERASE:+Benchmarking+Feature+Selection+Methods+for+Deep+Recommender+Systems)|0|
|[Beyond Binary Preference: Leveraging Bayesian Approaches for Joint Optimization of Ranking and Calibration](https://doi.org/10.1145/3637528.3671577)|Chang Liu, Qiwei Wang, Wenqing Lin, Yue Ding, Hongtao Lu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Binary+Preference:+Leveraging+Bayesian+Approaches+for+Joint+Optimization+of+Ranking+and+Calibration)|0|
|[Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning](https://doi.org/10.1145/3637528.3671618)|Amit Sharma, Hua Li, Xue Li, Jian Jiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Novelty+of+Top-k+Recommendations+using+Large+Language+Models+and+Reinforcement+Learning)|0|
|[Measuring an LLM's Proficiency at using APIs: A Query Generation Strategy](https://doi.org/10.1145/3637528.3671592)|Ying Sheng, Sudeep Gandhe, Bhargav Kanagal, Nick Edmonds, Zachary Fisher, Sandeep Tata, Aarush Selvan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Measuring+an+LLM's+Proficiency+at+using+APIs:+A+Query+Generation+Strategy)|0|
|[PEMBOT: Pareto-Ensembled Multi-task Boosted Trees](https://doi.org/10.1145/3637528.3671619)|Gokul Swamy, Anoop Saladi, Arunita Das, Shobhit Niranjan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEMBOT:+Pareto-Ensembled+Multi-task+Boosted+Trees)|0|
|[Enhancing Personalized Headline Generation via Offline Goal-conditioned Reinforcement Learning with Large Language Models](https://doi.org/10.1145/3637528.3671638)|Xiaoyu Tan, Leijun Cheng, Xihe Qiu, Shaojie Shi, Yuan Cheng, Wei Chu, Yinghui Xu, Yuan Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Personalized+Headline+Generation+via+Offline+Goal-conditioned+Reinforcement+Learning+with+Large+Language+Models)|0|
|[Future Impact Decomposition in Request-level Recommendations](https://doi.org/10.1145/3637528.3671506)|Xiaobei Wang, Shuchang Liu, Xueliang Wang, Qingpeng Cai, Lantao Hu, Han Li, Peng Jiang, Kun Gai, Guangming Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Future+Impact+Decomposition+in+Request-level+Recommendations)|0|
|[A Self-boosted Framework for Calibrated Ranking](https://doi.org/10.1145/3637528.3671570)|Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, Yang Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Self-boosted+Framework+for+Calibrated+Ranking)|0|
|[Bringing Multimodality to Amazon Visual Search System](https://doi.org/10.1145/3637528.3671640)|Xinliang Zhu, ShengWei Huang, Han Ding, Jinyu Yang, Kelvin Chen, Tao Zhou, Tal Neiman, Ouye Xie, Son Tran, Benjamin Z. Yao, Douglas Gray, Anuj Bindal, Arnab Dhua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bringing+Multimodality+to+Amazon+Visual+Search+System)|0|
|[A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3637528.3671470)|Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, TatSeng Chua, Qing Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+on+RAG+Meeting+LLMs:+Towards+Retrieval-Augmented+Large+Language+Models)|0|
|[Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey](https://doi.org/10.1145/3637528.3671473)|Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, XiaoMing Wu, Zhou Zhao, Rui Zhang, Zhenhua Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Pretraining,+Adaptation,+and+Generation+for+Recommendation:+A+Survey)|0|
|[AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive Learning](https://doi.org/10.1145/3637528.3671498)|Qingsong Wen, Jing Liang, Carles Sierra, Rose Luckin, Richard Jiarui Tong, Zitao Liu, Peng Cui, Jiliang Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+for+Education+(AI4EDU):+Advancing+Personalized+Education+with+LLM+and+Adaptive+Learning)|0|
|[Understanding Inter-Session Intentions via Complex Logical Reasoning](https://doi.org/10.1145/3637528.3671808)|Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Yangqiu Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+Inter-Session+Intentions+via+Complex+Logical+Reasoning)|0|
|[Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network](https://doi.org/10.1145/3637528.3671664)|Yucen Gao, Zhehao Zhu, Mingqian Ma, Fei Gao, Hui Gao, Yangguang Shi, Xiaofeng Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Preference+Weight+Estimation+Algorithm+with+Vanishing+Regret+for+Car-Hailing+in+Road+Network)|0|
|[Robust Auto-Bidding Strategies for Online Advertising](https://doi.org/10.1145/3637528.3671729)|Qilong Lin, Zhenzhe Zheng, Fan Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Auto-Bidding+Strategies+for+Online+Advertising)|0|
|[QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams](https://doi.org/10.1145/3637528.3671695)|Yiyan Qi, Rundong Li, Pinghui Wang, Yufang Sun, Rui Xing||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QSketch:+An+Efficient+Sketch+for+Weighted+Cardinality+Estimation+in+Streams)|0|
|[Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask](https://doi.org/10.1145/3637528.3671708)|Jingyu Xiao, Zhiyao Xu, Qingsong Zou, Qing Li, Dan Zhao, Dong Fang, Ruoyu Li, Wenxin Tang, Kang Li, Xudong Zuo, Penghui Hu, Yong Jiang, Zixuan Weng, Michael R. Lyu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Make+Your+Home+Safe:+Time-aware+Unsupervised+User+Behavior+Anomaly+Detection+in+Smart+Homes+via+Loss-guided+Mask)|0|
|[Top-Down Bayesian Posterior Sampling for Sum-Product Networks](https://doi.org/10.1145/3637528.3671876)|Soma Yokoi, Issei Sato||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Top-Down+Bayesian+Posterior+Sampling+for+Sum-Product+Networks)|0|
|[CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification](https://doi.org/10.1145/3637528.3671515)|Lele Cao, Vilhelm von Ehrenheim, Mark GranrothWilding, Richard Anselmo Stahl, Andrew McCornack, Armin Catovic, Dhiana Deva Cavalcanti Rocha||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CompanyKG:+A+Large-Scale+Heterogeneous+Graph+for+Company+Similarity+Quantification)|0|
|[CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning](https://doi.org/10.1145/3637528.3671538)|Ulrik FriisJensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. Ø. Jensen, Raghavendra Selvan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CHILI:+Chemically-Informed+Large-scale+Inorganic+Nanomaterials+Dataset+for+Advancing+Graph+Machine+Learning)|0|
|[Offline Reinforcement Learning for Optimizing Production Bidding Policies](https://doi.org/10.1145/3637528.3671555)|Dmytro Korenkevych, Frank Cheng, Artsiom Balakir, Alex Nikulkov, Lingnan Gao, Zhihao Cen, Zuobing Xu, Zheqing Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Reinforcement+Learning+for+Optimizing+Production+Bidding+Policies)|0|
|[Spending Programmed Bidding: Privacy-friendly Bid Optimization with ROI Constraint in Online Advertising](https://doi.org/10.1145/3637528.3671540)|Yumin Su, Min Xiang, Yifei Chen, Yanbiao Li, Tian Qin, Hongyi Zhang, Yasong Li, Xiaobing Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spending+Programmed+Bidding:+Privacy-friendly+Bid+Optimization+with+ROI+Constraint+in+Online+Advertising)|0|
|[Know in AdVance:  Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest](https://doi.org/10.1145/3637528.3671528)|Xiaoyu Wang, Yonghui Guo, Hui Sheng, Peili Lv, Chi Zhou, Wei Huang, Shiqin Ta, Dongbo Huang, Xiujin Yang, Lan Xu, Hao Zhou, Yusheng Ji||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know+in+AdVance:++Linear-Complexity+Forecasting+of+Ad+Campaign+Performance+with+Evolving+User+Interest)|0|
|[Trinity: Syncretizing Multi-/Long-Tail/Long-Term Interests All in One](https://doi.org/10.1145/3637528.3671651)|Jing Yan, Liu Jiang, Jianfei Cui, Zhichen Zhao, Xingyan Bin, Feng Zhang, Zuotao Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Trinity:+Syncretizing+Multi-/Long-Tail/Long-Term+Interests+All+in+One)|0|
|[Temporal Uplift Modeling for Online Marketing](https://doi.org/10.1145/3637528.3671560)|Xin Zhang, Kai Wang, Zengmao Wang, Bo Du, Shiwei Zhao, Runze Wu, Xudong Shen, Tangjie Lv, Changjie Fan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Uplift+Modeling+for+Online+Marketing)|0|
|[STATE: A Robust ATE Estimator of Heavy-Tailed Metrics for Variance Reduction in Online Controlled Experiments](https://doi.org/10.1145/3637528.3672352)|Hao Zhou, Kun Sun, Shaoming Li, Yangfeng Fan, Guibin Jiang, Jiaqi Zheng, Tao Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STATE:+A+Robust+ATE+Estimator+of+Heavy-Tailed+Metrics+for+Variance+Reduction+in+Online+Controlled+Experiments)|0|
|[Practical Machine Learning for Streaming Data](https://doi.org/10.1145/3637528.3671442)|Heitor Murilo Gomes, Albert Bifet||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Machine+Learning+for+Streaming+Data)|0|
|[Empower an End-to-end Scalable and Interpretable Data Science Ecosystem using Statistics, AI and Domain Science](https://doi.org/10.1145/3637528.3672194)|Xihong Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empower+an+End-to-end+Scalable+and+Interpretable+Data+Science+Ecosystem+using+Statistics,+AI+and+Domain+Science)|0|
|[Semi-Supervised Learning for Time Series Collected at a Low Sampling Rate](https://doi.org/10.1145/3637528.3672033)|Minyoung Bae, Yooju Shin, Youngeun Nam, Youngseop Lee, JaeGil Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-Supervised+Learning+for+Time+Series+Collected+at+a+Low+Sampling+Rate)|0|
|[Meta Clustering of Neural Bandits](https://doi.org/10.1145/3637528.3671691)|Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Meta+Clustering+of+Neural+Bandits)|0|
|[Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias](https://doi.org/10.1145/3637528.3671824)|Miaomiao Cai, Lei Chen, Yifan Wang, Haoyue Bai, Peijie Sun, Le Wu, Min Zhang, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Popularity-Aware+Alignment+and+Contrast+for+Mitigating+Popularity+Bias)|0|
|[Enhancing Contrastive Learning on Graphs with Node Similarity](https://doi.org/10.1145/3637528.3671898)|Hongliang Chi, Yao Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Contrastive+Learning+on+Graphs+with+Node+Similarity)|0|
|[Fairness in Streaming Submodular Maximization Subject to a Knapsack Constraint](https://doi.org/10.1145/3637528.3671778)|Shuang Cui, Kai Han, Shaojie Tang, Feng Li, Jun Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness+in+Streaming+Submodular+Maximization+Subject+to+a+Knapsack+Constraint)|0|
|[AGS-GNN: Attribute-guided Sampling for Graph Neural Networks](https://doi.org/10.1145/3637528.3671940)|Siddhartha Shankar Das, S. M. Ferdous, Mahantesh M. Halappanavar, Edoardo Serra, Alex Pothen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AGS-GNN:+Attribute-guided+Sampling+for+Graph+Neural+Networks)|0|
|[Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable](https://doi.org/10.1145/3637528.3671907)|Andrew F. Dreher, Etienne Vouga, Donald S. Fussell||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Estimated+Judge+Reliabilities+for+Weighted+Bradley-Terry-Luce+Are+Not+Reliable)|0|
|[Influence Maximization via Graph Neural Bandits](https://doi.org/10.1145/3637528.3671983)|Yuting Feng, Vincent Y. F. Tan, Bogdan Cautis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Influence+Maximization+via+Graph+Neural+Bandits)|0|
|[A Unified Core Structure in Multiplex Networks: From Finding the Densest Subgraph to Modeling User Engagement](https://doi.org/10.1145/3637528.3672011)|Farnoosh Hashemi, Ali Behrouz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Core+Structure+in+Multiplex+Networks:+From+Finding+the+Densest+Subgraph+to+Modeling+User+Engagement)|0|
|[Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals](https://doi.org/10.1145/3637528.3671833)|Marco Heyden, Vadim Arzamasov, Edouard Fouché, Klemens Böhm||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Budgeted+Multi-Armed+Bandits+with+Asymmetric+Confidence+Intervals)|0|
|[Can Modifying Data Address Graph Domain Adaptation?](https://doi.org/10.1145/3637528.3672058)|Renhong Huang, Jiarong Xu, Xin Jiang, Ruichuan An, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Modifying+Data+Address+Graph+Domain+Adaptation?)|0|
|[Uplift Modelling via Gradient Boosting](https://doi.org/10.1145/3637528.3672019)|Bulat Ibragimov, Anton Vakhrushev||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uplift+Modelling+via+Gradient+Boosting)|0|
|[Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification](https://doi.org/10.1145/3637528.3671981)|Ziyu Jia, Haichao Wang, Yucheng Liu, Tianzi Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Distillation+Extracting+Spatial-temporal+Knowledge+for+Lightweight+Multi-channel+Sleep+Stage+Classification)|0|
|[Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain](https://doi.org/10.1145/3637528.3672069)|Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masked+LoGoNet:+Fast+and+Accurate+3D+Image+Analysis+for+Medical+Domain)|0|
|[Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition](https://doi.org/10.1145/3637528.3671670)|Junghun Kim, Ka Hyun Park, JunGi Jang, U Kang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+and+Accurate+Domain+Adaptation+for+Irregular+Tensor+Decomposition)|0|
|[SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning](https://doi.org/10.1145/3637528.3671845)|Jongha Lee, Sunwoo Kim, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SLADE:+Detecting+Dynamic+Anomalies+in+Edge+Streams+without+Labels+via+Self-Supervised+Learning)|0|
|[Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity](https://doi.org/10.1145/3637528.3671835)|Dongyue Li, Aneesh Sharma, Hongyang R. Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Multitask+Learning+Using+Gradient-based+Estimation+of+Task+Affinity)|0|
|[Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions](https://doi.org/10.1145/3637528.3671813)|Haoming Li, Yumou Liu, Zhenzhe Zheng, Zhilin Zhang, Jian Xu, Fan Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Truthful+Bandit+Mechanisms+for+Repeated+Two-stage+Ad+Auctions)|0|
|[Self-Distilled Disentangled Learning for Counterfactual Prediction](https://doi.org/10.1145/3637528.3671782)|Xinshu Li, Mingming Gong, Lina Yao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Distilled+Disentangled+Learning+for+Counterfactual+Prediction)|0|
|[Predicting Long-term Dynamics of Complex Networks via Identifying Skeleton in Hyperbolic Space](https://doi.org/10.1145/3637528.3671968)|Ruikun Li, Huandong Wang, Jinghua Piao, Qingmin Liao, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Long-term+Dynamics+of+Complex+Networks+via+Identifying+Skeleton+in+Hyperbolic+Space)|0|
|[Image Similarity Using an Ensemble of Context-Sensitive Models](https://doi.org/10.1145/3637528.3672004)|Zukang Liao, Min Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Image+Similarity+Using+an+Ensemble+of+Context-Sensitive+Models)|0|
|[Neural Collapse Inspired Debiased Representation Learning for Min-max Fairness](https://doi.org/10.1145/3637528.3671902)|Shenyu Lu, Junyi Chai, Xiaoqian Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Collapse+Inspired+Debiased+Representation+Learning+for+Min-max+Fairness)|0|
|[AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation](https://doi.org/10.1145/3637528.3671699)|Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaGMLP:+AdaBoosting+GNN-to-MLP+Knowledge+Distillation)|0|
|[Handling Varied Objectives by Online Decision Making](https://doi.org/10.1145/3637528.3671812)|Lanjihong Ma, ZhenYu Zhang, YaoXiang Ding, ZhiHua Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Handling+Varied+Objectives+by+Online+Decision+Making)|0|
|[Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series](https://doi.org/10.1145/3637528.3671995)|Jamal Mohammed, Michael H. Böhlen, Sven Helmer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Quantifying+and+Estimating+the+Predictability+Upper+Bound+of+Univariate+Numeric+Time+Series)|0|
|[Scalable Rule Lists Learning with Sampling](https://doi.org/10.1145/3637528.3671989)|Leonardo Pellegrina, Fabio Vandin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Rule+Lists+Learning+with+Sampling)|0|
|[Fredformer: Frequency Debiased Transformer for Time Series Forecasting](https://doi.org/10.1145/3637528.3671928)|Xihao Piao, Zheng Chen, Taichi Murayama, Yasuko Matsubara, Yasushi Sakurai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fredformer:+Frequency+Debiased+Transformer+for+Time+Series+Forecasting)|0|
|[ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education Systems](https://doi.org/10.1145/3637528.3671988)|Hong Qian, Shuo Liu, Mingjia Li, Bingdong Li, Zhi Liu, Aimin Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ORCDF:+An+Oversmoothing-Resistant+Cognitive+Diagnosis+Framework+for+Student+Learning+in+Online+Education+Systems)|0|
|[LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation](https://doi.org/10.1145/3637528.3671772)|Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LARP:+Language+Audio+Relational+Pre-training+for+Cold-Start+Playlist+Continuation)|0|
|[CrossLight: Offline-to-Online Reinforcement Learning for Cross-City Traffic Signal Control](https://doi.org/10.1145/3637528.3671927)|Qian Sun, Rui Zha, Le Zhang, Jingbo Zhou, Yu Mei, Zhiling Li, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CrossLight:+Offline-to-Online+Reinforcement+Learning+for+Cross-City+Traffic+Signal+Control)|0|
|[Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference and Temporal Regularity](https://doi.org/10.1145/3637528.3671916)|Tianao Sun, Ke Fu, Weiming Huang, Kai Zhao, Yongshun Gong, Meng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Going+Where,+by+Whom,+and+at+What+Time:+Next+Location+Prediction+Considering+User+Preference+and+Temporal+Regularity)|0|
|[EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://doi.org/10.1145/3637528.3672068)|Ayush K. Tarun, Vikram S. Chundawat, Murari Mandal, Hong Ming Tan, Bowei Chen, Mohan S. Kankanhalli||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EcoVal:+An+Efficient+Data+Valuation+Framework+for+Machine+Learning)|0|
|[Causal Estimation of Exposure Shifts with Neural Networks and an Application to Inform Air Quality Standards in the US](https://doi.org/10.1145/3637528.3671761)|Mauricio Tec, Kevin Josey, Oladimeji Mudele, Francesca Dominici||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Estimation+of+Exposure+Shifts+with+Neural+Networks+and+an+Application+to+Inform+Air+Quality+Standards+in+the+US)|0|
|[Online Drift Detection with Maximum Concept Discrepancy](https://doi.org/10.1145/3637528.3672016)|Ke Wan, Yi Liang, Susik Yoon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Online+Drift+Detection+with+Maximum+Concept+Discrepancy)|0|
|[CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation](https://doi.org/10.1145/3637528.3672054)|Fan Wang, Chaochao Chen, Weiming Liu, Tianhao Fan, Xinting Liao, Yanchao Tan, Lianyong Qi, Xiaolin Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CE-RCFR:+Robust+Counterfactual+Regression+for+Consensus-Enabled+Treatment+Effect+Estimation)|0|
|[Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks](https://doi.org/10.1145/3637528.3671776)|Jiachuan Wang, Shimin Di, Lei Chen, Charles Wang Wai Ng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+from+Emergence:+A+Study+on+Proactively+Inhibiting+the+Monosemantic+Neurons+of+Artificial+Neural+Networks)|0|
|[POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning](https://doi.org/10.1145/3637528.3671721)|Junxiang Wang, Guangji Bai, Wei Cheng, Zhengzhang Chen, Liang Zhao, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=POND:+Multi-Source+Time+Series+Domain+Adaptation+with+Information-Aware+Prompt+Tuning)|0|
|[DPSW-Sketch: A Differentially Private Sketch Framework for Frequency Estimation over Sliding Windows](https://doi.org/10.1145/3637528.3671694)|Yiping Wang, Yanhao Wang, Cen Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DPSW-Sketch:+A+Differentially+Private+Sketch+Framework+for+Frequency+Estimation+over+Sliding+Windows)|0|
|[DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback](https://doi.org/10.1145/3637528.3671701)|Yiqing Wu, Ruobing Xie, Zhao Zhang, Xu Zhang, Fuzhen Zhuang, Leyu Lin, Zhanhui Kang, Yongjun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DFGNN:+Dual-frequency+Graph+Neural+Network+for+Sign-aware+Feedback)|0|
|[Predicting Cascading Failures with a Hyperparametric Diffusion Model](https://doi.org/10.1145/3637528.3672048)|Bin Xiang, Bogdan Cautis, Xiaokui Xiao, Olga Mula, Dusit Niyato, Laks V. S. Lakshmanan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Cascading+Failures+with+a+Hyperparametric+Diffusion+Model)|0|
|[FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting](https://doi.org/10.1145/3637528.3671713)|Xinyu Zhang, Shanshan Feng, Jianghong Ma, Huiwei Lin, Xutao Li, Yunming Ye, Fan Li, Yew Soon Ong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FRNet:+Frequency-based+Rotation+Network+for+Long-term+Time+Series+Forecasting)|0|
|[Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space](https://doi.org/10.1145/3637528.3672039)|Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King, Rex Ying||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypformer:+Exploring+Efficient+Transformer+Fully+in+Hyperbolic+Space)|0|
|[Practical Single Domain Generalization via Training-time and Test-time Learning](https://doi.org/10.1145/3637528.3671806)|Shuai Yang, Zhen Zhang, Lichuan Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Single+Domain+Generalization+via+Training-time+and+Test-time+Learning)|0|
|[Rethinking Order Dispatching in Online Ride-Hailing Platforms](https://doi.org/10.1145/3637528.3672028)|Zhaoxing Yang, Haiming Jin, Guiyun Fan, Min Lu, Yiran Liu, Xinlang Yue, Hao Pan, Zhe Xu, Guobin Wu, Qun Li, Xiaotong Wang, Jiecheng Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Order+Dispatching+in+Online+Ride-Hailing+Platforms)|0|
|[BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification](https://doi.org/10.1145/3637528.3671963)|Linzhu Yu, Huan Li, Ke Chen, Lidan Shou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BoKA:+Bayesian+Optimization+based+Knowledge+Amalgamation+for+Multi-unknown-domain+Text+Classification)|0|
|[Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition](https://doi.org/10.1145/3637528.3671828)|Junru Zhang, Lang Feng, Zhidan Liu, Yuhan Wu, Yang He, Yabo Dong, Duanqing Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diverse+Intra-+and+Inter-Domain+Activity+Style+Fusion+for+Cross-Person+Generalization+in+Activity+Recognition)|0|
|[Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher](https://doi.org/10.1145/3637528.3671851)|Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, Chao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Distillation+with+Perturbed+Loss:+From+a+Vanilla+Teacher+to+a+Proxy+Teacher)|0|
|[Joint Auction in the Online Advertising Market](https://doi.org/10.1145/3637528.3671746)|Zhen Zhang, Weian Li, Yahui Lei, Bingzhe Wang, Zhicheng Zhang, Qi Qi, Qiang Liu, Xingxing Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Joint+Auction+in+the+Online+Advertising+Market)|0|
|[Long-Term Vessel Trajectory Imputation with Physics-Guided Diffusion Probabilistic Model](https://doi.org/10.1145/3637528.3672086)|Zhiwen Zhang, Zipei Fan, Zewu Lv, Xuan Song, Ryosuke Shibasaki||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long-Term+Vessel+Trajectory+Imputation+with+Physics-Guided+Diffusion+Probabilistic+Model)|0|
|[All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining](https://doi.org/10.1145/3637528.3671913)|Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=All+in+One+and+One+for+All:+A+Simple+yet+Effective+Method+towards+Cross-domain+Graph+Pretraining)|0|
|[Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling](https://doi.org/10.1145/3637528.3671829)|Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, Suhang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-source+Unsupervised+Domain+Adaptation+on+Graphs+with+Transferability+Modeling)|0|
|[Bridging and Compressing Feature and Semantic Spaces for Robust Graph Neural Networks: An Information Theory Perspective](https://doi.org/10.1145/3637528.3671870)|Luying Zhong, Renjie Lin, Jiayin Li, Shiping Wang, Zheyi Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+and+Compressing+Feature+and+Semantic+Spaces+for+Robust+Graph+Neural+Networks:+An+Information+Theory+Perspective)|0|
|[Dynamic Hotel Pricing at Online Travel Platforms: A Popularity and Competitiveness Aware Demand Learning Approach](https://doi.org/10.1145/3637528.3671921)|Fanwei Zhu, Wendong Xiao, Yao Yu, Zemin Liu, Zulong Chen, Weibin Cai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Hotel+Pricing+at+Online+Travel+Platforms:+A+Popularity+and+Competitiveness+Aware+Demand+Learning+Approach)|0|
|[Repeat-Aware Neighbor Sampling for Dynamic Graph Learning](https://doi.org/10.1145/3637528.3672001)|Tao Zou, Yuhao Mao, Junchen Ye, Bowen Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Repeat-Aware+Neighbor+Sampling+for+Dynamic+Graph+Learning)|0|
|[Machine Learning for Clinical Management: From the Lab to the Hospital](https://doi.org/10.1145/3637528.3672497)|Ricard Gavaldà||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Learning+for+Clinical+Management:+From+the+Lab+to+the+Hospital)|0|
|[Metric Decomposition in A/B Tests](https://doi.org/10.1145/3637528.3671556)|Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, Amit Gandhi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Metric+Decomposition+in+A/B+Tests)|0|
|[LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment](https://doi.org/10.1145/3637528.3671550)|Yongfeng Gu, Yupeng Wu, Huakang Lu, Xingyu Lu, Hong Qian, Jun Zhou, Aimin Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LASCA:+A+Large-Scale+Stable+Customer+Segmentation+Approach+to+Credit+Risk+Assessment)|0|
|[Generative Auto-bidding via Conditional Diffusion Modeling](https://doi.org/10.1145/3637528.3671526)|Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Bo Zheng, Yan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Auto-bidding+via+Conditional+Diffusion+Modeling)|0|
|[Learning Metrics that Maximise Power for Accelerated A/B-Tests](https://doi.org/10.1145/3637528.3671512)|Olivier Jeunen, Aleksei Ustimenko||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Metrics+that+Maximise+Power+for+Accelerated+A/B-Tests)|0|
|[Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction](https://doi.org/10.1145/3637528.3671507)|Wenzhao Jiang, Jindong Han, Hao Liu, Tao Tao, Naiqiang Tan, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Cascading+Mixture-of-Experts+for+Urban+Traffic+Congestion+Prediction)|0|
|[False Positives in A/B Tests](https://doi.org/10.1145/3637528.3671631)|Ron Kohavi, Nanyu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=False+Positives+in+A/B+Tests)|0|
|[Causal Machine Learning for Cost-Effective Allocation of Development Aid](https://doi.org/10.1145/3637528.3671551)|Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Machine+Learning+for+Cost-Effective+Allocation+of+Development+Aid)|0|
|[Chromosomal Structural Abnormality Diagnosis by Homologous Similarity](https://doi.org/10.1145/3637528.3671642)|Juren Li, Fanzhe Fu, Ran Wei, Yifei Sun, Zeyu Lai, Ning Song, Xin Chen, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Chromosomal+Structural+Abnormality+Diagnosis+by+Homologous+Similarity)|0|
|[An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware Crop Yield Predictions](https://doi.org/10.1145/3637528.3671536)|Fudong Lin, Kaleb Guillot, Summer Crawford, Yihe Zhang, Xu Yuan, NianFeng Tzeng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Open+and+Large-Scale+Dataset+for+Multi-Modal+Climate+Change-aware+Crop+Yield+Predictions)|0|
|[Modeling User Retention through Generative Flow Networks](https://doi.org/10.1145/3637528.3671531)|Ziru Liu, Shuchang Liu, Bin Yang, Zhenghai Xue, Qingpeng Cai, Xiangyu Zhao, Zijian Zhang, Lantao Hu, Han Li, Peng Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Retention+through+Generative+Flow+Networks)|0|
|[BacktrackSTL: Ultra-Fast Online Seasonal-Trend Decomposition with Backtrack Technique](https://doi.org/10.1145/3637528.3671510)|Haoyu Wang, Hongke Guo, Zhaoliang Zhu, You Zhang, Yu Zhou, Xudong Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BacktrackSTL:+Ultra-Fast+Online+Seasonal-Trend+Decomposition+with+Backtrack+Technique)|0|
|[Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising](https://doi.org/10.1145/3637528.3671529)|Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, Yifan Zeng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Ensemble+Shape+Calibration:+Multi-Field+Post-hoc+Calibration+in+Online+Advertising)|0|
|[GraphStorm: All-in-one Graph Machine Learning Framework for Industry Applications](https://doi.org/10.1145/3637528.3671603)|Da Zheng, Xiang Song, Qi Zhu, Jian Zhang, Theodore Vasiloudis, Runjie Ma, Houyu Zhang, Zichen Wang, Soji Adeshina, Israt Nisa, Alejandro Mottini, Qingjun Cui, Huzefa Rangwala, Belinda Zeng, Christos Faloutsos, George Karypis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphStorm:+All-in-one+Graph+Machine+Learning+Framework+for+Industry+Applications)|0|
|[A Tutorial on Multi-Armed Bandit Applications for Large Language Models](https://doi.org/10.1145/3637528.3671440)|Djallel Bouneffouf, Raphaël Féraud||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Tutorial+on+Multi-Armed+Bandit+Applications+for+Large+Language+Models)|0|
|[Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices](https://doi.org/10.1145/3637528.3671445)|José Cassio dos Santos Junior, Rachel Hu, Richard Song, Yunfei Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain-Driven+LLM+Development:+Insights+into+RAG+and+Fine-Tuning+Practices)|0|
|[Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning](https://doi.org/10.1145/3637528.3671461)|Michal Derezinski, Michael W. Mahoney||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+and+Upcoming+Developments+in+Randomized+Numerical+Linear+Algebra+for+Machine+Learning)|0|
|[Graph Machine Learning Meets Multi-Table Relational Data](https://doi.org/10.1145/3637528.3671471)|Quan Gan, Minjie Wang, David Wipf, Christos Faloutsos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Machine+Learning+Meets+Multi-Table+Relational+Data)|0|
|[Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods](https://doi.org/10.1145/3637528.3671472)|Da Yan, Lyuheng Yuan, Akhlaque Ahmad, Chenguang Zheng, Hongzhi Chen, James Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Systems+for+Scalable+Graph+Analytics+and+Machine+Learning:+Trends+and+Methods)|0|
|[Machine Learning in Finance](https://doi.org/10.1145/3637528.3671488)|Leman Akoglu, Nitesh V. Chawla, Josep DomingoFerrer, Eren Kurshan, Senthil Kumar, Vidyut M. Naware, José A. RodríguezSerrano, Isha Chaturvedi, Saurabh Nagrecha, Mahashweta Das, Tanveer A. Faruquie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Learning+in+Finance)|0|
|[From Word-prediction to Complex Skills: Compositional Thinking and Metacognition in LLMs](https://doi.org/10.1145/3637528.3672193)|Sanjeev Arora||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Word-prediction+to+Complex+Skills:+Compositional+Thinking+and+Metacognition+in+LLMs)|0|
|[GEO: Generative Engine Optimization](https://doi.org/10.1145/3637528.3671900)|Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GEO:+Generative+Engine+Optimization)|0|
|[AI for Nature: From Science to Impact](https://doi.org/10.1145/3637528.3672192)|Tanya Y. BergerWolf||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AI+for+Nature:+From+Science+to+Impact)|0|
|[Statistical Models of Top-k Partial Orders](https://doi.org/10.1145/3637528.3672014)|Amel Awadelkarim, Johan Ugander||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Statistical+Models+of+Top-k+Partial+Orders)|0|
|[Resilient k-Clustering](https://doi.org/10.1145/3637528.3671888)|Sara Ahmadian, MohammadHossein Bateni, Hossein Esfandiari, Silvio Lattanzi, Morteza Monemizadeh, Ashkan NorouziFard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resilient+k-Clustering)|0|
|[A Learned Generalized Geodesic Distance Function-Based Approach for Node Feature Augmentation on Graphs](https://doi.org/10.1145/3637528.3671858)|Amitoz Azad, Yuan Fang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Learned+Generalized+Geodesic+Distance+Function-Based+Approach+for+Node+Feature+Augmentation+on+Graphs)|0|
|[Improved Active Covering via Density-Based Space Transformation](https://doi.org/10.1145/3637528.3671794)|MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, Alipasha Montaseri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improved+Active+Covering+via+Density-Based+Space+Transformation)|0|
|[Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence](https://doi.org/10.1145/3637528.3672067)|Yinhao Bai, Yuhua Zhao, Zhixin Han, Hang Gao, Chao Xue, Mengting Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+Information+Extraction+via+Binomial+Distribution+Guided+Counterpart+Sequence)|0|
|[Graph Mamba: Towards Learning on Graphs with State Space Models](https://doi.org/10.1145/3637528.3672044)|Ali Behrouz, Farnoosh Hashemi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Mamba:+Towards+Learning+on+Graphs+with+State+Space+Models)|0|
|[FaultInsight: Interpreting Hyperscale Data Center Host Faults](https://doi.org/10.1145/3637528.3672051)|Tingzhu Bi, Yang Zhang, Yicheng Pan, Yu Zhang, Meng Ma, Xinrui Jiang, Linlin Han, Feng Wang, Xian Liu, Ping Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FaultInsight:+Interpreting+Hyperscale+Data+Center+Host+Faults)|0|
|[Making Temporal Betweenness Computation Faster and Restless](https://doi.org/10.1145/3637528.3671825)|Filippo Brunelli, Pierluigi Crescenzi, Laurent Viennot||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Making+Temporal+Betweenness+Computation+Faster+and+Restless)|0|
|[Tackling Instance-Dependent Label Noise with Class Rebalance and Geometric Regularization](https://doi.org/10.1145/3637528.3671707)|Shuzhi Cao, Jianfei Ruan, Bo Dong, Bin Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+Instance-Dependent+Label+Noise+with+Class+Rebalance+and+Geometric+Regularization)|0|
|[DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks](https://doi.org/10.1145/3637528.3671997)|Zongsheng Cao, Jing Li, Zigan Wang, Jinliang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffusionE:+Reasoning+on+Knowledge+Graphs+via+Diffusion-based+Graph+Neural+Networks)|0|
|[Path-based Explanation for Knowledge Graph Completion](https://doi.org/10.1145/3637528.3671683)|Heng Chang, Jiangnan Ye, Alejo LopezAvila, Jinhua Du, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-based+Explanation+for+Knowledge+Graph+Completion)|0|
|[Cluster-Wide Task Slowdown Detection in Cloud System](https://doi.org/10.1145/3637528.3671936)|Feiyi Chen, Yingying Zhang, Lunting Fan, Yuxuan Liang, Guansong Pang, Qingsong Wen, Shuiguang Deng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cluster-Wide+Task+Slowdown+Detection+in+Cloud+System)|0|
|[Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in Signed Networks](https://doi.org/10.1145/3637528.3671674)|Jingbang Chen, Qiuyang Mang, Hangrui Zhou, Richard Peng, Yu Gao, Chenhao Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Algorithm+for+Finding+Balanced+Subgraphs+with+Tolerance+in+Signed+Networks)|0|
|[QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering](https://doi.org/10.1145/3637528.3671839)|Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, Yiuming Cheung||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QGRL:+Quaternion+Graph+Representation+Learning+for+Heterogeneous+Feature+Data+Clustering)|0|
|[Can a Deep Learning Model be a Sure Bet for Tabular Prediction?](https://doi.org/10.1145/3637528.3671893)|Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Z. Chen, Jian Wu, Jimeng Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+a+Deep+Learning+Model+be+a+Sure+Bet+for+Tabular+Prediction?)|0|
|[Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology](https://doi.org/10.1145/3637528.3671918)|Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, Yilong Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Profiling+Urban+Streets:+A+Semi-Supervised+Prediction+Model+Based+on+Street+View+Imagery+and+Spatial+Topology)|0|
|[Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network](https://doi.org/10.1145/3637528.3671965)|Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model-driven+Meta-structure+Discovery+in+Heterogeneous+Information+Network)|0|
|[Hate Speech Detection with Generalizable Target-aware Fairness](https://doi.org/10.1145/3637528.3671821)|Tong Chen, Danny Wang, Xurong Liang, Marten Risius, Gianluca Demartini, Hongzhi Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hate+Speech+Detection+with+Generalizable+Target-aware+Fairness)|0|
|[GraphWiz: An Instruction-Following Language Model for Graph Computational Problems](https://doi.org/10.1145/3637528.3672010)|Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphWiz:+An+Instruction-Following+Language+Model+for+Graph+Computational+Problems)|0|
|[Calibration of Time-Series Forecasting: Detecting and Adapting Context-Driven Distribution Shift](https://doi.org/10.1145/3637528.3671926)|Mouxiang Chen, Lefei Shen, Han Fu, Zhuo Li, Jianling Sun, Chenghao Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Calibration+of+Time-Series+Forecasting:+Detecting+and+Adapting+Context-Driven+Distribution+Shift)|0|
|[Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction](https://doi.org/10.1145/3637528.3671770)|Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, Bowen Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Co-Neighbor+Encoding+Schema:+A+Light-cost+Structure+Encoding+Method+for+Dynamic+Link+Prediction)|0|
|[Resurrecting Label Propagation for Graphs with Heterophily and Label Noise](https://doi.org/10.1145/3637528.3671774)|Yao Cheng, Caihua Shan, Yifei Shen, Xiang Li, Siqiang Luo, Dongsheng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resurrecting+Label+Propagation+for+Graphs+with+Heterophily+and+Label+Noise)|0|
|[DyGKT: Dynamic Graph Learning for Knowledge Tracing](https://doi.org/10.1145/3637528.3671773)|Ke Cheng, Linzhi Peng, Pengyang Wang, Junchen Ye, Leilei Sun, Bowen Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyGKT:+Dynamic+Graph+Learning+for+Knowledge+Tracing)|0|
|[Conformal Counterfactual Inference under Hidden Confounding](https://doi.org/10.1145/3637528.3671976)|Zonghao Chen, Ruocheng Guo, JeanFrancois Ton, Yang Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conformal+Counterfactual+Inference+under+Hidden+Confounding)|0|
|[Leveraging Pedagogical Theories to Understand Student Learning Process with Graph-based Reasonable Knowledge Tracing](https://doi.org/10.1145/3637528.3671853)|Jiajun Cui, Hong Qian, Bo Jiang, Wei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Pedagogical+Theories+to+Understand+Student+Learning+Process+with+Graph-based+Reasonable+Knowledge+Tracing)|0|
|[Iterative Weak Learnability and Multiclass AdaBoost](https://doi.org/10.1145/3637528.3671842)|InKoo Cho, Jonathan A. Libgober, Cheng Ding||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Iterative+Weak+Learnability+and+Multiclass+AdaBoost)|0|
|[Divide and Denoise: Empowering Simple Models for Robust Semi-Supervised Node Classification against Label Noise](https://doi.org/10.1145/3637528.3671798)|Kaize Ding, Xiaoxiao Ma, Yixin Liu, Shirui Pan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Divide+and+Denoise:+Empowering+Simple+Models+for+Robust+Semi-Supervised+Node+Classification+against+Label+Noise)|0|
|[Unraveling Block Maxima Forecasting Models with Counterfactual Explanation](https://doi.org/10.1145/3637528.3671923)|Yue Deng, Asadullah Hill Galib, PangNing Tan, Lifeng Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unraveling+Block+Maxima+Forecasting+Models+with+Counterfactual+Explanation)|0|
|[Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance](https://doi.org/10.1145/3637528.3671959)|Thomas Decker, Alexander Koebler, Michael Lebacher, Ingo Thon, Volker Tresp, Florian Buettner||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explanatory+Model+Monitoring+to+Understand+the+Effects+of+Feature+Shifts+on+Performance)|0|
|[Fast Unsupervised Deep Outlier Model Selection with Hypernetworks](https://doi.org/10.1145/3637528.3672003)|Xueying Ding, Yue Zhao, Leman Akoglu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Unsupervised+Deep+Outlier+Model+Selection+with+Hypernetworks)|0|
|[Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions](https://doi.org/10.1145/3637528.3671679)|Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tang, Chengfei Lyu, Guihai Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+On-Device+LLM+Inference+with+Historical+Cloud-Based+LLM+Interactions)|0|
|[IDEA: A Flexible Framework of Certified Unlearning for Graph Neural Networks](https://doi.org/10.1145/3637528.3671744)|Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, Jundong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDEA:+A+Flexible+Framework+of+Certified+Unlearning+for+Graph+Neural+Networks)|0|
|[Unsupervised Alignment of Hypergraphs with Different Scales](https://doi.org/10.1145/3637528.3671955)|Manh Tuan Do, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Alignment+of+Hypergraphs+with+Different+Scales)|0|
|[Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting](https://doi.org/10.1145/3637528.3671961)|Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qingsong Wen, Xuan Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneity-Informed+Meta-Parameter+Learning+for+Spatiotemporal+Time+Series+Forecasting)|0|
|[Representation Learning of Temporal Graphs with Structural Roles](https://doi.org/10.1145/3637528.3671854)|Huaming Du, Long Shi, Xingyan Chen, Yu Zhao, Hegui Zhang, Carl Yang, Fuzhen Zhuang, Gang Kou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+of+Temporal+Graphs+with+Structural+Roles)|0|
|[Reserving-Masking-Reconstruction Model for Self-Supervised Heterogeneous Graph Representation](https://doi.org/10.1145/3637528.3671719)|Haoran Duan, Cheng Xie, Linyu Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reserving-Masking-Reconstruction+Model+for+Self-Supervised+Heterogeneous+Graph+Representation)|0|
|[Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks](https://doi.org/10.1145/3637528.3671912)|Wenying Duan, Tianxiang Fang, Hong Rao, Xiaoxi He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+Identification+of+Graph+Winning+Tickets+in+Adaptive+Spatial-Temporal+Graph+Neural+Networks)|0|
|[Auctions with LLM Summaries](https://doi.org/10.1145/3637528.3672022)|Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, Di Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Auctions+with+LLM+Summaries)|0|
|[GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models](https://doi.org/10.1145/3637528.3672035)|Yi Fang, Dongzhe Fan, Daochen Zha, Qiaoyu Tan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAugLLM:+Improving+Graph+Contrastive+Learning+for+Text-Attributed+Graphs+with+Large+Language+Models)|0|
|[CAT: Interpretable Concept-based Taylor Additive Models](https://doi.org/10.1145/3637528.3672020)|Viet Duong, Qiong Wu, Zhengyi Zhou, Hongjue Zhao, Chenxiang Luo, Eric Zavesky, Huaxiu Yao, Huajie Shao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAT:+Interpretable+Concept-based+Taylor+Additive+Models)|0|
|[SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns](https://doi.org/10.1145/3637528.3671919)|Yuye Feng, Wei Zhang, Yao Fu, Weihao Jiang, Jiang Zhu, Wenqi Ren||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SensitiveHUE:+Multivariate+Time+Series+Anomaly+Detection+by+Enhancing+the+Sensitivity+to+Normal+Patterns)|0|
|[Communication-efficient Multi-service Mobile Traffic Prediction by Leveraging Cross-service Correlations](https://doi.org/10.1145/3637528.3671730)|Zhiying Feng, Qiong Wu, Xu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Communication-efficient+Multi-service+Mobile+Traffic+Prediction+by+Leveraging+Cross-service+Correlations)|0|
|[Federated Graph Learning with Structure Proxy Alignment](https://doi.org/10.1145/3637528.3671717)|Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, Jundong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Federated+Graph+Learning+with+Structure+Proxy+Alignment)|0|
|[Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning](https://doi.org/10.1145/3637528.3671705)|Heyang Gao, Zexu Sun, Hao Yang, Xu Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Policy-Based+Bayesian+Active+Causal+Discovery+with+Deep+Reinforcement+Learning)|0|
|[Graph Condensation for Open-World Graph Learning](https://doi.org/10.1145/3637528.3671917)|Xinyi Gao, Tong Chen, Wentao Zhang, Yayong Li, Xiangguo Sun, Hongzhi Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Condensation+for+Open-World+Graph+Learning)|0|
|[PATE: Proximity-Aware Time Series Anomaly Evaluation](https://doi.org/10.1145/3637528.3671971)|Ramin Ghorbani, Marcel J. T. Reinders, David M. J. Tax||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PATE:+Proximity-Aware+Time+Series+Anomaly+Evaluation)|0|
|[Hierarchical Neural Constructive Solver for Real-world TSP Scenarios](https://doi.org/10.1145/3637528.3672053)|Yong Liang Goh, Zhiguang Cao, Yining Ma, Yanfei Dong, Mohammed Haroon Dupty, Wee Sun Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Neural+Constructive+Solver+for+Real-world+TSP+Scenarios)|0|
|[An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs](https://doi.org/10.1145/3637528.3671939)|Zheng Gong, Ying Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Energy-centric+Framework+for+Category-free+Out-of-distribution+Node+Detection+in+Graphs)|0|
|[Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective](https://doi.org/10.1145/3637528.3671792)|Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Investigating+Out-of-Distribution+Generalization+of+GNNs:+An+Architecture+Perspective)|0|
|[HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning](https://doi.org/10.1145/3637528.3671660)|Zhuoning Guo, Duanyi Yao, Qiang Yang, Hao Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiFGL:+A+Hierarchical+Framework+for+Cross-silo+Cross-device+Federated+Graph+Learning)|0|
|[AnyLoss: Transforming Classification Metrics into Loss Functions](https://doi.org/10.1145/3637528.3672017)|Do Heon Han, Nuno Moniz, Nitesh V. Chawla||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AnyLoss:+Transforming+Classification+Metrics+into+Loss+Functions)|0|
|[Expander Hierarchies for Normalized Cuts on Graphs](https://doi.org/10.1145/3637528.3671978)|Kathrin Hanauer, Monika Henzinger, Robin Münk, Harald Räcke, Maximilian Vötsch||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Expander+Hierarchies+for+Normalized+Cuts+on+Graphs)|0|
|[Model-Agnostic Random Weighting for Out-of-Distribution Generalization](https://doi.org/10.1145/3637528.3671762)|Yue He, Pengfei Tian, Renzhe Xu, Xinwei Shen, Xingxuan Zhang, Peng Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Model-Agnostic+Random+Weighting+for+Out-of-Distribution+Generalization)|0|
|[RoutePlacer: An End-to-End Routability-Aware Placer with Graph Neural Network](https://doi.org/10.1145/3637528.3671895)|Yunbo Hou, Haoran Ye, Yingxue Zhang, Siyuan Xu, Guojie Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RoutePlacer:+An+End-to-End+Routability-Aware+Placer+with+Graph+Neural+Network)|0|
|[Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination](https://doi.org/10.1145/3637528.3671722)|Ming Hu, Zhihao Yue, Xiaofei Xie, Cheng Chen, Yihao Huang, Xian Wei, Xiang Lian, Yang Liu, Mingsong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Is+Aggregation+the+Only+Choice?+Federated+Learning+via+Layer-wise+Model+Recombination)|0|
|[Privacy-Preserved Neural Graph Databases](https://doi.org/10.1145/3637528.3671678)|Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserved+Neural+Graph+Databases)|0|
|[EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy](https://doi.org/10.1145/3637528.3671943)|Yihong Huang, Yuang Zhang, Liping Wang, Fan Zhang, Xuemin Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EntropyStop:+Unsupervised+Deep+Outlier+Detection+with+Loss+Entropy)|0|
|[RC-Mixup: A Data Augmentation Strategy against Noisy Data for Regression Tasks](https://doi.org/10.1145/3637528.3671993)|Seonghyeon Hwang, Minsu Kim, Steven Euijong Whang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RC-Mixup:+A+Data+Augmentation+Strategy+against+Noisy+Data+for+Regression+Tasks)|0|
|[Learn Together Stop Apart: An Inclusive Approach to Ensemble Pruning](https://doi.org/10.1145/3637528.3672018)|Bulat Ibragimov, Gleb Gusev||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learn+Together+Stop+Apart:+An+Inclusive+Approach+to+Ensemble+Pruning)|0|
|[Efficient Discovery of Time Series Motifs under both Length Differences and Warping](https://doi.org/10.1145/3637528.3671726)|Makoto Imamura, Takaaki Nakamura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Discovery+of+Time+Series+Motifs+under+both+Length+Differences+and+Warping)|0|
|[Promoting Fairness and Priority in Selecting k-Winners Using IRV](https://doi.org/10.1145/3637528.3671735)|Md Mouinul Islam, Soroush Vahidi, Baruch Schieber, Senjuti Basu Roy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Promoting+Fairness+and+Priority+in+Selecting+k-Winners+Using+IRV)|0|
|[FreQuant: A Reinforcement-Learning based Adaptive Portfolio Optimization with Multi-frequency Decomposition](https://doi.org/10.1145/3637528.3671668)|Jihyeong Jeon, Jiwon Park, Chanhee Park, U Kang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FreQuant:+A+Reinforcement-Learning+based+Adaptive+Portfolio+Optimization+with+Multi-frequency+Decomposition)|0|
|[Addressing Prediction Delays in Time Series Forecasting: A Continuous GRU Approach with Derivative Regularization](https://doi.org/10.1145/3637528.3671969)|Sheo Yon Jhin, Seojin Kim, Noseong Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Prediction+Delays+in+Time+Series+Forecasting:+A+Continuous+GRU+Approach+with+Derivative+Regularization)|0|
|[MemMap: An Adaptive and Latent Memory Structure for Dynamic Graph Learning](https://doi.org/10.1145/3637528.3672060)|Shuo Ji, Mingzhe Liu, Leilei Sun, Chuanren Liu, Tongyu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MemMap:+An+Adaptive+and+Latent+Memory+Structure+for+Dynamic+Graph+Learning)|0|
|[Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning](https://doi.org/10.1145/3637528.3671689)|Jintian Ji, Songhe Feng, Yidong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tensorized+Unaligned+Multi-view+Clustering+with+Multi-scale+Representation+Learning)|0|
|[Killing Two Birds with One Stone: Cross-modal Reinforced Prompting for Graph and Language Tasks](https://doi.org/10.1145/3637528.3671742)|Wenyuan Jiang, Wenwei Wu, Le Zhang, Zixuan Yuan, Jian Xiang, Jingbo Zhou, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Killing+Two+Birds+with+One+Stone:+Cross-modal+Reinforced+Prompting+for+Graph+and+Language+Tasks)|0|
|[Sketch-Based Replay Projection for Continual Learning](https://doi.org/10.1145/3637528.3671714)|Jack Julian, Yun Sing Koh, Albert Bifet||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sketch-Based+Replay+Projection+for+Continual+Learning)|0|
|[RCTD: Reputation-Constrained Truth Discovery in Sybil Attack Crowdsourcing Environment](https://doi.org/10.1145/3637528.3671803)|Xing Jin, Zhihai Gong, Jiuchuan Jiang, Chao Wang, Jian Zhang, Zhen Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RCTD:+Reputation-Constrained+Truth+Discovery+in+Sybil+Attack+Crowdsourcing+Environment)|0|
|[Bivariate Decision Trees: Smaller, Interpretable, More Accurate](https://doi.org/10.1145/3637528.3671903)|Rasul Kairgeldin, Miguel Á. CarreiraPerpiñán||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bivariate+Decision+Trees:+Smaller,+Interpretable,+More+Accurate)|0|
|[CAFO: Feature-Centric Explanation on Time Series Classification](https://doi.org/10.1145/3637528.3671724)|Jaeho Kim, SeokJu Hahn, Yoontae Hwang, Junghye Lee, Seulki Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CAFO:+Feature-Centric+Explanation+on+Time+Series+Classification)|0|
|[Gandalf: Learning Label-label Correlations in Extreme Multi-label Classification via Label Features](https://doi.org/10.1145/3637528.3672063)|Siddhant Kharbanda, Devaansh Gupta, Erik Schultheis, Atmadeep Banerjee, ChoJui Hsieh, Rohit Babbar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gandalf:+Learning+Label-label+Correlations+in+Extreme+Multi-label+Classification+via+Label+Features)|0|
|[OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing](https://doi.org/10.1145/3637528.3671745)|Tanay Komarlu, Minhao Jiang, Xuan Wang, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OntoType:+Ontology-Guided+and+Pre-Trained+Language+Model+Assisted+Fine-Grained+Entity+Typing)|0|
|[LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions](https://doi.org/10.1145/3637528.3671850)|Ziyi Kou, Shichao Pei, Xiangliang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LeMon:+Automating+Portrait+Generation+for+Zero-Shot+Story+Visualization+with+Multi-Character+Interactions)|0|
|[Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent](https://doi.org/10.1145/3637528.3671890)|Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried N. Gansterer, Nils M. Kriege||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attacking+Graph+Neural+Networks+with+Bit+Flips:+Weisfeiler+and+Leman+Go+Indifferent)|0|
|[Max-Min Diversification with Asymmetric Distances](https://doi.org/10.1145/3637528.3671757)|Iiro Kumpulainen, Florian Adriaens, Nikolaj Tatti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Max-Min+Diversification+with+Asymmetric+Distances)|0|
|[Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks](https://doi.org/10.1145/3637528.3671765)|Yurui Lai, Xiaoyang Lin, Renchi Yang, Hongtao Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Topology-aware+Data+Augmentation+for+High-Degree+Graph+Neural+Networks)|0|
|[ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification](https://doi.org/10.1145/3637528.3671862)|XuanMay Thi Le, Ling Luo, Uwe Aickelin, MinhTuan Tran||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ShapeFormer:+Shapelet+Transformer+for+Multivariate+Time+Series+Classification)|0|
|[ReCTSi: Resource-efficient Correlated Time Series Imputation via Decoupled Pattern Learning and Completeness-aware Attentions](https://doi.org/10.1145/3637528.3671816)|Zhichen Lai, Dalin Zhang, Huan Li, Dongxiang Zhang, Hua Lu, Christian S. Jensen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCTSi:+Resource-efficient+Correlated+Time+Series+Imputation+via+Decoupled+Pattern+Learning+and+Completeness-aware+Attentions)|0|
|[Layer-Wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning](https://doi.org/10.1145/3637528.3671728)|Sunwoo Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Layer-Wise+Adaptive+Gradient+Norm+Penalizing+Method+for+Efficient+and+Accurate+Deep+Learning)|0|
|[Label Learning Method Based on Tensor Projection](https://doi.org/10.1145/3637528.3671671)|Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, DeYan Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Learning+Method+Based+on+Tensor+Projection)|0|
|[Physics-informed Neural ODE for Post-disaster Mobility Recovery](https://doi.org/10.1145/3637528.3672027)|Jiahao Li, Huandong Wang, Xinlei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Physics-informed+Neural+ODE+for+Post-disaster+Mobility+Recovery)|0|
|[Causal Subgraph Learning for Generalizable Inductive Relation Prediction](https://doi.org/10.1145/3637528.3671972)|Mei Li, Xiaoguang Liu, Hua Ji, Shuangjia Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Subgraph+Learning+for+Generalizable+Inductive+Relation+Prediction)|0|
|[SimDiff: Simple Denoising Probabilistic Latent Diffusion Model for Data Augmentation on Multi-modal Knowledge Graph](https://doi.org/10.1145/3637528.3671769)|Ran Li, Shimin Di, Lei Chen, Xiaofang Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SimDiff:+Simple+Denoising+Probabilistic+Latent+Diffusion+Model+for+Data+Augmentation+on+Multi-modal+Knowledge+Graph)|0|
|[ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving](https://doi.org/10.1145/3637528.3671681)|Rongqing Li, Changsheng Li, Yuhang Li, Hanjie Li, Yi Chen, Ye Yuan, Guoren Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ITPNet:+Towards+Instantaneous+Trajectory+Prediction+for+Autonomous+Driving)|0|
|[InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising](https://doi.org/10.1145/3637528.3672032)|Xujia Li, Jingshu Peng, Lei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=InLN:+Knowledge-aware+Incremental+Leveling+Network+for+Dynamic+Advertising)|0|
|[Bi-Objective Contract Allocation for Guaranteed Delivery Advertising](https://doi.org/10.1145/3637528.3671752)|Yan Li, Yundu Huang, Wuyang Mao, Furong Ye, Xiang He, Zhonglin Zu, Shaowei Cai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-Objective+Contract+Allocation+for+Guaranteed+Delivery+Advertising)|0|
|[Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis](https://doi.org/10.1145/3637528.3671875)|Yuekang Li, Yidan Mao, Yifei Yang, Dongmian Zou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Robustness+of+Hyperbolic+Neural+Networks+by+Lipschitz+Analysis)|0|
|[ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs](https://doi.org/10.1145/3637528.3671982)|Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ZeroG:+Investigating+Cross-dataset+Zero-shot+Transferability+in+Graphs)|0|
|[Rethinking Fair Graph Neural Networks from Re-balancing](https://doi.org/10.1145/3637528.3671826)|Zhixun Li, Yushun Dong, Qiang Liu, Jeffrey Xu Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Fair+Graph+Neural+Networks+from+Re-balancing)|0|
|[MulSTE: A Multi-view Spatio-temporal Learning Framework with Heterogeneous Event Fusion for Demand-supply Prediction](https://doi.org/10.1145/3637528.3672030)|Li Lin, Zhiqiang Lu, Shuai Wang, Yunhuai Liu, Zhiqing Hong, Haotian Wang, Shuai Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MulSTE:+A+Multi-view+Spatio-temporal+Learning+Framework+with+Heterogeneous+Event+Fusion+for+Demand-supply+Prediction)|0|
|[PSMC: Provable and Scalable Algorithms for Motif Conductance Based Graph Clustering](https://doi.org/10.1145/3637528.3671666)|Longlong Lin, Tao Jia, Zeli Wang, Jin Zhao, RongHua Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PSMC:+Provable+and+Scalable+Algorithms+for+Motif+Conductance+Based+Graph+Clustering)|0|
|[CONFIDE: Contextual Finite Difference Modelling of PDEs](https://doi.org/10.1145/3637528.3671676)|Ori Linial, Orly Avner, Dotan Di Castro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CONFIDE:+Contextual+Finite+Difference+Modelling+of+PDEs)|0|
|[CASA: Clustered Federated Learning with Asynchronous Clients](https://doi.org/10.1145/3637528.3671979)|Boyi Liu, Yiming Ma, Zimu Zhou, Yexuan Shi, Shuyuan Li, Yongxin Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CASA:+Clustered+Federated+Learning+with+Asynchronous+Clients)|0|
|[FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML](https://doi.org/10.1145/3637528.3671996)|Brian Liu, Rahul Mazumder||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FAST:+An+Optimization+Framework+for+Fast+Additive+Segmentation+in+Transparent+ML)|0|
|[Asymmetric Beta Loss for Evidence-Based Safe Semi-Supervised Multi-Label Learning](https://doi.org/10.1145/3637528.3671756)|HaoZhe Liu, MingKun Xie, ChenChen Zong, ShengJun Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asymmetric+Beta+Loss+for+Evidence-Based+Safe+Semi-Supervised+Multi-Label+Learning)|0|
|[An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem](https://doi.org/10.1145/3637528.3671704)|Huaiyuan Liu, Xianzhang Liu, Donghua Yang, Hongzhi Wang, Yingchi Long, Mengtong Ji, Dongjing Miao, Zhiyu Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Unsupervised+Learning+Framework+Combined+with+Heuristics+for+the+Maximum+Minimal+Cut+Problem)|0|
|[ACER: Accelerating Complex Event Recognition via Two-Phase Filtering under Range Bitmap-Based Indexes](https://doi.org/10.1145/3637528.3671814)|Shizhe Liu, Haipeng Dai, Shaoxu Song, Meng Li, Jingsong Dai, Rong Gu, Guihai Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACER:+Accelerating+Complex+Event+Recognition+via+Two-Phase+Filtering+under+Range+Bitmap-Based+Indexes)|0|
|[Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective](https://doi.org/10.1145/3637528.3671967)|Yunfei Liu, Jintang Li, Yuehe Chen, Ruofan Wu, Ericbk Wang, Jing Zhou, Sheng Tian, Shuheng Shen, Xing Fu, Changhua Meng, Weiqiang Wang, Liang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Modularity+Maximization+for+Graph+Clustering:+A+Contrastive+Learning+Perspective)|0|
|[Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://doi.org/10.1145/3637528.3671710)|Zhanyu Liu, Chaolv Zeng, Guanjie Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Data+Condensation+via+Self-expressive+Graph+Structure+Reconstruction)|0|
|[Generative Pretrained Hierarchical Transformer for Time Series Forecasting](https://doi.org/10.1145/3637528.3671855)|Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Pretrained+Hierarchical+Transformer+for+Time+Series+Forecasting)|0|
|[AIM: Attributing, Interpreting, Mitigating Data Unfairness](https://doi.org/10.1145/3637528.3671797)|Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Yada Zhu, Hendrik F. Hamann, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AIM:+Attributing,+Interpreting,+Mitigating+Data+Unfairness)|0|
|[High-Dimensional Distributed Sparse Classification with Scalable Communication-Efficient Global Updates](https://doi.org/10.1145/3637528.3672038)|Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, James Holt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High-Dimensional+Distributed+Sparse+Classification+with+Scalable+Communication-Efficient+Global+Updates)|0|
|[FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks](https://doi.org/10.1145/3637528.3671834)|Renqiang Luo, Huafei Huang, Shuo Yu, Zhuoyang Han, Estrid He, Xiuzhen Zhang, Feng Xia||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FUGNN:+Harmonizing+Fairness+and+Utility+in+Graph+Neural+Networks)|0|
|[Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge](https://doi.org/10.1145/3637528.3672043)|Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zikun Nie, Hao Zhou, Zaiqing Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Multi-view+Molecular+Representations+with+Structured+and+Unstructured+Knowledge)|0|
|[Cross-Context Backdoor Attacks against Graph Prompt Learning](https://doi.org/10.1145/3637528.3671956)|Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, Xiangliang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-Context+Backdoor+Attacks+against+Graph+Prompt+Learning)|0|
|[PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer](https://doi.org/10.1145/3637528.3671849)|Jiahong Ma, Mingguo He, Zhewei Wei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PolyFormer:+Scalable+Node-wise+Filters+via+Polynomial+Graph+Transformer)|0|
|[Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior](https://doi.org/10.1145/3637528.3672031)|Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Differentiable+Causal+Discovery+in+the+Presence+of+Latent+Confounders+with+Skeleton+Posterior)|0|
|[Graph Anomaly Detection with Few Labels: A Data-Centric Approach](https://doi.org/10.1145/3637528.3671929)|Xiaoxiao Ma, Ruikun Li, Fanzhen Liu, Kaize Ding, Jian Yang, Jia Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Anomaly+Detection+with+Few+Labels:+A+Data-Centric+Approach)|0|
|[A Uniformly Bounded Correlation Function for Spatial Point Patterns](https://doi.org/10.1145/3637528.3671891)|Evgenia Martynova, Johannes Textor||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Uniformly+Bounded+Correlation+Function+for+Spatial+Point+Patterns)|0|
|[Fair Column Subset Selection](https://doi.org/10.1145/3637528.3672005)|Antonis Matakos, Bruno Ordozgoiti, Suhas Thejaswi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Column+Subset+Selection)|0|
|[FLAIM: AIM-based Synthetic Data Generation in the Federated Setting](https://doi.org/10.1145/3637528.3671990)|Samuel Maddock, Graham Cormode, Carsten Maple||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLAIM:+AIM-based+Synthetic+Data+Generation+in+the+Federated+Setting)|0|
|[Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks](https://doi.org/10.1145/3637528.3671720)|Zizhuo Meng, Ke Wan, Yadong Huang, Zhidong Li, Yang Wang, Feng Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Transformer+Hawkes+Processes:+Unveiling+Complex+Interactions+in+Social+Networks)|0|
|[Scaling Training Data with Lossy Image Compression](https://doi.org/10.1145/3637528.3671904)|Katherine L. Mentzer, Andrea Montanari||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Training+Data+with+Lossy+Image+Compression)|0|
|[Learning Causal Networks from Episodic Data](https://doi.org/10.1145/3637528.3671999)|Osman Mian, Sarah Mameche, Jilles Vreeken||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Causal+Networks+from+Episodic+Data)|0|
|[Money Never Sleeps: Maximizing Liquidity Mining Yields in Decentralized Finance](https://doi.org/10.1145/3637528.3671942)|Wangze Ni, Yiwei Zhao, Weijie Sun, Lei Chen, Peng Cheng, Chen Jason Zhang, Xuemin Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Money+Never+Sleeps:+Maximizing+Liquidity+Mining+Yields+in+Decentralized+Finance)|0|
|[Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series](https://doi.org/10.1145/3637528.3671760)|Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mining+of+Switching+Sparse+Networks+for+Missing+Value+Imputation+in+Multivariate+Time+Series)|0|
|[Ontology Enrichment for Effective Fine-grained Entity Typing](https://doi.org/10.1145/3637528.3671857)|Siru Ouyang, Jiaxin Huang, Pranav Pillai, Yunyi Zhang, Yu Zhang, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ontology+Enrichment+for+Effective+Fine-grained+Entity+Typing)|0|
|[BTTackler: A Diagnosis-based Framework for Efficient Deep Learning Hyperparameter Optimization](https://doi.org/10.1145/3637528.3671933)|Zhongyi Pei, Zhiyao Cen, Yipeng Huang, Chen Wang, Lin Liu, Philip S. Yu, Mingsheng Long, Jianmin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BTTackler:+A+Diagnosis-based+Framework+for+Efficient+Deep+Learning+Hyperparameter+Optimization)|0|
|[Fast Multidimensional Partial Fourier Transform with Automatic Hyperparameter Selection](https://doi.org/10.1145/3637528.3671667)|Yongchan Park, Jongjin Kim, U Kang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Multidimensional+Partial+Fourier+Transform+with+Automatic+Hyperparameter+Selection)|0|
|[CoMAL: Contrastive Active Learning for Multi-Label Text Classification](https://doi.org/10.1145/3637528.3671754)|Cheng Peng, Haobo Wang, Ke Chen, Lidan Shou, Chang Yao, Runze Wu, Gang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoMAL:+Contrastive+Active+Learning+for+Multi-Label+Text+Classification)|0|
|[TSC: A Simple Two-Sided Constraint against Over-Smoothing](https://doi.org/10.1145/3637528.3671954)|Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, HongRen Yan, Chao Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSC:+A+Simple+Two-Sided+Constraint+against+Over-Smoothing)|0|
|[CASH via Optimal Diversity for Ensemble Learning](https://doi.org/10.1145/3637528.3671894)|Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, Siddhartha Asthana||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CASH+via+Optimal+Diversity+for+Ensemble+Learning)|0|
|[Unifying Evolution, Explanation, and Discernment: A Generative Approach for Dynamic Graph Counterfactuals](https://doi.org/10.1145/3637528.3671831)|Bardh Prenkaj, Mario VillaizánVallelado, Tobias Leemann, Gjergji Kasneci||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Evolution,+Explanation,+and+Discernment:+A+Generative+Approach+for+Dynamic+Graph+Counterfactuals)|0|
|[Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem](https://doi.org/10.1145/3637528.3671696)|Chen Qian, Huayi Tang, Hong Liang, Yong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reimagining+Graph+Classification+from+a+Prototype+View+with+Optimal+Transport:+Algorithm+and+Theorem)|0|
|[Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation](https://doi.org/10.1145/3637528.3671686)|Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, DitYan Yeung||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-train+and+Refine:+Towards+Higher+Efficiency+in+K-Agnostic+Community+Detection+without+Quality+Degradation)|0|
|[RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms](https://doi.org/10.1145/3637528.3672062)|Luis Roque, Carlos Soares, Luís Torgo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RHiOTS:+A+Framework+for+Evaluating+Hierarchical+Time+Series+Forecasting+Algorithms)|0|
|[A Fast Exact Algorithm to Enumerate Maximal Pseudo-cliques in Large Sparse Graphs](https://doi.org/10.1145/3637528.3672066)|Ahsanur Rahman, Kalyan Roy, Ramiza Maliha, Townim Faisal Chowdhury||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Fast+Exact+Algorithm+to+Enumerate+Maximal+Pseudo-cliques+in+Large+Sparse+Graphs)|0|
|[CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control](https://doi.org/10.1145/3637528.3671998)|Jingqing Ruan, Ziyue Li, Hua Wei, Haoyuan Jiang, Jiaming Lu, Xuantang Xiong, Hangyu Mao, Rui Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoSLight:+Co-optimizing+Collaborator+Selection+and+Decision-making+to+Enhance+Traffic+Signal+Control)|0|
|[A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability](https://doi.org/10.1145/3637528.3671736)|Sakhawat Hossain Saimon, Tanzira Najnin, Jianhua Ruan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Feature+Space+Augmentation+Method+to+Improve+Classification+Performance+and+Evaluation+Reliability)|0|
|[DPHGNN: A Dual Perspective Hypergraph Neural Networks](https://doi.org/10.1145/3637528.3672047)|Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, Tanmoy Chakraborty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DPHGNN:+A+Dual+Perspective+Hypergraph+Neural+Networks)|0|
|[Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask](https://doi.org/10.1145/3637528.3671673)|Zineb Senane, Lele Cao, Valentin Leonhard Buchner, Yusuke Tashiro, Lei You, Pawel Andrzej Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Learning+of+Time+Series+Representation+via+Diffusion+Process+and+Imputation-Interpolation-Forecasting+Mask)|0|
|[Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck](https://doi.org/10.1145/3637528.3671962)|Sangwoo Seo, Sungwon Kim, Jihyeong Jung, Yoonho Lee, Chanyoung Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Explainable+Temporal+Graph+Networks+based+on+Graph+Information+Bottleneck)|0|
|[Offline Imitation Learning with Model-based Reverse Augmentation](https://doi.org/10.1145/3637528.3672059)|JieJing Shao, HaoSen Shi, LanZhe Guo, YuFeng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Offline+Imitation+Learning+with+Model-based+Reverse+Augmentation)|0|
|[NeuroCut: A Neural Approach for Robust Graph Partitioning](https://doi.org/10.1145/3637528.3671815)|Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NeuroCut:+A+Neural+Approach+for+Robust+Graph+Partitioning)|0|
|[Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems](https://doi.org/10.1145/3637528.3672002)|Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, Aimin Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capturing+Homogeneous+Influence+among+Students:+Hypergraph+Cognitive+Diagnosis+for+Intelligent+Education+Systems)|0|
|[Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models](https://doi.org/10.1145/3637528.3671785)|Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, Xin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+OOD+Detection+in+Molecular+Graphs:+A+Novel+Approach+with+Diffusion+Models)|0|
|[Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model](https://doi.org/10.1145/3637528.3671945)|JiangXin Shi, Chi Zhang, Tong Wei, YuFeng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Long-Tailed+Generalization+for+Pre-trained+Vision-Language+Model)|0|
|[MSPipe: Efficient Temporal GNN Training via Staleness-Aware Pipeline](https://doi.org/10.1145/3637528.3671844)|Guangming Sheng, Junwei Su, Chao Huang, Chuan Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MSPipe:+Efficient+Temporal+GNN+Training+via+Staleness-Aware+Pipeline)|0|
|[LPFormer: An Adaptive Graph Transformer for Link Prediction](https://doi.org/10.1145/3637528.3672025)|Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LPFormer:+An+Adaptive+Graph+Transformer+for+Link+Prediction)|0|
|[Orthogonality Matters: Invariant Time Series Representation for Out-of-distribution Classification](https://doi.org/10.1145/3637528.3671768)|Ruize Shi, Hong Huang, Kehan Yin, Wei Zhou, Hai Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Orthogonality+Matters:+Invariant+Time+Series+Representation+for+Out-of-distribution+Classification)|0|
|[CoLiDR: Concept Learning using Aggregated Disentangled Representations](https://doi.org/10.1145/3637528.3671938)|Sanchit Sinha, Guangzhi Xiong, Aidong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoLiDR:+Concept+Learning+using+Aggregated+Disentangled+Representations)|0|
|[On Early Detection of Hallucinations in Factual Question Answering](https://doi.org/10.1145/3637528.3671796)|Ben Snyder, Marius Moisescu, Muhammad Bilal Zafar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Early+Detection+of+Hallucinations+in+Factual+Question+Answering)|0|
|[MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning](https://doi.org/10.1145/3637528.3671905)|Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, Aidong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAML-en-LLM:+Model+Agnostic+Meta-Training+of+LLMs+for+Improved+In-Context+Learning)|0|
|[Fast Computation for the Forest Matrix of an Evolving Graph](https://doi.org/10.1145/3637528.3671822)|Haoxin Sun, Xiaotian Zhou, Zhongzhi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Computation+for+the+Forest+Matrix+of+an+Evolving+Graph)|0|
|[Dual-Assessment Driven Pruning: Iterative Optimizing Layer-wise Sparsity for Large Language Model](https://doi.org/10.1145/3637528.3671780)|Qinghui Sun, Weilun Wang, Yanni Zhu, Shenghuan He, Hao Yi, Zehua Cai, Hong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dual-Assessment+Driven+Pruning:+Iterative+Optimizing+Layer-wise+Sparsity+for+Large+Language+Model)|0|
|[DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization](https://doi.org/10.1145/3637528.3671878)|Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DIVE:+Subgraph+Disagreement+for+Graph+Out-of-Distribution+Generalization)|0|
|[Hierarchical Linear Symbolized Tree-Structured Neural Processes](https://doi.org/10.1145/3637528.3671861)|Jin yang Tai, YiKe Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Linear+Symbolized+Tree-Structured+Neural+Processes)|0|
|[Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute](https://doi.org/10.1145/3637528.3671970)|Tajima Shinji, Ren Sugihara, Ryota Kitahara, Masayuki Karasuyama||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Attributed+Graphlets:+Predictive+Graph+Mining+by+Graphlets+with+Trainable+Attribute)|0|
|[HiGPT: Heterogeneous Graph Language Model](https://doi.org/10.1145/3637528.3671987)|Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HiGPT:+Heterogeneous+Graph+Language+Model)|0|
|[URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering](https://doi.org/10.1145/3637528.3671887)|Ge Teng, Ting Mao, Chen Shen, Xiang Tian, Xuesong Liu, Yaowu Chen, Jieping Ye||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=URRL-IMVC:+Unified+and+Robust+Representation+Learning+for+Incomplete+Multi-View+Clustering)|0|
|[Rotative Factorization Machines](https://doi.org/10.1145/3637528.3671740)|Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, JiRong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rotative+Factorization+Machines)|0|
|[Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model](https://doi.org/10.1145/3637528.3671863)|Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, Yiyan Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Latent+Diffusion-based+Data+Augmentation+for+Continuous-Time+Dynamic+Graph+Model)|0|
|[Flexible Graph Neural Diffusion with Latent Class Representation Learning](https://doi.org/10.1145/3637528.3671860)|Liangtian Wan, Huijin Han, Lu Sun, Zixun Zhang, Zhaolong Ning, Xiaoran Yan, Feng Xia||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Flexible+Graph+Neural+Diffusion+with+Latent+Class+Representation+Learning)|0|
|[STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts](https://doi.org/10.1145/3637528.3671680)|Binwu Wang, Jiaming Ma, Pengkun Wang, Xu Wang, Yudong Zhang, Zhengyang Zhou, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=STONE:+A+Spatio-temporal+OOD+Learning+Framework+Kills+Both+Spatial+and+Temporal+Shifts)|0|
|[Provable Adaptivity of Adam under Non-uniform Smoothness](https://doi.org/10.1145/3637528.3671718)|Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Ruoyu Sun, ZhiMing Ma, TieYan Liu, ZhiQuan Luo, Wei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Provable+Adaptivity+of+Adam+under+Non-uniform+Smoothness)|0|
|[Multi-Scale Detection of Anomalous Spatio-Temporal Trajectories in Evolving Trajectory Datasets](https://doi.org/10.1145/3637528.3671874)|Chenhao Wang, Lisi Chen, Shuo Shang, Christian S. Jensen, Panos Kalnis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Scale+Detection+of+Anomalous+Spatio-Temporal+Trajectories+in+Evolving+Trajectory+Datasets)|0|
|[Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning](https://doi.org/10.1145/3637528.3672045)|Danqing Wang, Antonis Antoniades, KhaDinh Luong, Edwin Zhang, Mert Kosan, Jiachen Li, Ambuj Singh, William Yang Wang, Lei Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Global+Human-guided+Counterfactual+Explanations+for+Molecular+Properties+via+Reinforcement+Learning)|0|
|[Mastering Long-Tail Complexity on Graphs: Characterization, Learning, and Generalization](https://doi.org/10.1145/3637528.3671880)|Haohui Wang, Baoyu Jing, Kaize Ding, Yada Zhu, Wei Cheng, Si Zhang, Yonghui Fan, Liqing Zhang, Dawei Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mastering+Long-Tail+Complexity+on+Graphs:+Characterization,+Learning,+and+Generalization)|0|
|[Unsupervised Heterogeneous Graph Rewriting Attack via Node Clustering](https://doi.org/10.1145/3637528.3671716)|Haosen Wang, Can Xu, Chenglong Shi, Pengfei Zheng, Shiming Zhang, Minhao Cheng, Hongyang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Heterogeneous+Graph+Rewriting+Attack+via+Node+Clustering)|0|
|[Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs](https://doi.org/10.1145/3637528.3671805)|Hewen Wang, Renchi Yang, Xiaokui Xiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Edge-wise+Representation+Learning+in+Edge-Attributed+Bipartite+Graphs)|0|
|[FedNLR: Federated Learning with Neuron-wise Learning Rates](https://doi.org/10.1145/3637528.3672042)|Haozhao Wang, Peirong Zheng, Xingshuo Han, Wenchao Xu, Ruixuan Li, Tianwei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedNLR:+Federated+Learning+with+Neuron-wise+Learning+Rates)|0|
|[Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy](https://doi.org/10.1145/3637528.3671920)|Jiajie Wang, Zhiyuan Jerry Lin, Wen Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Predictions+with+Ambiguous+Time+Delays:+A+Bootstrap+Strategy)|0|
|[A Novel Prompt Tuning for Graph Transformers: Tailoring Prompts to Graph Topologies](https://doi.org/10.1145/3637528.3671804)|Jingchao Wang, Zhengnan Deng, Tongxu Lin, Wenyuan Li, Shaobin Ling||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Novel+Prompt+Tuning+for+Graph+Transformers:+Tailoring+Prompts+to+Graph+Topologies)|0|
|[DyPS: Dynamic Parameter Sharing in Multi-Agent Reinforcement Learning for Spatio-Temporal Resource Allocation](https://doi.org/10.1145/3637528.3672052)|Jingwei Wang, Qianyue Hao, Wenzhen Huang, Xiaochen Fan, Zhentao Tang, Bin Wang, Jianye Hao, Yong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DyPS:+Dynamic+Parameter+Sharing+in+Multi-Agent+Reinforcement+Learning+for+Spatio-Temporal+Resource+Allocation)|0|
|[The Snowflake Hypothesis: Training and Powering GNN with One Node One Receptive Field](https://doi.org/10.1145/3637528.3671766)|Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Junfeng Fang, Xiaojiang Peng, Yuxuan Liang, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Snowflake+Hypothesis:+Training+and+Powering+GNN+with+One+Node+One+Receptive+Field)|0|
|[The Heterophilic Snowflake Hypothesis: Training and Empowering GNNs for Heterophilic Graphs](https://doi.org/10.1145/3637528.3671791)|Kun Wang, Guibin Zhang, Xinnan Zhang, Junfeng Fang, Xun Wu, Guohao Li, Shirui Pan, Wei Huang, Yuxuan Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Heterophilic+Snowflake+Hypothesis:+Training+and+Empowering+GNNs+for+Heterophilic+Graphs)|0|
|[CutAddPaste: Time Series Anomaly Detection by Exploiting Abnormal Knowledge](https://doi.org/10.1145/3637528.3671739)|Rui Wang, Xudong Mou, Renyu Yang, Kai Gao, Pin Liu, Chongwei Liu, Tianyu Wo, Xudong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CutAddPaste:+Time+Series+Anomaly+Detection+by+Exploiting+Abnormal+Knowledge)|0|
|[Advancing Molecule Invariant Representation via Privileged Substructure Identification](https://doi.org/10.1145/3637528.3671886)|Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, Chuan Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advancing+Molecule+Invariant+Representation+via+Privileged+Substructure+Identification)|0|
|[Optimizing Long-tailed Link Prediction in Graph Neural Networks through Structure Representation Enhancement](https://doi.org/10.1145/3637528.3671864)|Yakun Wang, Daixin Wang, Hongrui Liu, Binbin Hu, Yingcui Yan, Qiyang Zhang, Zhiqiang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Long-tailed+Link+Prediction+in+Graph+Neural+Networks+through+Structure+Representation+Enhancement)|0|
|[DiffCrime: A Multimodal Conditional Diffusion Model for Crime Risk Map Inference](https://doi.org/10.1145/3637528.3671843)|Shuliang Wang, Xinyu Pan, Sijie Ruan, Haoyu Han, Ziyu Wang, Hanning Yuan, Jiabao Zhu, Qi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffCrime:+A+Multimodal+Conditional+Diffusion+Model+for+Crime+Risk+Map+Inference)|0|
|[AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing](https://doi.org/10.1145/3637528.3671832)|YunCheng Wang, Xiou Ge, Bin Wang, C.C. Jay Kuo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AsyncET:+Asynchronous+Representation+Learning+for+Knowledge+Graph+Entity+Typing)|0|
|[Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks](https://doi.org/10.1145/3637528.3671838)|Yuwen Wang, Shunyu Liu, Tongya Zheng, Kaixuan Chen, Mingli Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Global+Interactive+Patterns+across+Graphs:+Towards+Interpretable+Graph+Neural+Networks)|0|
|[Self-Supervised Learning for Graph Dataset Condensation](https://doi.org/10.1145/3637528.3671682)|Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, Jiawei Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Supervised+Learning+for+Graph+Dataset+Condensation)|0|
|[From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models](https://doi.org/10.1145/3637528.3671975)|Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Supervised+to+Generative:+A+Novel+Paradigm+for+Tabular+Deep+Learning+with+Large+Language+Models)|0|
|[Dense Subgraph Discovery Meets Strong Triadic Closure](https://doi.org/10.1145/3637528.3671697)|Chamalee Wickrama Arachchi, Iiro Kumpulainen, Nikolaj Tatti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Subgraph+Discovery+Meets+Strong+Triadic+Closure)|0|
|[FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model](https://doi.org/10.1145/3637528.3671897)|Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedBiOT:+LLM+Local+Fine-tuning+in+Federated+Learning+without+Full+Model)|0|
|[Neural Manifold Operators for Learning the Evolution of Physical Dynamics](https://doi.org/10.1145/3637528.3671779)|Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, Wei Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Manifold+Operators+for+Learning+the+Evolution+of+Physical+Dynamics)|0|
|[Distributional Network of Networks for Modeling Data Heterogeneity](https://doi.org/10.1145/3637528.3671994)|Jun Wu, Jingrui He, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributional+Network+of+Networks+for+Modeling+Data+Heterogeneity)|0|
|[Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks](https://doi.org/10.1145/3637528.3671977)|Jiaying Wu, Jiafeng Guo, Bryan Hooi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fake+News+in+Sheep's+Clothing:+Robust+Fake+News+Detection+Against+LLM-Empowered+Style+Attacks)|0|
|[Counterfactual Generative Models for Time-Varying Treatments](https://doi.org/10.1145/3637528.3671950)|Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Generative+Models+for+Time-Varying+Treatments)|0|
|[ProCom: A Few-shot Targeted Community Detection Algorithm](https://doi.org/10.1145/3637528.3671749)|Xixi Wu, Kaiyu Xiong, Yun Xiong, Xiaoxin He, Yao Zhang, Yizhu Jiao, Jiawei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProCom:+A+Few-shot+Targeted+Community+Detection+Algorithm)|0|
|[Cost-Efficient Fraud Risk Optimization with Submodularity in Insurance Claim](https://doi.org/10.1145/3637528.3672012)|Yupeng Wu, Zhibo Zhu, Chaoyi Ma, Hong Qian, Xingyu Lu, Yangwenhui Zhang, Xiaobo Qin, Binjie Fei, Jun Zhou, Aimin Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cost-Efficient+Fraud+Risk+Optimization+with+Submodularity+in+Insurance+Claim)|0|
|[A Deep Prediction Framework for Multi-Source Information via Heterogeneous GNN](https://doi.org/10.1145/3637528.3671966)|Zhen Wu, Jingya Zhou, Jinghui Zhang, Ling Liu, Chizhou Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Deep+Prediction+Framework+for+Multi-Source+Information+via+Heterogeneous+GNN)|0|
|[Fast Computation of Kemeny's Constant for Directed Graphs](https://doi.org/10.1145/3637528.3671859)|Haisong Xia, Zhongzhi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fast+Computation+of+Kemeny's+Constant+for+Directed+Graphs)|0|
|[FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation](https://doi.org/10.1145/3637528.3671899)|Tong Xia, Abhirup Ghosh, Xinchi Qiu, Cecilia Mascolo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FLea:+Addressing+Data+Scarcity+and+Label+Skew+in+Federated+Learning+via+Privacy-preserving+Feature+Augmentation)|0|
|[Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-level Anomaly Detection](https://doi.org/10.1145/3637528.3672050)|Chunjing Xiao, Shikang Pang, Wenxin Tai, Yanlong Huang, Goce Trajcevski, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Motif-Consistent+Counterfactuals+with+Adversarial+Refinement+for+Graph-level+Anomaly+Detection)|0|
|[ReFound: Crafting a Foundation Model for Urban Region Understanding upon Language and Visual Foundations](https://doi.org/10.1145/3637528.3671992)|Congxi Xiao, Jingbo Zhou, Yixiong Xiao, Jizhou Huang, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReFound:+Crafting+a+Foundation+Model+for+Urban+Region+Understanding+upon+Language+and+Visual+Foundations)|0|
|[How to Avoid Jumping to Conclusions: Measuring the Robustness of Outstanding Facts in Knowledge Graphs](https://doi.org/10.1145/3637528.3671763)|Hanhua Xiao, Yuchen Li, Yanhao Wang, Panagiotis Karras, Kyriakos Mouratidis, Natalia Rozalia Avlona||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=How+to+Avoid+Jumping+to+Conclusions:+Measuring+the+Robustness+of+Outstanding+Facts+in+Knowledge+Graphs)|0|
|[Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks](https://doi.org/10.1145/3637528.3671790)|Feiyang Xu, Shunyu Liu, Yunpeng Qing, Yihe Zhou, Yuwen Wang, Mingli Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Prototype-Aware+Learning+for+Active+Voltage+Control+on+Power+Distribution+Networks)|0|
|[FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction](https://doi.org/10.1145/3637528.3671974)|Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Yawei Zhao, Kunlun He, Yao Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FlexCare:+Leveraging+Cross-Task+Synergy+for+Flexible+Multimodal+Healthcare+Prediction)|0|
|[PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly Detection](https://doi.org/10.1145/3637528.3671753)|Ronghui Xu, Hao Miao, Senzhang Wang, Philip S. Yu, Jianxin Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PeFAD:+A+Parameter-Efficient+Federated+Framework+for+Time+Series+Anomaly+Detection)|0|
|[ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup](https://doi.org/10.1145/3637528.3671937)|Yongxin Xu, Xinke Jiang, Xu Chu, Yuzhen Xiao, Chaohe Zhang, Hongxin Ding, Junfeng Zhao, Yasha Wang, Bing Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProtoMix:+Augmenting+Health+Status+Representation+Learning+via+Prototype-based+Mixup)|0|
|[FedRoLA: Robust Federated Learning Against Model Poisoning via Layer-based Aggregation](https://doi.org/10.1145/3637528.3671906)|Gang Yan, Hao Wang, Xu Yuan, Jian Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedRoLA:+Robust+Federated+Learning+Against+Model+Poisoning+via+Layer-based+Aggregation)|0|
|[Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs](https://doi.org/10.1145/3637528.3671964)|Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, Jian Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Team+up+GBDTs+and+DNNs:+Advancing+Efficient+and+Effective+Tabular+Prediction+with+Tree-hybrid+MLPs)|0|
|[Efficient Mixture of Experts based on Large Language Models for Low-Resource Data Preprocessing](https://doi.org/10.1145/3637528.3671873)|Mengyi Yan, Yaoshu Wang, Kehan Pang, Min Xie, Jianxin Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Mixture+of+Experts+based+on+Large+Language+Models+for+Low-Resource+Data+Preprocessing)|0|
|[An Efficient Subgraph GNN with Provable Substructure Counting Power](https://doi.org/10.1145/3637528.3671731)|Zuoyu Yan, Junru Zhou, Liangcai Gao, Zhi Tang, Muhan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Efficient+Subgraph+GNN+with+Provable+Substructure+Counting+Power)|0|
|[Towards Test Time Adaptation via Calibrated Entropy Minimization](https://doi.org/10.1145/3637528.3671672)|Hao Yang, Min Wang, Jinshen Jiang, Yun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Test+Time+Adaptation+via+Calibrated+Entropy+Minimization)|0|
|[Noisy Label Removal for Partial Multi-Label Learning](https://doi.org/10.1145/3637528.3671677)|Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, Junhui Hou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Noisy+Label+Removal+for+Partial+Multi-Label+Learning)|0|
|[Balanced Confidence Calibration for Graph Neural Networks](https://doi.org/10.1145/3637528.3671741)|Hao Yang, Min Wang, Qi Wang, Mingrui Lao, Yun Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Balanced+Confidence+Calibration+for+Graph+Neural+Networks)|0|
|[Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization](https://doi.org/10.1145/3637528.3671827)|Linxiao Yang, Jingbang Yang, Liang Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Decision+Rule+List+Learning+via+Unified+Sequence+Submodular+Optimization)|0|
|[Effective Clustering on Large Attributed Bipartite Graphs](https://doi.org/10.1145/3637528.3671764)|Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, Jieming Shi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Clustering+on+Large+Attributed+Bipartite+Graphs)|0|
|[ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection](https://doi.org/10.1145/3637528.3672007)|Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, Edith C. H. Ngai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCDA:+Concept+Drift+Adaptation+with+Representation+Enhancement+for+Network+Intrusion+Detection)|0|
|[Your Neighbor Matters: Towards Fair Decisions Under Networked Interference](https://doi.org/10.1145/3637528.3671960)|Wenjing Yang, Haotian Wang, Haoxuan Li, Hao Zou, Ruochun Jin, Kun Kuang, Peng Cui||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Your+Neighbor+Matters:+Towards+Fair+Decisions+Under+Networked+Interference)|0|
|[SEBot: Structural Entropy Guided Multi-View Contrastive learning for Social Bot Detection](https://doi.org/10.1145/3637528.3671871)|Yingguang Yang, Qi Wu, Buyun He, Hao Peng, Renyu Yang, Zhifeng Hao, Yong Liao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEBot:+Structural+Entropy+Guided+Multi-View+Contrastive+learning+for+Social+Bot+Detection)|0|
|[AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling](https://doi.org/10.1145/3637528.3671684)|Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, Yuanjing He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdaRD:+An+Adaptive+Response+Denoising+Framework+for+Robust+Learner+Modeling)|0|
|[RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large Spatial-Temporal Data](https://doi.org/10.1145/3637528.3671881)|ChinChia Michael Yeh, Yujie Fan, Xin Dai, Uday Singh Saini, Vivian Lai, Prince Osei Aboagye, Junpeng Wang, Huiyuan Chen, Yan Zheng, Zhongfang Zhuang, Liang Wang, Wei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RPMixer:+Shaking+Up+Time+Series+Forecasting+with+Random+Projections+for+Large+Spatial-Temporal+Data)|0|
|[Using Self-supervised Learning Can Improve Model Fairness](https://doi.org/10.1145/3637528.3671991)|Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Athena Vakali, Daniele Quercia, Fahim Kawsar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Self-supervised+Learning+Can+Improve+Model+Fairness)|0|
|[Self-consistent Deep Geometric Learning for Heterogeneous Multi-source Spatial Point Data Prediction](https://doi.org/10.1145/3637528.3671737)|Dazhou Yu, Xiaoyun Gong, Yun Li, Meikang Qiu, Liang Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-consistent+Deep+Geometric+Learning+for+Heterogeneous+Multi-source+Spatial+Point+Data+Prediction)|0|
|[PolygonGNN: Representation Learning for Polygonal Geometries with Heterogeneous Visibility Graph](https://doi.org/10.1145/3637528.3671738)|Dazhou Yu, Yuntong Hu, Yun Li, Liang Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PolygonGNN:+Representation+Learning+for+Polygonal+Geometries+with+Heterogeneous+Visibility+Graph)|0|
|[GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing](https://doi.org/10.1145/3637528.3672055)|Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, Wei Wei, Yongjun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GinAR:+An+End-To-End+Multivariate+Time+Series+Forecasting+Model+Suitable+for+Variable+Missing)|0|
|[RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes](https://doi.org/10.1145/3637528.3671711)|Xiaoshan Yu, Chuan Qin, Dazhong Shen, Shangshang Yang, Haiping Ma, Hengshu Zhu, Xingyi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RIGL:+A+Unified+Reciprocal+Approach+for+Tracing+the+Independent+and+Group+Learning+Processes)|0|
|[Unveiling Privacy Vulnerabilities: Investigating the Role of Structure in Graph Data](https://doi.org/10.1145/3637528.3672013)|Hanyang Yuan, Jiarong Xu, Cong Wang, Ziqi Yang, Chunping Wang, Keting Yin, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+Privacy+Vulnerabilities:+Investigating+the+Role+of+Structure+in+Graph+Data)|0|
|[Graph Cross Supervised Learning via Generalized Knowledge](https://doi.org/10.1145/3637528.3671830)|Xiangchi Yuan, Yijun Tian, Chunhui Zhang, Yanfang Ye, Nitesh V. Chawla, Chuxu Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Cross+Supervised+Learning+via+Generalized+Knowledge)|0|
|[Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion](https://doi.org/10.1145/3637528.3671783)|Hao Zeng, Jiaqi Wang, Avirup Das, Junying He, Kunpeng Han, Haoyuan Hu, Mingfei Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+Generation+of+Feasible+Solutions+for+Integer+Programming+via+Guided+Diffusion)|0|
|[Path-Specific Causal Reasoning for Fairness-aware Cognitive Diagnosis](https://doi.org/10.1145/3637528.3672049)|Dacao Zhang, Kun Zhang, Le Wu, Mi Tian, Richang Hong, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Path-Specific+Causal+Reasoning+for+Fairness-aware+Cognitive+Diagnosis)|0|
|[Brant-X: A Unified Physiological Signal Alignment Framework](https://doi.org/10.1145/3637528.3671953)|Daoze Zhang, Zhizhang Yuan, Junru Chen, Kerui Chen, Yang Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Brant-X:+A+Unified+Physiological+Signal+Alignment+Framework)|0|
|[Subspace Selection based Prompt Tuning with Nonconvex Nonsmooth Black-Box Optimization](https://doi.org/10.1145/3637528.3671986)|Haozhen Zhang, Hualin Zhang, Bin Gu, Yi Chang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Subspace+Selection+based+Prompt+Tuning+with+Nonconvex+Nonsmooth+Black-Box+Optimization)|0|
|[Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction](https://doi.org/10.1145/3637528.3671946)|Juzheng Zhang, Lanning Wei, Zhen Xu, Quanming Yao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heuristic+Learning+with+Graph+Neural+Networks:+A+Unified+Framework+for+Link+Prediction)|0|
|[Asynchronous Vertical Federated Learning for Kernelized AUC Maximization](https://doi.org/10.1145/3637528.3671930)|Ke Zhang, Ganyu Wang, Han Li, Yulong Wang, Hong Chen, Bin Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Asynchronous+Vertical+Federated+Learning+for+Kernelized+AUC+Maximization)|0|
|[Multivariate Log-based Anomaly Detection for Distributed Database](https://doi.org/10.1145/3637528.3671725)|Lingzhe Zhang, Tong Jia, Mengxi Jia, Ying Li, Yong Yang, Zhonghai Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multivariate+Log-based+Anomaly+Detection+for+Distributed+Database)|0|
|[Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion](https://doi.org/10.1145/3637528.3671911)|Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, Xiao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Logical+Reasoning+with+Relation+Network+for+Inductive+Knowledge+Graph+Completion)|0|
|[Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling](https://doi.org/10.1145/3637528.3671877)|Siwei Zhang, Xi Chen, Yun Xiong, Xixi Wu, Yao Zhang, Yongrui Fu, Yinglong Zhao, Jiawei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Adaptive+Neighborhood+for+Advancing+Temporal+Interaction+Graph+Modeling)|0|
|[Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Networks](https://doi.org/10.1145/3637528.3671665)|Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Yanjie Fu, Jingbo Zhou, Yu Mei, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Irregular+Traffic+Time+Series+Forecasting+Based+on+Asynchronous+Spatio-Temporal+Graph+Convolutional+Networks)|0|
|[A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist](https://doi.org/10.1145/3637528.3671801)|Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multimodal+Foundation+Agent+for+Financial+Trading:+Tool-Augmented,+Diversified,+and+Generalist)|0|
|[Geometric View of Soft Decorrelation in Self-Supervised Learning](https://doi.org/10.1145/3637528.3671914)|Yifei Zhang, Hao Zhu, Zixing Song, Yankai Chen, Xinyu Fu, Ziqiao Meng, Piotr Koniusz, Irwin King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Geometric+View+of+Soft+Decorrelation+in+Self-Supervised+Learning)|0|
|[Representation Learning of Geometric Trees](https://doi.org/10.1145/3637528.3671688)|Zheng Zhang, Allen Zhang, Ruth Nelson, Giorgio Ascoli, Liang Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+of+Geometric+Trees)|0|
|[Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective](https://doi.org/10.1145/3637528.3671910)|Zhiwei Zhang, Minhua Lin, Enyan Dai, Suhang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+Graph+Backdoor+Attacks:+A+Distribution-Preserving+Perspective)|0|
|[Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data](https://doi.org/10.1145/3637528.3672023)|Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Flexible+Time-windowed+Granger+Causality+Integrating+Heterogeneous+Interventional+Time+Series+Data)|0|
|[Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously](https://doi.org/10.1145/3637528.3671909)|Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, Feng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Algorithmic+Fairness+Generalization+under+Covariate+and+Dependence+Shifts+Simultaneously)|0|
|[VertiMRF: Differentially Private Vertical Federated Data Synthesis](https://doi.org/10.1145/3637528.3671771)|Fangyuan Zhao, Zitao Li, Xuebin Ren, Bolin Ding, Shusen Yang, Yaliang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VertiMRF:+Differentially+Private+Vertical+Federated+Data+Synthesis)|0|
|[Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs](https://doi.org/10.1145/3637528.3671952)|Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-Training+and+Prompting+for+Few-Shot+Node+Classification+on+Text-Attributed+Graphs)|0|
|[Conformalized Link Prediction on Graph Neural Networks](https://doi.org/10.1145/3637528.3672061)|Tianyi Zhao, Jian Kang, Lu Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Conformalized+Link+Prediction+on+Graph+Neural+Networks)|0|
|[GeoMix: Towards Geometry-Aware Data Augmentation](https://doi.org/10.1145/3637528.3671700)|Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GeoMix:+Towards+Geometry-Aware+Data+Augmentation)|0|
|[Spuriousness-Aware Meta-Learning for Learning Robust Classifiers](https://doi.org/10.1145/3637528.3672006)|Guangtao Zheng, Wenqian Ye, Aidong Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spuriousness-Aware+Meta-Learning+for+Learning+Robust+Classifiers)|0|
|[SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape](https://doi.org/10.1145/3637528.3671712)|Hua Zheng, KuangHung Liu, Igor Fedorov, Xin Zhang, WenYen Chen, Wei Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SiGeo:+Sub-One-Shot+NAS+via+Geometry+of+Loss+Landscape)|0|
|[Relaxing Continuous Constraints of Equivariant Graph Neural Networks for Broad Physical Dynamics Learning](https://doi.org/10.1145/3637528.3671957)|Zinan Zheng, Yang Liu, Jia Li, Jianhua Yao, Yu Rong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relaxing+Continuous+Constraints+of+Equivariant+Graph+Neural+Networks+for+Broad+Physical+Dynamics+Learning)|0|
|[LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models](https://doi.org/10.1145/3637528.3671810)|Aoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu, Qingda Lu, Qi Zhou, Jiesheng Wu, Quanzheng Li, Qingsong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LogParser-LLM:+Advancing+Efficient+Log+Parsing+with+Large+Language+Models)|0|
|[BitLINK: Temporal Linkage of Address Clusters in Bitcoin Blockchain](https://doi.org/10.1145/3637528.3672037)|Sheng Zhong, Abdullah Mueen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BitLINK:+Temporal+Linkage+of+Address+Clusters+in+Bitcoin+Blockchain)|0|
|[Efficient and Effective Implicit Dynamic Graph Neural Network](https://doi.org/10.1145/3637528.3672026)|Yongjian Zhong, Hieu Vu, Tianbao Yang, Bijaya Adhikari||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+and+Effective+Implicit+Dynamic+Graph+Neural+Network)|0|
|[CURLS: Causal Rule Learning for Subgroups with Significant Treatment Effect](https://doi.org/10.1145/3637528.3671951)|Jiehui Zhou, Linxiao Yang, Xingyu Liu, Xinyue Gu, Liang Sun, Wei Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CURLS:+Causal+Rule+Learning+for+Subgroups+with+Significant+Treatment+Effect)|0|
|[Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models](https://doi.org/10.1145/3637528.3671690)|Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang, Chao Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Collapse+Anchored+Prompt+Tuning+for+Generalizable+Vision-Language+Models)|0|
|[Distributed Thresholded Counting with Limited Interaction](https://doi.org/10.1145/3637528.3671868)|Xiaoyi Zhu, Yuxiang Tian, Zengfeng Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Thresholded+Counting+with+Limited+Interaction)|0|
|[Propagation Structure-Aware Graph Transformer for Robust and Interpretable Fake News Detection](https://doi.org/10.1145/3637528.3672024)|Junyou Zhu, Chao Gao, Ze Yin, Xianghua Li, Jürgen Kurths||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Propagation+Structure-Aware+Graph+Transformer+for+Robust+and+Interpretable+Fake+News+Detection)|0|
|[ControlTraj: Controllable Trajectory Generation with Topology-Constrained Diffusion Model](https://doi.org/10.1145/3637528.3671866)|Yuanshao Zhu, James Jian Qiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian Zhang, Xuetao Wei, Yuxuan Liang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ControlTraj:+Controllable+Trajectory+Generation+with+Topology-Constrained+Diffusion+Model)|0|
|[One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes](https://doi.org/10.1145/3637528.3672029)|Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, Liang Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=One+Fits+All:+Learning+Fair+Graph+Neural+Networks+for+Various+Sensitive+Attributes)|0|
|[Topology-monitorable Contrastive Learning on Dynamic Graphs](https://doi.org/10.1145/3637528.3671777)|Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, Siqiang Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Topology-monitorable+Contrastive+Learning+on+Dynamic+Graphs)|0|
|[MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading](https://doi.org/10.1145/3637528.3672064)|Chuqiao Zong, Chaojie Wang, Molei Qin, Lei Feng, Xinrun Wang, Bo An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MacroHFT:+Memory+Augmented+Context-aware+Reinforcement+Learning+On+High+Frequency+Trading)|0|
|[Lessons Learned while Running ML Models in Harsh Environments](https://doi.org/10.1145/3637528.3672499)|Pedro Bizarro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lessons+Learned+while+Running+ML+Models+in+Harsh+Environments)|0|
|[Next-generation Intelligent Assistants for Wearable Devices](https://doi.org/10.1145/3637528.3672500)|Xin Luna Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Next-generation+Intelligent+Assistants+for+Wearable+Devices)|0|
|[Scalable Graph Learning for your Enterprise](https://doi.org/10.1145/3637528.3672501)|Hema Raghavan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Graph+Learning+for+your+Enterprise)|0|
|[Dynamic Pricing for Multi-Retailer Delivery Platforms with Additive Deep Learning and Evolutionary Optimization](https://doi.org/10.1145/3637528.3671634)|Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Pricing+for+Multi-Retailer+Delivery+Platforms+with+Additive+Deep+Learning+and+Evolutionary+Optimization)|0|
|[Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale](https://doi.org/10.1145/3637528.3671532)|Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, Hitkul Jangra, Ponnurangam Kumaraguru, Kiran Garimella||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Television+Discourse+Decoded:+Comprehensive+Multimodal+Analytics+at+Scale)|0|
|[Large Scale Generative AI Text Applied to Sports and Music](https://doi.org/10.1145/3637528.3671542)|Aaron K. Baughman, Eduardo Morales, Rahul Agarwal, Gozde Akay, Rogério Feris, Tony Johnson, Stephen Hammer, Leonid Karlinsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Scale+Generative+AI+Text+Applied+to+Sports+and+Music)|0|
|[LiGNN: Graph Neural Networks at LinkedIn](https://doi.org/10.1145/3637528.3671566)|Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, KuangHsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, Souvik Ghosh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LiGNN:+Graph+Neural+Networks+at+LinkedIn)|0|
|[Diffusion Model-based Mobile Traffic Generation with Open Data for Network Planning and Optimization](https://doi.org/10.1145/3637528.3671544)|Haoye Chai, Tao Jiang, Li Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Model-based+Mobile+Traffic+Generation+with+Open+Data+for+Network+Planning+and+Optimization)|0|
|[RareBench: Can LLMs Serve as Rare Diseases Specialists?](https://doi.org/10.1145/3637528.3671576)|Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RareBench:+Can+LLMs+Serve+as+Rare+Diseases+Specialists?)|0|
|[MARLP: Time-series Forecasting Control for Agricultural Managed Aquifer Recharge](https://doi.org/10.1145/3637528.3671533)|Yuning Chen, Kang Yang, Zhiyu An, Brady Holder, Luke Paloutzian, Khaled M. Bali, Wan Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MARLP:+Time-series+Forecasting+Control+for+Agricultural+Managed+Aquifer+Recharge)|0|
|[Time-Aware Attention-Based Transformer (TAAT) for Cloud Computing System Failure Prediction](https://doi.org/10.1145/3637528.3671547)|Lingfei Deng, Yunong Wang, Haoran Wang, Xuhua Ma, Xiaoming Du, Xudong Zheng, Dongrui Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Time-Aware+Attention-Based+Transformer+(TAAT)+for+Cloud+Computing+System+Failure+Prediction)|0|
|[FNSPID: A Comprehensive Financial News Dataset in Time Series](https://doi.org/10.1145/3637528.3671629)|Zihan Dong, Xinyu Fan, Zhiyuan Peng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FNSPID:+A+Comprehensive+Financial+News+Dataset+in+Time+Series)|0|
|[Transportation Marketplace Rate Forecast Using Signature Transform](https://doi.org/10.1145/3637528.3671637)|Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip M. Kaminsky, Xinyu Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transportation+Marketplace+Rate+Forecast+Using+Signature+Transform)|0|
|[Intelligent Agents with LLM-based Process Automation](https://doi.org/10.1145/3637528.3671646)|Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Chenyi Zhuang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intelligent+Agents+with+LLM-based+Process+Automation)|0|
|[SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization](https://doi.org/10.1145/3637528.3671626)|Stephen Hahn, Jerry Yin, Rico Zhu, Weihan Xu, Yue Jiang, Simon Mak, Cynthia Rudin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SentHYMNent:+An+Interpretable+and+Sentiment-Driven+Model+for+Algorithmic+Melody+Harmonization)|0|
|[FedSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs](https://doi.org/10.1145/3637528.3671545)|Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin, Lichao Sun, Xiaoyang Wang, Wenxuan Wu, Chulin Xie, Yuhang Yao, Kai Zhang, Qifan Zhang, Yuhui Zhang, Carlee JoeWong, Salman Avestimehr, Chaoyang He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedSecurity:+A+Benchmark+for+Attacks+and+Defenses+in+Federated+Learning+and+Federated+LLMs)|0|
|[Paths2Pair: Meta-path Based Link Prediction in Billion-Scale Commercial Heterogeneous Graphs](https://doi.org/10.1145/3637528.3671563)|Jinquan Hang, Zhiqing Hong, Xinyue Feng, Guang Wang, Guang Yang, Feng Li, Xining Song, Desheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Paths2Pair:+Meta-path+Based+Link+Prediction+in+Billion-Scale+Commercial+Heterogeneous+Graphs)|0|
|[Distributed Harmonization: Federated Clustered Batch Effect Adjustment and Generalization](https://doi.org/10.1145/3637528.3671590)|Bao Hoang, Yijiang Pang, Siqi Liang, Liang Zhan, Paul M. Thompson, Jiayu Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distributed+Harmonization:+Federated+Clustered+Batch+Effect+Adjustment+and+Generalization)|0|
|[Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay](https://doi.org/10.1145/3637528.3671657)|Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, Tridib Mukherjee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+and+Interpretable+Forecasts+on+Non-Smooth+Multivariate+Time+Series+for+Responsible+Gameplay)|0|
|[Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction](https://doi.org/10.1145/3637528.3671546)|Seungwon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, Youjin Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decomposed+Attention+Segment+Recurrent+Neural+Network+for+Orbit+Prediction)|0|
|[RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning](https://doi.org/10.1145/3637528.3671644)|Congyun Jin, Ming Zhang, Weixiao Ma, Yujiao Li, Yingbo Wang, Yabo Jia, Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi, Xiangguo Lv, Fangzhou Li, Wei Xue, Yiran Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RJUA-MedDQA:+A+Multimodal+Benchmark+for+Medical+Document+Question+Answering+and+Clinical+Reasoning)|0|
|[Large Scale Hierarchical Industrial Demand Time-Series Forecasting incorporating Sparsity](https://doi.org/10.1145/3637528.3671632)|Harshavardhan Kamarthi, Aditya B. Sasanur, Xinjie Tong, Xingyu Zhou, James Peters, Joe Czyzyk, B. Aditya Prakash||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Scale+Hierarchical+Industrial+Demand+Time-Series+Forecasting+incorporating+Sparsity)|0|
|[Know, Grow, and Protect Net Worth: Using ML for Asset Protection by Preventing Overdraft Fees](https://doi.org/10.1145/3637528.3671628)|Avishek Kumar, Tyson Silver||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know,+Grow,+and+Protect+Net+Worth:+Using+ML+for+Asset+Protection+by+Preventing+Overdraft+Fees)|0|
|[AutoWebGLM: A Large Language Model-based Web Navigating Agent](https://doi.org/10.1145/3637528.3671620)|Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoWebGLM:+A+Large+Language+Model-based+Web+Navigating+Agent)|0|
|[SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning](https://doi.org/10.1145/3637528.3671534)|Kaidi Li, Tianmeng Yang, Min Zhou, Jiahao Meng, Shendi Wang, Yihui Wu, Boshuai Tan, Hu Song, Lujia Pan, Fan Yu, Zhenli Sheng, Yunhai Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SEFraud:+Graph-based+Self-Explainable+Fraud+Detection+via+Interpretative+Mask+Learning)|0|
|[Harvesting Efficient On-Demand Order Pooling from Skilled Couriers: Enhancing Graph Representation Learning for Refining Real-time Many-to-One Assignments](https://doi.org/10.1145/3637528.3671643)|Yile Liang, Jiuxia Zhao, Donghui Li, Jie Feng, Chen Zhang, Xuetao Ding, Jinghua Hao, Renqing He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Harvesting+Efficient+On-Demand+Order+Pooling+from+Skilled+Couriers:+Enhancing+Graph+Representation+Learning+for+Refining+Real-time+Many-to-One+Assignments)|0|
|[Hyper-Local Deformable Transformers for Text Spotting on Historical Maps](https://doi.org/10.1145/3637528.3671589)|Yijun Lin, YaoYi Chiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hyper-Local+Deformable+Transformers+for+Text+Spotting+on+Historical+Maps)|0|
|[Source Localization for Cross Network Information Diffusion](https://doi.org/10.1145/3637528.3671624)|Chen Ling, Tanmoy Chowdhury, Jie Ji, Sirui Li, Andreas Züfle, Liang Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Source+Localization+for+Cross+Network+Information+Diffusion)|0|
|[MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning](https://doi.org/10.1145/3637528.3671609)|Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu, Jianguo Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MFTCoder:+Boosting+Code+LLMs+with+Multitask+Fine-Tuning)|0|
|[Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm](https://doi.org/10.1145/3637528.3671575)|Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu, Ming Zhang, Xiaowei Ma, Xiangguo Lv, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Automatic+Evaluation+for+LLMs'+Clinical+Capabilities:+Metric,+Data,+and+Algorithm)|0|
|[DAG: Deep Adaptive and Generative K-Free Community Detection on Attributed Graphs](https://doi.org/10.1145/3637528.3671615)|Chang Liu, Yuwen Yang, Yue Ding, Hongtao Lu, Wenqing Lin, Ziming Wu, Wendong Bi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DAG:+Deep+Adaptive+and+Generative+K-Free+Community+Detection+on+Attributed+Graphs)|0|
|[EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis](https://doi.org/10.1145/3637528.3671552)|Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EmoLLMs:+A+Series+of+Emotional+Large+Language+Models+and+Annotation+Tools+for+Comprehensive+Affective+Analysis)|0|
|[MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems](https://doi.org/10.1145/3637528.3671568)|Xianting Lu, Yunong Wang, Yu Fu, Qi Sun, Xuhua Ma, Xudong Zheng, Cheng Zhuo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MISP:+A+Multimodal-based+Intelligent+Server+Failure+Prediction+Model+for+Cloud+Computing+Systems)|0|
|[Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction](https://doi.org/10.1145/3637528.3671508)|Yang Luo, Mohan Gao, Zhemeng Yu, Haoyuan Ge, Xiaofeng Gao, Tengwei Cai, Guihai Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Integrating+System+State+into+Spatio+Temporal+Graph+Neural+Network+for+Microservice+Workload+Prediction)|0|
|[FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting](https://doi.org/10.1145/3637528.3671509)|Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang Sun, Rong Jin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FusionSF:+Fuse+Heterogeneous+Modalities+in+a+Vector+Quantized+Framework+for+Robust+Solar+Power+Forecasting)|0|
|[Valuing an Engagement Surface using a Large Scale Dynamic Causal Model](https://doi.org/10.1145/3637528.3671604)|Abhimanyu Mukerji, Sushant More, Ashwin Viswanathan Kannan, Lakshmi Ravi, Hua Chen, Naman Kohli, Chris Khawand, Dinesh Mandalapu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Valuing+an+Engagement+Surface+using+a+Large+Scale+Dynamic+Causal+Model)|0|
|[EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs](https://doi.org/10.1145/3637528.3671600)|Navid Mohammadi Foumani, Geoffrey Mackellar, Soheila Ghane, Saad Irtza, Nam Nguyen, Mahsa Salehi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EEG2Rep:+Enhancing+Self-supervised+EEG+Representation+Through+Informative+Masked+Inputs)|0|
|[Detecting Abnormal Operations in Concentrated Solar Power Plants from Irregular Sequences of Thermal Images](https://doi.org/10.1145/3637528.3671623)|Sukanya Patra, Nicolas Sournac, Souhaib Ben Taieb||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+Abnormal+Operations+in+Concentrated+Solar+Power+Plants+from+Irregular+Sequences+of+Thermal+Images)|0|
|[Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction](https://doi.org/10.1145/3637528.3671608)|Dekang Qi, Xiuwen Yi, Chengjie Guo, Yanyong Huang, Junbo Zhang, Tianrui Li, Yu Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Spatio-Temporal+Consistency+Enhanced+Differential+Network+for+Interpretable+Indoor+Temperature+Prediction)|0|
|[Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark](https://doi.org/10.1145/3637528.3671616)|Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang Wang, Yao Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Addressing+Shortcomings+in+Fair+Graph+Learning+Datasets:+Towards+a+New+Benchmark)|0|
|[Class-incremental Learning for Time Series: Benchmark and Evaluation](https://doi.org/10.1145/3637528.3671581)|Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H. Le, Ponnuthurai N. Suganthan, Xudong Jiang, Savitha Ramasamy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Class-incremental+Learning+for+Time+Series:+Benchmark+and+Evaluation)|0|
|[Leveraging Exposure Networks for Detecting Fake News Sources](https://doi.org/10.1145/3637528.3671539)|Maor Reuben, Lisa Friedland, Rami Puzis, Nir Grinberg||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Exposure+Networks+for+Detecting+Fake+News+Sources)|0|
|[Tackling Concept Shift in Text Classification using Entailment-style Modeling](https://doi.org/10.1145/3637528.3671541)|Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kasa, Prasanna Srinivasa Murthy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Tackling+Concept+Shift+in+Text+Classification+using+Entailment-style+Modeling)|0|
|[Hierarchical Knowledge Guided Fault Intensity Diagnosis of Complex Industrial Systems](https://doi.org/10.1145/3637528.3671610)|Yu Sha, Shuiping Gou, Bo Liu, Johannes Faber, Ningtao Liu, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Knowledge+Guided+Fault+Intensity+Diagnosis+of+Complex+Industrial+Systems)|0|
|[Lumos: Empowering Multimodal LLMs with Scene Text Recognition](https://doi.org/10.1145/3637528.3671633)|Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lumos:+Empowering+Multimodal+LLMs+with+Scene+Text+Recognition)|0|
|[From Variability to Stability: Advancing RecSys Benchmarking Practices](https://doi.org/10.1145/3637528.3671655)|Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, Alexey Zaytsev||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Variability+to+Stability:+Advancing+RecSys+Benchmarking+Practices)|0|
|[Improving Ego-Cluster for Network Effect Measurement](https://doi.org/10.1145/3637528.3671557)|Wentao Su, Weitao Duan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Ego-Cluster+for+Network+Effect+Measurement)|0|
|[Beimingwu: A Learnware Dock System](https://doi.org/10.1145/3637528.3671617)|ZhiHao Tan, JianDong Liu, XiaoDong Bi, Peng Tan, QinCheng Zheng, HaiTian Liu, Yi Xie, XiaoChuan Zou, Yang Yu, ZhiHua Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beimingwu:+A+Learnware+Dock+System)|0|
|[Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash](https://doi.org/10.1145/3637528.3671574)|Yixin Tang, Yicong Lin, Navdeep S. Sahni||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Business+Policy+Experiments+using+Fractional+Factorial+Designs:+Consumer+Retention+on+DoorDash)|0|
|[TnT-LLM: Text Mining at Scale with Large Language Models](https://doi.org/10.1145/3637528.3671647)|Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W. White, Longqi Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, Nagu Rangan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TnT-LLM:+Text+Mining+at+Scale+with+Large+Language+Models)|0|
|[Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs](https://doi.org/10.1145/3637528.3671583)|Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, Jinjie Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Know+Your+Needs+Better:+Towards+Structured+Understanding+of+Marketer+Demands+with+Analogical+Reasoning+Augmented+LLMs)|0|
|[COMET: NFT Price Prediction with Wallet Profiling](https://doi.org/10.1145/3637528.3671621)|Tianfu Wang, Liwei Deng, Chao Wang, Jianxun Lian, Yue Yan, Nicholas Jing Yuan, Qi Zhang, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COMET:+NFT+Price+Prediction+with+Wallet+Profiling)|0|
|[Neural Optimization with Adaptive Heuristics for Intelligent Marketing System](https://doi.org/10.1145/3637528.3671591)|Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T. Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Sathiya Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Optimization+with+Adaptive+Heuristics+for+Intelligent+Marketing+System)|0|
|[On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications](https://doi.org/10.1145/3637528.3671521)|Chengyao Wen, Yin Lou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Finding+Bi-objective+Pareto-optimal+Fraud+Prevention+Rule+Sets+for+Fintech+Applications)|0|
|[Nested Fusion: A Method for Learning High Resolution Latent Structure of Multi-Scale Measurement Data on Mars](https://doi.org/10.1145/3637528.3671596)|Austin P. Wright, Scott Davidoff, Duen Horng Chau||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Nested+Fusion:+A+Method+for+Learning+High+Resolution+Latent+Structure+of+Multi-Scale+Measurement+Data+on+Mars)|0|
|[TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records](https://doi.org/10.1145/3637528.3671558)|Dongen Wu, Ziquan Fang, Qichen Sun, Lu Chen, Haiyang Hu, Fei Wang, Yunjun Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TrajRecovery:+An+Efficient+Vehicle+Trajectory+Recovery+Framework+based+on+Urban-Scale+Traffic+Camera+Records)|0|
|[LaDe: The First Comprehensive Last-mile Express Dataset from Industry](https://doi.org/10.1145/3637528.3671548)|Lixia Wu, Haomin Wen, Haoyuan Hu, Xiaowei Mao, Yutong Xia, Ergang Shan, Jianbin Zheng, Junhong Lou, Yuxuan Liang, Liuqing Yang, Roger Zimmermann, Youfang Lin, Huaiyu Wan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LaDe:+The+First+Comprehensive+Last-mile+Express+Dataset+from+Industry)|0|
|[Xinyu: An Efficient LLM-based System for Commentary Generation](https://doi.org/10.1145/3637528.3671537)|Yiquan Wu, Bo Tang, Chenyang Xi, Yu Yu, Pengyu Wang, Yifei Liu, Kun Kuang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Jie Hu, Peng Cheng, Zhonghao Wang, Yi Wang, Yi Luo, Mingchuan Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Xinyu:+An+Efficient+LLM-based+System+for+Commentary+Generation)|0|
|[DuMapNet: An End-to-End Vectorization System for City-Scale Lane-Level Map Generation](https://doi.org/10.1145/3637528.3671579)|Deguo Xia, Weiming Zhang, Xiyan Liu, Wei Zhang, Chenting Gong, Jizhou Huang, Mengmeng Yang, Diange Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DuMapNet:+An+End-to-End+Vectorization+System+for+City-Scale+Lane-Level+Map+Generation)|0|
|[VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for Enhanced Detection](https://doi.org/10.1145/3637528.3671527)|Fei Xiao, Shaofeng Cai, Gang Chen, H. V. Jagadish, Beng Chin Ooi, Meihui Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=VecAug:+Unveiling+Camouflaged+Frauds+with+Cohort+Augmentation+for+Enhanced+Detection)|0|
|[Weather Knows What Will Occur: Urban Public Nuisance Events Prediction and Control with Meteorological Assistance](https://doi.org/10.1145/3637528.3671639)|Yi Xie, Tianyu Qiu, Yun Xiong, Xiuqi Huang, Xiaofeng Gao, Chao Chen, Qiang Wang, Haihong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weather+Knows+What+Will+Occur:+Urban+Public+Nuisance+Events+Prediction+and+Control+with+Meteorological+Assistance)|0|
|[Microservice Root Cause Analysis With Limited Observability Through Intervention Recognition in the Latent Space](https://doi.org/10.1145/3637528.3671530)|Zhe Xie, Shenglin Zhang, Yitong Geng, Yao Zhang, Minghua Ma, Xiaohui Nie, Zhenhe Yao, Longlong Xu, Yongqian Sun, Wentao Li, Dan Pei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Microservice+Root+Cause+Analysis+With+Limited+Observability+Through+Intervention+Recognition+in+the+Latent+Space)|0|
|[Understanding the Weakness of Large Language Model Agents within a Complex Android Environment](https://doi.org/10.1145/3637528.3671650)|Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, Zhen Xiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+the+Weakness+of+Large+Language+Model+Agents+within+a+Complex+Android+Environment)|0|
|[XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques](https://doi.org/10.1145/3637528.3671595)|Yu Xiong, Zhipeng Hu, Ye Huang, Runze Wu, Kai Guan, Xingchen Fang, Ji Jiang, Tianze Zhou, Yujing Hu, Haoyu Liu, Tangjie Lyu, Changjie Fan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=XRL-Bench:+A+Benchmark+for+Evaluating+and+Comparing+Explainable+Reinforcement+Learning+Techniques)|0|
|[FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction](https://doi.org/10.1145/3637528.3671613)|Linghua Yang, Wantong Chen, Xiaoxi He, Shuyue Wei, Yi Xu, Zimu Zhou, Yongxin Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedGTP:+Exploiting+Inter-Client+Spatial+Dependency+in+Federated+Graph-based+Traffic+Prediction)|0|
|[OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning](https://doi.org/10.1145/3637528.3671582)|Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenFedLLM:+Training+Large+Language+Models+on+Decentralized+Private+Data+via+Federated+Learning)|0|
|[PAIL: Performance based Adversarial Imitation Learning Engine for Carbon Neutral Optimization](https://doi.org/10.1145/3637528.3671611)|Yuyang Ye, LuAn Tang, Haoyu Wang, Runlong Yu, Wenchao Yu, Erhu He, Haifeng Chen, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAIL:+Performance+based+Adversarial+Imitation+Learning+Engine+for+Carbon+Neutral+Optimization)|0|
|[SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing](https://doi.org/10.1145/3637528.3671586)|Changchang Yin, PinYu Chen, Bingsheng Yao, Dakuo Wang, Jeffrey M. Caterino, Ping Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SepsisLab:+Early+Sepsis+Prediction+with+Uncertainty+Quantification+and+Active+Sensing)|0|
|[Pre-trained KPI Anomaly Detection Model Through Disentangled Transformer](https://doi.org/10.1145/3637528.3671522)|Zhaoyang Yu, Changhua Pei, Xin Wang, Minghua Ma, Chetan Bansal, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang, Xidao Wen, Jianhui Li, Gaogang Xie, Dan Pei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-trained+KPI+Anomaly+Detection+Model+Through+Disentangled+Transformer)|0|
|[An Offline Meta Black-box Optimization Framework for Adaptive Design of Urban Traffic Light Management Systems](https://doi.org/10.1145/3637528.3671606)|Taeyoung Yun, Kanghoon Lee, Sujin Yun, Ilmyung Kim, WonWoo Jung, MinCheol Kwon, Kyujin Choi, Yoohyeon Lee, Jinkyoo Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Offline+Meta+Black-box+Optimization+Framework+for+Adaptive+Design+of+Urban+Traffic+Light+Management+Systems)|0|
|[OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining](https://doi.org/10.1145/3637528.3672354)|Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OAG-Bench:+A+Human-Curated+Benchmark+for+Academic+Graph+Mining)|0|
|[Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English](https://doi.org/10.1145/3637528.3671554)|Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro LopezLira, XiaoYang Liu, Meikang Qiu, Sophia Ananiadou, Min Peng, Jimin Huang, Qianqian Xie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dólares+or+Dollars?+Unraveling+the+Bilingual+Prowess+of+Financial+LLMs+Between+Spanish+and+English)|0|
|[Large Language Model with Curriculum Reasoning for Visual Concept Recognition](https://doi.org/10.1145/3637528.3671653)|Yipeng Zhang, Xin Wang, Hong Chen, Jiapei Fan, Weigao Wen, Hui Xue, Hong Mei, Wenwu Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+with+Curriculum+Reasoning+for+Visual+Concept+Recognition)|0|
|[GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver Selection](https://doi.org/10.1145/3637528.3671627)|Zhanguang Zhang, Didier Chételat, Joseph Cotnareanu, Amur Ghose, Wenyi Xiao, HuiLing Zhen, Yingxue Zhang, Jianye Hao, Mark Coates, Mingxuan Yuan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraSS:+Combining+Graph+Neural+Networks+with+Expert+Knowledge+for+SAT+Solver+Selection)|0|
|[Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns](https://doi.org/10.1145/3637528.3671587)|Zheyuan Zhang, Zehong Wang, Shifu Hou, Evan Hall, Landon Bachman, Jasmine White, Vincent Galassi, Nitesh V. Chawla, Chuxu Zha, Yanfang Ye||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diet-ODIN:+A+Novel+Framework+for+Opioid+Misuse+Detection+with+Interpretable+Dietary+Patterns)|0|
|[TACCO: Task-guided Co-clustering of Clinical Concepts and Patient Visits for Disease Subtyping based on EHR Data](https://doi.org/10.1145/3637528.3671594)|Ziyang Zhang, Hejie Cui, Ran Xu, Yuzhang Xie, Joyce C. Ho, Carl Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TACCO:+Task-guided+Co-clustering+of+Clinical+Concepts+and+Patient+Visits+for+Disease+Subtyping+based+on+EHR+Data)|0|
|[DUE: Dynamic Uncertainty-Aware Explanation Supervision via 3D Imputation](https://doi.org/10.1145/3637528.3671641)|Qilong Zhao, Yifei Zhang, Mengdan Zhu, Siyi Gu, Yuyang Gao, Xiaofeng Yang, Liang Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DUE:+Dynamic+Uncertainty-Aware+Explanation+Supervision+via+3D+Imputation)|0|
|[Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy](https://doi.org/10.1145/3637528.3671614)|Yao Zhao, Zhitian Xie, Chen Liang, Chenyi Zhuang, Jinjie Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lookahead:+An+Inference+Acceleration+Framework+for+Large+Language+Model+with+Lossless+Generation+Accuracy)|0|
|[Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization](https://doi.org/10.1145/3637528.3672353)|Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, Bing Cheng, Wei Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decision+Focused+Causal+Learning+for+Direct+Counterfactual+Marketing+Optimization)|0|
|[A Hands-on Introduction to Time Series Classification and Regression](https://doi.org/10.1145/3637528.3671443)|Anthony J. Bagnall, Matthew Middlehurst, Germain Forestier, Ali IsmailFawaz, Antoine Guillaume, David GuijoRubio, Chang Wei Tan, Angus Dempster, Geoffrey I. Webb||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Hands-on+Introduction+to+Time+Series+Classification+and+Regression)|0|
|[Multi-modal Data Processing for Foundation Models: Practical Guidances and Use Cases](https://doi.org/10.1145/3637528.3671441)|Daoyuan Chen, Yaliang Li, Bolin Ding||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-modal+Data+Processing+for+Foundation+Models:+Practical+Guidances+and+Use+Cases)|0|
|[DARE to Diversify: DAta Driven and Diverse LLM REd Teaming](https://doi.org/10.1145/3637528.3671444)|Manish Nagireddy, Bernat Guillen Pegueroles, Ioana Baldini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DARE+to+Diversify:+DAta+Driven+and+Diverse+LLM+REd+Teaming)|0|
|[Privacy-Preserving Federated Learning using Flower Framework](https://doi.org/10.1145/3637528.3671447)|Mohammad Naseri, Javier FernándezMarqués, Yan Gao, Heng Pan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Privacy-Preserving+Federated+Learning+using+Flower+Framework)|0|
|[Graph Reasoning with LLMs (GReaL)](https://doi.org/10.1145/3637528.3671448)|Anton Tsitsulin, Bryan Perozzi, Bahare Fatemi, Jonathan J. Halcrow||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Reasoning+with+LLMs+(GReaL))|0|
|[Breaking Barriers: A Hands-On Tutorial on AI-Enabled Accessibility to Social Media Content](https://doi.org/10.1145/3637528.3671446)|Julio Villena, Rosa Català, Janine García, Concepción Polo, Yessika Labrador, Francisco delValle, Bhargav Ayyagari||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+Barriers:+A+Hands-On+Tutorial+on+AI-Enabled+Accessibility+to+Social+Media+Content)|0|
|[Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://doi.org/10.1145/3637528.3671463)|Sara Abdali, Richard Anarfi, C. J. Barberan, Jia He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Decoding+the+AI+Pen:+Techniques+and+Challenges+in+Detecting+AI-Generated+Text)|0|
|[Advances in Human Event Modeling: From Graph Neural Networks to Language Models](https://doi.org/10.1145/3637528.3671466)|Songgaojun Deng, Maarten de Rijke, Yue Ning||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Advances+in+Human+Event+Modeling:+From+Graph+Neural+Networks+to+Language+Models)|0|
|[Reasoning and Planning with Large Language Models in Code Development](https://doi.org/10.1145/3637528.3671452)|Hao Ding, Ziwei Fan, Ingo Gühring, Gaurav Gupta, Wooseok Ha, Jun Huan, Linbo Liu, Behrooz OmidvarTehrani, Shiqi Wang, Hao Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reasoning+and+Planning+with+Large+Language+Models+in+Code+Development)|0|
|[Sharing is Caring: A Practical Guide to FAIR(ER) Open Data Release](https://doi.org/10.1145/3637528.3671468)|Amelia Henriksen, Miranda Mundt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sharing+is+Caring:+A+Practical+Guide+to+FAIR(ER)+Open+Data+Release)|0|
|[Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)](https://doi.org/10.1145/3637528.3671467)|Krishnaram Kenthapadi, Mehrnoosh Sameki, Ankur Taly||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grounding+and+Evaluation+for+Large+Language+Models:+Practical+Challenges+and+Lessons+Learned+(Survey))|0|
|[A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide](https://doi.org/10.1145/3637528.3671457)|Sunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, Kijung Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+on+Hypergraph+Neural+Networks:+An+In-Depth+and+Step-By-Step+Guide)|0|
|[Graph Intelligence with Large Language Models and Prompt Learning](https://doi.org/10.1145/3637528.3671456)|Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, Jeffrey Xu Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Intelligence+with+Large+Language+Models+and+Prompt+Learning)|0|
|[Foundation Models for Time Series Analysis: A Tutorial and Survey](https://doi.org/10.1145/3637528.3671451)|Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Foundation+Models+for+Time+Series+Analysis:+A+Tutorial+and+Survey)|0|
|[Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery](https://doi.org/10.1145/3637528.3671464)|Nour Makke, Sanjay Chawla||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Symbolic+Regression:+A+Pathway+to+Interpretability+Towards+Automated+Scientific+Discovery)|0|
|[A Survey of Large Language Models for Graphs](https://doi.org/10.1145/3637528.3671460)|Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+of+Large+Language+Models+for+Graphs)|0|
|[Explainable Artificial Intelligence on Biosignals for Clinical Decision Support](https://doi.org/10.1145/3637528.3671459)|Miriam Cindy Maurer, Jacqueline Michelle Metsch, Philip Hempel, Theresa Bender, Nicolai Spicher, AnneChristin Hauschild||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Artificial+Intelligence+on+Biosignals+for+Clinical+Decision+Support)|0|
|[Urban Foundation Models: A Survey](https://doi.org/10.1145/3637528.3671453)|Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Urban+Foundation+Models:+A+Survey)|0|
|[Inference Optimization of Foundation Models on AI Accelerators](https://doi.org/10.1145/3637528.3671465)|Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas M. Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inference+Optimization+of+Foundation+Models+on+AI+Accelerators)|0|
|[Automated Mining of Structured Knowledge from Text in the Era of Large Language Models](https://doi.org/10.1145/3637528.3671469)|Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Mining+of+Structured+Knowledge+from+Text+in+the+Era+of+Large+Language+Models)|0|
|[Causal Inference with Latent Variables: Recent Advances and Future Prospectives](https://doi.org/10.1145/3637528.3671450)|Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, Jundong Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causal+Inference+with+Latent+Variables:+Recent+Advances+and+Future+Prospectives)|0|
|[A Survey on Safe Multi-Modal Learning Systems](https://doi.org/10.1145/3637528.3671462)|Tianyi Zhao, Liangliang Zhang, Yao Ma, Lu Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Survey+on+Safe+Multi-Modal+Learning+Systems)|0|
|[Responsible AI Day](https://doi.org/10.1145/3637528.3673867)|Ricardo BaezaYates, Nataly Buslón||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Responsible+AI+Day)|0|
|[Heterogeneous Contrastive Learning for Foundation Models and Beyond](https://doi.org/10.1145/3637528.3671454)|Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Heterogeneous+Contrastive+Learning+for+Foundation+Models+and+Beyond)|0|
|[Equity, Diversity & Inclusion (EDI): Special Day at ACM KDD 2024](https://doi.org/10.1145/3637528.3673870)|Tania Cerquitelli, Amin Mantrach||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Equity,+Diversity+&+Inclusion+(EDI):+Special+Day+at+ACM+KDD+2024)|0|
|[Health Day: Building Health AI Ecosystem: From Data Harmonization to Knowledge Discovery](https://doi.org/10.1145/3637528.3673866)|Jake Chen, Peipei Ping||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Health+Day:+Building+Health+AI+Ecosystem:+From+Data+Harmonization+to+Knowledge+Discovery)|0|
|[Overview of ACM SIGKDD 2024 AI4Science4AI Special Day](https://doi.org/10.1145/3637528.3673871)|Wei Ding, Gustau CampsValls||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overview+of+ACM+SIGKDD+2024+AI4Science4AI+Special+Day)|0|
|[KDD 2024 Special Day - AI for Environment](https://doi.org/10.1145/3637528.3673869)|Karina Gibert, Wee Hyong Tok, Miquel SànchezMarrè||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KDD+2024+Special+Day+-+AI+for+Environment)|0|
|[European Data Science Day: KDD-2024 Special Day](https://doi.org/10.1145/3637528.3673868)|Dunja Mladenic, Dumitru Roman||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=European+Data+Science+Day:+KDD-2024+Special+Day)|0|
|[Generative AI Day](https://doi.org/10.1145/3637528.3673872)|Jie Tang, Yuxiao Dong, Michalis Vazirgiannis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+AI+Day)|0|
|[KDD 2024 Finance Day](https://doi.org/10.1145/3637528.3673865)|Guiling Wang, Daniel Borrajo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KDD+2024+Finance+Day)|0|
|[AdKDD 2024](https://doi.org/10.1145/3637528.3671476)|Abraham Bagherjeiran, Nemanja Djuric, KuangChih Lee, Linsey Pang, Vladan Radosavljevic, Suju Rajan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AdKDD+2024)|0|
|[Fragile Earth: Generative and Foundational Models for Sustainable Development](https://doi.org/10.1145/3637528.3671493)|Emre Eftelioglu, Bistra Dilkina, Naoki Abe, Ramakrishnan Kannan, Yuzhou Chen, Yulia R. Gel, Kathleen Buckingham, Auroop R. Ganguly, James Hodson, Jiafu Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fragile+Earth:+Generative+and+Foundational+Models+for+Sustainable+Development)|0|
|[Artificial Intelligence and Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare](https://doi.org/10.1145/3637528.3671497)|Shenda Hong, Daoxin Yin, Gongzheng Tang, Tianfan Fu, Liantao Ma, Junyi Gao, Mengling Feng, Mai Wang, Yu Yang, Fei Wang, Hongfang Liu, Luxia Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Artificial+Intelligence+and+Data+Science+for+Healthcare:+Bridging+Data-Centric+AI+and+People-Centric+Healthcare)|0|
|[TSMO 2024: Two-sided Marketplace Optimization](https://doi.org/10.1145/3637528.3671484)|Mihajlo Grbovic, Vladan Radosavljevic, Minmin Chen, Katerina IliakopoulouZanos, Thanasis Noulas, Amit Goyal, Fabrizio Silvestri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TSMO+2024:+Two-sided+Marketplace+Optimization)|0|
|[KDD workshop on Evaluation and Trustworthiness of Generative AI Models](https://doi.org/10.1145/3637528.3671481)|Yuan Ling, Shujing Dong, Yarong Feng, Zongyi Joe Liu, George Karypis, Chandan K. Reddy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=KDD+workshop+on+Evaluation+and+Trustworthiness+of+Generative+AI+Models)|0|
|[NL2Code-Reasoning and Planning with LLMs for Code Development](https://doi.org/10.1145/3637528.3671505)|Ye Xing, Jun Huan, Wee Hyong Tok, Cong Shen, Johannes Gehrke, Katherine Lin, Arjun Guha, Omer Tripp, Murali Krishna Ramanathan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NL2Code-Reasoning+and+Planning+with+LLMs+for+Code+Development)|0|
