# SIGIR2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation](https://doi.org/10.1145/3626772.3657783)|Alireza Salemi, Surya Kallumadi, Hamed Zamani|University of Massachusetts Amherst; Lowe's Companies, Inc.|This paper studies retrieval-augmented approaches for personalizing large language models (LLMs), which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the retrieval models that deliver a limited number of personal documents to large language models for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for retrieval optimization–one based on reinforcement learning whose reward function is defined using any arbitrary metric for personalized generation and another based on knowledge distillation from the downstream LLM to the retrieval model. This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each LLM input. Extensive experiments on diverse tasks from the language model personalization (LaMP) benchmark reveal statistically significant improvements in six out of seven datasets.|本文研究了个性化大型语言模型(LLM)的检索增强方法，这些方法可能对各种应用和领域产生重大影响。我们首次尝试优化检索模型，将有限数量的个人文档提供给大型语言模型，以实现个性化生成。我们开发了两个优化算法，从下游的个性化生成任务中寻求反馈进行检索优化-一个基于强化学习，其奖励函数是定义使用任意指标的个性化生成和另一个基于知识提取从下游 LLM 到检索模型。本文还介绍了一个前生成和后生成的检索器选择模型，该模型决定检索器为每个 LLM 输入选择什么。从语言模型个性化(LaMP)基准对不同任务的广泛实验显示，七个数据集中有六个在统计学上有显著的改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimization+Methods+for+Personalizing+Large+Language+Models+through+Retrieval+Augmentation)|4|
|[On Generative Agents in Recommendation](https://doi.org/10.1145/3626772.3657844)|An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Generative+Agents+in+Recommendation)|4|
|[C-Pack: Packed Resources For General Chinese Embeddings](https://doi.org/10.1145/3626772.3657878)|Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, JianYun Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C-Pack:+Packed+Resources+For+General+Chinese+Embeddings)|3|
|[Large Language Models can Accurately Predict Searcher Preferences](https://doi.org/10.1145/3626772.3657707)|Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+can+Accurately+Predict+Searcher+Preferences)|3|
|[Data-efficient Fine-tuning for LLM-based Recommendation](https://doi.org/10.1145/3626772.3657807)|Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-efficient+Fine-tuning+for+LLM-based+Recommendation)|2|
|[The Power of Noise: Redefining Retrieval for RAG Systems](https://doi.org/10.1145/3626772.3657834)|Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio Silvestri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Power+of+Noise:+Redefining+Retrieval+for+RAG+Systems)|2|
|[LLaRA: Large Language-Recommendation Assistant](https://doi.org/10.1145/3626772.3657690)|Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLaRA:+Large+Language-Recommendation+Assistant)|2|
|[Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning](https://doi.org/10.1145/3626772.3657828)|Yuyue Zhao, Jiancan Wu, Xiang Wang, Wei Tang, Dingxian Wang, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Let+Me+Do+It+For+You:+Towards+LLM+Empowered+Recommendation+via+Tool+Learning)|2|
|[Evaluating Retrieval Quality in Retrieval-Augmented Generation](https://doi.org/10.1145/3626772.3657957)|Alireza Salemi, Hamed Zamani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Retrieval+Quality+in+Retrieval-Augmented+Generation)|2|
|[Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization](https://doi.org/10.1145/3626772.3657923)|Hamed Zamani, Michael Bendersky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stochastic+RAG:+End-to-End+Retrieval-Augmented+Generation+through+Expected+Utility+Maximization)|2|
|[What do Users Really Ask Large Language Models? An Initial Log Analysis of Google Bard Interactions in the Wild](https://doi.org/10.1145/3626772.3657914)|Johanne R. Trippas, Sara Fahad Dawood Al Lawati, Joel Mackenzie, Luke Gallagher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+do+Users+Really+Ask+Large+Language+Models?+An+Initial+Log+Analysis+of+Google+Bard+Interactions+in+the+Wild)|2|
|[GraphGPT: Graph Instruction Tuning for Large Language Models](https://doi.org/10.1145/3626772.3657775)|Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphGPT:+Graph+Instruction+Tuning+for+Large+Language+Models)|2|
|[UniSAR: Modeling User Transition Behaviors between Search and Recommendation](https://doi.org/10.1145/3626772.3657811)|Teng Shi, Zihua Si, Jun Xu, Xiao Zhang, Xiaoxue Zang, Kai Zheng, Dewei Leng, Yanan Niu, Yang Song|Kuaishou Technology Co., Ltd.; Renmin University of China Gaoling School of Artificial Intelligence|Nowadays, many platforms provide users with both search and recommendation services as important tools for accessing information. The phenomenon has led to a correlation between user search and recommendation behaviors, providing an opportunity to model user interests in a fine-grained way. Existing approaches either model user search and recommendation behaviors separately or overlook the different transitions between user search and recommendation behaviors. In this paper, we propose a framework named UniSAR that effectively models the different types of fine-grained behavior transitions for providing users a Unified Search And Recommendation service. Specifically, UniSAR models the user transition behaviors between search and recommendation through three steps: extraction, alignment, and fusion, which are respectively implemented by transformers equipped with pre-defined masks, contrastive learning that aligns the extracted fine-grained user transitions, and cross-attentions that fuse different transitions. To provide users with a unified service, the learned representations are fed into the downstream search and recommendation models. Joint learning on both search and recommendation data is employed to utilize the knowledge and enhance each other. Experimental results on two public datasets demonstrated the effectiveness of UniSAR in terms of enhancing both search and recommendation simultaneously. The experimental analysis further validates that UniSAR enhances the results by successfully modeling the user transition behaviors between search and recommendation.|目前，许多平台为用户提供搜索和推荐服务，作为获取信息的重要工具。这种现象导致了用户搜索和推荐行为之间的相关性，为用户兴趣的细粒度建模提供了机会。现有的方法或者分别模拟用户搜索和推荐行为，或者忽略用户搜索和推荐行为之间的不同转换。在本文中，我们提出一个名为 UniSAR 的框架，有效地模拟不同类型的细粒度行为转换，为用户提供一个统一的搜索和推荐服务。具体而言，UniSAR 通过三个步骤对搜索和推荐之间的用户转换行为进行建模: 提取、对齐和融合，这些步骤分别由配备预定义掩码的变压器实现，对比学习将提取的细粒度用户转换对齐，交叉注意融合不同的转换。为了向用户提供统一的服务，学习表示被反馈到下游搜索和推荐模型中。对搜索数据和推荐数据进行联合学习，以利用知识并相互增强。在两个公共数据集上的实验结果证明了 UniSAR 在同时提高搜索和推荐能力方面的有效性。实验分析进一步验证了 UniSAR 通过成功地模拟用户在搜索和推荐之间的转换行为，提高了结果的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniSAR:+Modeling+User+Transition+Behaviors+between+Search+and+Recommendation)|1|
|[Poisoning Decentralized Collaborative Recommender System and Its Countermeasures](https://doi.org/10.1145/3626772.3657814)|Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, Hongzhi Yin|University of Electronic Science and Technology of China; Southern University of Science and Technology; The University of Queensland School of Electrical Engineering and Computer Science; The University of Queensland|To make room for privacy and efficiency, the deployment of many recommender systems is experiencing a shift from central servers to personal devices, where the federated recommender systems (FedRecs) and decentralized collaborative recommender systems (DecRecs) are arguably the two most representative paradigms. While both leverage knowledge (e.g., gradients) sharing to facilitate learning local models, FedRecs rely on a central server to coordinate the optimization process, yet in DecRecs, the knowledge sharing directly happens between clients. Knowledge sharing also opens a backdoor for model poisoning attacks, where adversaries disguise themselves as benign clients and disseminate polluted knowledge to achieve malicious goals like promoting an item's exposure rate. Although research on such poisoning attacks provides valuable insights into finding security loopholes and corresponding countermeasures, existing attacks mostly focus on FedRecs, and are either inapplicable or ineffective for DecRecs. Compared with FedRecs where the tampered information can be universally distributed to all clients once uploaded to the cloud, each adversary in DecRecs can only communicate with neighbor clients of a small size, confining its impact to a limited range. To fill the gap, we present a novel attack method named Poisoning with Adaptive Malicious Neighbors (PAMN). With item promotion in top-K recommendation as the attack objective, PAMN effectively boosts target items' ranks with several adversaries that emulate benign clients and transfers adaptively crafted gradients conditioned on each adversary's neighbors. Moreover, with the vulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on user-level gradient clipping with sparsified updating is proposed. Extensive experiments demonstrate the effectiveness of the poisoning attack and the robustness of our defensive mechanism.|为了给隐私和效率腾出空间，许多推荐系统的部署正在经历从中央服务器到个人设备的转变，其中联邦推荐系统(FedRecs)和分散式协作推荐系统(DecRecs)可以说是两个最具代表性的范例。虽然两者都利用知识共享(例如，梯度)来促进本地模型的学习，FedRecs 依赖于一个中央服务器来协调优化过程，但在 DecRecs 中，知识共享直接发生在客户之间。知识共享还为模型中毒攻击打开了一个后门，在这种攻击中，对手把自己伪装成良性的客户，传播受污染的知识，以达到恶意目的，比如提高项目的曝光率。尽管对这类中毒攻击的研究为发现安全漏洞和相应的对策提供了有价值的见解，但现有的攻击主要集中在 FedRecs 上，对 DecRecs 要么不适用，要么无效。与 FedRecs 相比，DecRecs 中的每个对手只能与小规模的邻居客户机通信，将其影响限制在有限的范围内。为了填补这一空白，我们提出了一种新的攻击方法，称为自适应恶意邻居中毒(PAMN)。通过在 top-K 推荐中的物品推广作为攻击目标，PAMN 可以有效地提高目标物品的等级，其中有几个对手可以模仿良性客户，并根据每个对手的邻居传输自适应的精心制作的渐变。此外，针对 DecRecs 的漏洞，提出了一种基于用户级梯度裁剪和稀疏更新的专用防御机制。大量的实验证明了中毒攻击的有效性和我们防御机制的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Decentralized+Collaborative+Recommender+System+and+Its+Countermeasures)|1|
|[Resources for Combining Teaching and Research in Information Retrieval Coursework](https://doi.org/10.1145/3626772.3657886)|Maik Fröbe, Harrisen Scells, Theresa Elstner, Christopher Akiki, Lukas Gienapp, Jan Heinrich Reimer, Sean MacAvaney, Benno Stein, Matthias Hagen, Martin Potthast|Friedrich-Schiller-Universität Jena, Jena, Germany; Bauhaus-Universität Weimar, imar, Germany; Institute for Computer Science, Friedrich-Schiller-Universität Jena, Jena, Germany; University of Kassel, hessian.AI, and ScaDS.AI, Kassel, Germany; University of Glasgow, Glasgow, UK, United Kingdom; Leipzig University, Leipzig, Germany; Informatik, Leipzig University, Leipzig, Germany|The first International Workshop on Open Web Search (WOWS) was held on Thursday, March 28th, at ECIR 2024 in Glasgow, UK. The full-day workshop had two calls for contributions: the first call aimed at scientific contributions to building, operating, and evaluating search engines cooperatively and the cooperative use of the web as a resource for researchers and innovators. The second call for implementations of retrieval components aimed to gain practical experience with joint, cooperative evaluation of search engines and their components. In total, 2~papers were accepted for the first call, and 11~software components were submitted for the second. The workshop ended with breakout sessions on how the OpenWebSearch.eu project can incorporate collaborative evaluations and a hub of search engines.|首届开放网络搜索(WOWS)国际研讨会于3月28日(星期四)在英国格拉斯哥的 ECIR 2024上举行。为期一天的讲习班有两项要求作出贡献的呼吁: 第一项呼吁旨在对合作建立、运营和评价搜索引擎作出科学贡献，以及合作利用网络作为研究人员和创新者的资源。第二个实施检索组件的呼吁旨在通过联合、合作评估搜索引擎及其组件获得实际经验。第一次调用共接受2篇论文，第二次调用提交了11个软件组件。研讨会最后分组讨论了 openwebsearch.eu 项目如何将协作评估和搜索引擎中心结合起来。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resources+for+Combining+Teaching+and+Research+in+Information+Retrieval+Coursework)|1|
|[Leveraging LLMs for Unsupervised Dense Retriever Ranking](https://doi.org/10.1145/3626772.3657798)|Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon||This paper introduces a novel unsupervised technique that utilizes large language models (LLMs) to determine the most suitable dense retriever for a specific test(target) corpus. Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus. The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set. The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in zero-shot scenarios, rendering direct evaluation of the model's effectiveness on the target corpus unattainable. Therefore, the unsupervised selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge. Existing methodologies for ranking dense retrievers fall short in addressing these domain shift scenarios. To tackle this, our method capitalizes on LLMs to create pseudo-relevant queries, labels, and reference lists by analyzing a subset of documents from the target corpus. This allows for the ranking of dense retrievers based on their performance with these pseudo-relevant signals. Significantly, this strategy is the first to depend exclusively on the target corpus data, removing the necessity for training data and test labels. We assessed the effectiveness of our approach by compiling a comprehensive pool of cutting-edge dense retrievers and comparing our method against traditional dense retriever selection benchmarks. The findings reveal that our proposed solution surpasses the existing benchmarks in both the selection and ranking of dense retrievers.|本文介绍了一种新的无监督检索技术，该技术利用大语言模型(LLM)来确定特定测试(目标)语料库中最适合的密集检索器。选择合适的密集检索器对于许多 IR 应用程序至关重要，这些应用程序使用这些检索器，在公共数据集上进行培训，以便在新的私有目标语料库中进行编码或搜索。密集检索器的有效性可以显著降低时，应用于目标语料库的领域或任务偏离原来的训练集。在目标语料没有标记的情况下，这个问题变得更加明显，例如，在零射击情况下，无法直接评估模型对目标语料的有效性。因此，无监督选择最佳预训练密集检索，特别是在领域移动的条件下，出现了一个关键的挑战。现有的密集检索器排名方法在解决这些领域转移场景方面存在不足。为了解决这个问题，我们的方法利用 LLM 通过分析目标语料库中的文档子集来创建伪相关查询、标签和引用列表。这允许根据这些伪相关信号的性能对密集检索器进行排名。值得注意的是，这个策略是第一个完全依赖于目标语料库数据的策略，它消除了培训数据和测试标签的必要性。我们评估了我们的方法的有效性，编制了一个全面的前沿密集检索器库，并将我们的方法与传统的密集检索器选择基准进行了比较。研究结果表明，我们提出的解决方案在密集检索器的选择和排序方面都超过了现有的基准。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+LLMs+for+Unsupervised+Dense+Retriever+Ranking)|1|
|[Large Language Models for Intent-Driven Session Recommendations](https://doi.org/10.1145/3626772.3657688)|Zhu Sun, Hongyang Liu, Xinghua Qu, Kaidong Feng, Yan Wang, Yew Soon Ong|Shanda Group AI Lab; Agency for Science, Technology and Research; Yanshan University; A*STAR Centre for Frontier AI Research and Nanyang Technological University; Macquarie University|Intent-aware session recommendation (ISR) is pivotal in discerning user intents within sessions for precise predictions. Traditional approaches, however, face limitations due to their presumption of a uniform number of intents across all sessions. This assumption overlooks the dynamic nature of user sessions, where the number and type of intentions can significantly vary. In addition, these methods typically operate in latent spaces, thus hinder the model's transparency.Addressing these challenges, we introduce a novel ISR approach, utilizing the advanced reasoning capabilities of large language models (LLMs). First, this approach begins by generating an initial prompt that guides LLMs to predict the next item in a session, based on the varied intents manifested in user sessions. Then, to refine this process, we introduce an innovative prompt optimization mechanism that iteratively self-reflects and adjusts prompts. Furthermore, our prompt selection module, built upon the LLMs' broad adaptability, swiftly selects the most optimized prompts across diverse domains. This new paradigm empowers LLMs to discern diverse user intents at a semantic level, leading to more accurate and interpretable session recommendations. Our extensive experiments on three real-world datasets demonstrate the effectiveness of our method, marking a significant advancement in ISR systems.|意图感知会话推荐(ISR)在识别会话中的用户意图以进行精确预测方面非常关键。然而，传统的方法由于假定所有会话的意图数量一致而面临局限性。这个假设忽略了用户会话的动态特性，其中意图的数量和类型可能有很大的不同。此外，这些方法通常在潜在的空间操作，从而阻碍了模型的透明度。针对这些挑战，我们引入了一种新的 ISR 方法，利用大型语言模型(LLM)的高级推理能力。首先，这种方法首先生成一个初始提示，指导 LLM 根据用户会话中显示的不同意图预测会话中的下一个项目。然后，为了完善这个过程，我们引入了一个创新的提示优化机制，它可以迭代地自我反映和调整提示。此外，我们的提示选择模块，建立在 LLM 的广泛适应性，迅速选择最优化的提示跨不同的领域。这种新的范式使 LLM 能够在语义层次上识别不同的用户意图，从而产生更加准确和可解释的会话建议。我们在三个实际数据集上的广泛实验证明了我们方法的有效性，标志着 ISR 系统的重大进步。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Intent-Driven+Session+Recommendations)|1|
|[Scalable Community Search over Large-scale Graphs based on Graph Transformer](https://doi.org/10.1145/3626772.3657771)|Yuxiang Wang, Xiaoxuan Gou, Xiaoliang Xu, Yuxia Geng, Xiangyu Ke, Tianxing Wu, Zhiyuan Yu, Runhuai Chen, Xiangying Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Community+Search+over+Large-scale+Graphs+based+on+Graph+Transformer)|1|
|[LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction](https://doi.org/10.1145/3626772.3661357)|Chenhao Fang, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Kaushiki Nag, Evren Körpeoglu, Sushant Kumar, Kannan Achan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-Ensemble:+Optimal+Large+Language+Model+Ensemble+Method+for+E-commerce+Product+Attribute+Value+Extraction)|1|
|[Question Suggestion for Conversational Shopping Assistants Using Product Metadata](https://doi.org/10.1145/3626772.3661371)|Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Question+Suggestion+for+Conversational+Shopping+Assistants+Using+Product+Metadata)|1|
|[A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models](https://doi.org/10.1145/3626772.3657813)|Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Setwise+Approach+for+Effective+and+Highly+Efficient+Zero-shot+Ranking+with+Large+Language+Models)|1|
|[Ranked List Truncation for Large Language Model-based Re-Ranking](https://doi.org/10.1145/3626772.3657864)|Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranked+List+Truncation+for+Large+Language+Model-based+Re-Ranking)|1|
|[Fine-grained Textual Inversion Network for Zero-Shot Composed Image Retrieval](https://doi.org/10.1145/3626772.3657831)|Haoqiang Lin, Haokun Wen, Xuemeng Song, Meng Liu, Yupeng Hu, Liqiang Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-grained+Textual+Inversion+Network+for+Zero-Shot+Composed+Image+Retrieval)|1|
|[Denoising Diffusion Recommender Model](https://doi.org/10.1145/3626772.3657825)|Jujia Zhao, Wenjie Wang, Yiyan Xu, Teng Sun, Fuli Feng, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Denoising+Diffusion+Recommender+Model)|1|
|[Systematic Evaluation of Neural Retrieval Models on the Touché 2020 Argument Retrieval Subset of BEIR](https://doi.org/10.1145/3626772.3657861)|Nandan Thakur, Luiz Bonifacio, Maik Fröbe, Alexander Bondarenko, Ehsan Kamalloo, Martin Potthast, Matthias Hagen, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Systematic+Evaluation+of+Neural+Retrieval+Models+on+the+Touché+2020+Argument+Retrieval+Subset+of+BEIR)|1|
|[Generative Retrieval as Multi-Vector Dense Retrieval](https://doi.org/10.1145/3626772.3657697)|Shiguang Wu, Wenda Wei, Mengqi Zhang, Zhumin Chen, Jun Ma, Zhaochun Ren, Maarten de Rijke, Pengjie Ren||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Retrieval+as+Multi-Vector+Dense+Retrieval)|1|
|[A Workbench for Autograding Retrieve/Generate Systems](https://doi.org/10.1145/3626772.3657871)|Laura Dietz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Workbench+for+Autograding+Retrieve/Generate+Systems)|1|
|[Evaluating Generative Ad Hoc Information Retrieval](https://doi.org/10.1145/3626772.3657849)|Lukas Gienapp, Harrisen Scells, Niklas Deckers, Janek Bevendorff, Shuai Wang, Johannes Kiesel, Shahbaz Syed, Maik Fröbe, Guido Zuccon, Benno Stein, Matthias Hagen, Martin Potthast||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Generative+Ad+Hoc+Information+Retrieval)|1|
|[Embark on DenseQuest: A System for Selecting the Best Dense Retriever for a Custom Collection](https://doi.org/10.1145/3626772.3657674)|Ekaterina Khramtsova, Teerapong Leelanupab, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embark+on+DenseQuest:+A+System+for+Selecting+the+Best+Dense+Retriever+for+a+Custom+Collection)|1|
|[QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims](https://doi.org/10.1145/3626772.3657874)|Venktesh V, Abhijit Anand, Avishek Anand, Vinay Setty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QuanTemp:+A+real-world+open-domain+benchmark+for+fact-checking+numerical+claims)|1|
|[Instruction-based Hypergraph Pretraining](https://doi.org/10.1145/3626772.3657715)|Mingdai Yang, Zhiwei Liu, Liangwei Yang, Xiaolong Liu, Chen Wang, Hao Peng, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Instruction-based+Hypergraph+Pretraining)|1|
|[Characterizing Information Seeking Processes with Multiple Physiological Signals](https://doi.org/10.1145/3626772.3657793)|Kaixin Ji, Danula Hettiachchi, Flora D. Salim, Falk Scholer, Damiano Spina||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterizing+Information+Seeking+Processes+with+Multiple+Physiological+Signals)|1|
|[Resources for Brewing BEIR: Reproducible Reference Models and Statistical Analyses](https://doi.org/10.1145/3626772.3657862)|Ehsan Kamalloo, Nandan Thakur, Carlos Lassance, Xueguang Ma, JhengHong Yang, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resources+for+Brewing+BEIR:+Reproducible+Reference+Models+and+Statistical+Analyses)|1|
|[On the Evaluation of Machine-Generated Reports](https://doi.org/10.1145/3626772.3657846)|James Mayfield, Eugene Yang, Dawn J. Lawrie, Sean MacAvaney, Paul McNamee, Douglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Selin Kayi, Kate Sanders, Marc Mason, Noah Hibbler||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Evaluation+of+Machine-Generated+Reports)|1|
|[Are Large Language Models Good at Utility Judgments?](https://doi.org/10.1145/3626772.3657784)|Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Large+Language+Models+Good+at+Utility+Judgments?)|1|
|[CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web Relevance Ranking](https://doi.org/10.1145/3626772.3657851)|Josef Vonásek, Milan Straka, Rostislav Krc, Lenka Lasonová, Ekaterina Egorova, Jana Straková, Jakub Náplava|Seznam.cz; Institute of Formal and Applied Linguistics, Charles University|We present CWRCzech, Click Web Ranking dataset for Czech, a 100M query-document Czech click dataset for relevance ranking with user behavior data collected from search engine logs of Seznam.cz. To the best of our knowledge, CWRCzech is the largest click dataset with raw text published so far. It provides document positions in the search results as well as information about user behavior: 27.6M clicked documents and 10.8M dwell times. In addition, we also publish a manually annotated Czech test for the relevance task, containing nearly 50k query-document pairs, each annotated by at least 2 annotators. Finally, we analyze how the user behavior data improve relevance ranking and show that models trained on data automatically harnessed at sufficient scale can surpass the performance of models trained on human annotated data. CWRCzech is published under an academic non-commercial license and is available to the research community at https://github.com/seznam/CWRCzech.|我们为捷克提供了一个100M 的查询文档捷克点击数据集，用于从 Seznam.cz 的搜索引擎日志中收集的用户行为数据进行相关性排名。据我们所知，CWR 捷克是迄今为止发布原始文本的最大的点击数据集。它提供了搜索结果中的文档位置以及关于用户行为的信息: 27.6 M 的单击文档和10.8 M 的停留时间。此外，我们还为相关任务发布了一个手动注释的捷克测试，包含近50k 个查询-文档对，每个查询-文档对至少由2个注释者进行注释。最后，我们分析了用户行为数据如何提高相关性排名，并表明在足够大的规模上自动利用数据训练的模型可以超过在人类注释数据上训练的模型的性能。捷克语研究中心以学术非商业许可证的形式发表论文，研究团体可以在 https://github.com/seznam/CWRCzech 获得该论文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CWRCzech:+100M+Query-Document+Czech+Click+Dataset+and+Its+Application+to+Web+Relevance+Ranking)|0|
|[A Unified Search and Recommendation Framework Based on Multi-Scenario Learning for Ranking in E-commerce](https://doi.org/10.1145/3626772.3661356)|Jinhan Liu, Qiyu Chen, Junjie Xu, Junjie Li, Baoli Li, Sulong Xu|JD; JD or JD.com|Search and recommendation (S R) are the two most important scenarios in e-commerce. The majority of users typically interact with products in S R scenarios, indicating the need and potential for joint modeling. Traditional multi-scenario models use shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of individual tasks. This coarse-grained modeling approach does not effectively capture the differences between S R scenarios. Furthermore, this approach does not sufficiently exploit the information across the global label space. These issues can result in the suboptimal performance of multi-scenario models in handling both S R scenarios. To address these issues, we propose an effective and universal framework for Unified Search and Recommendation (USR), designed with S R Views User Interest Extractor Layer (IE) and S R Views Feature Generator Layer (FG) to separately generate user interests and scenario-agnostic feature representations for S R. Next, we introduce a Global Label Space Multi-Task Layer (GLMT) that uses global labels as supervised signals of auxiliary tasks and jointly models the main task and auxiliary tasks using conditional probability. Extensive experimental evaluations on real-world industrial datasets show that USR can be applied to various multi-scenario models and significantly improve their performance. Online A/B testing also indicates substantial performance gains across multiple metrics. Currently, USR has been successfully deployed in the 7Fresh App.|搜索和推荐(S R)是电子商务中最重要的两种情况。大多数用户通常在 S R 场景中与产品交互，这表明了联合建模的需要和潜力。传统的多场景模型使用共享参数来学习多个任务的相似性，使用特定任务的参数来学习单个任务的差异性。这种粗粒度建模方法不能有效地捕获 S R 场景之间的差异。此外，这种方法不能充分利用全局标签空间中的信息。这些问题可能导致多场景模型在处理两个 S R 场景时的次优性能。为了解决这些问题，我们提出了一个有效和通用的统一搜索和推荐(USR)框架，该框架使用 S R 视图用户兴趣提取层(IE)和 S R 视图特征生成层(FG)分别生成用户兴趣和场景无关的特征表示。接下来，我们引入了一个全局标签空间多任务层(GLMT) ，它使用全局标签作为辅助任务的监督信号，并使用条件概率联合建模主要任务和辅助任务。对现实世界工业数据集的大量实验评估表明，USR 可以应用于各种多场景模型，并显著提高其性能。在线 A/B 测试还表明跨多个指标的性能显著提高。目前，USR 已经成功地部署在7Fresh 应用程序中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Search+and+Recommendation+Framework+Based+on+Multi-Scenario+Learning+for+Ranking+in+E-commerce)|0|
|[Improving Embedding-Based Retrieval in Friend Recommendation with ANN Query Expansion](https://doi.org/10.1145/3626772.3661367)|Pau PerngHwa Kung, Zihao Fan, Tong Zhao, Yozen Liu, Zhixin Lai, Jiahui Shi, Yan Wu, Jun Yu, Neil Shah, Ganesh Venkataraman|Snap|Embedding-based retrieval in graph-based recommendation has shown great improvements over traditional graph walk retrieval methods, and has been adopted in large-scale industry applications such as friend recommendations [16]. However, it is not without its challenges: retraining graph embeddings frequently due to changing data is slow and costly, and producing high recall of approximate nearest neighbor search (ANN) on such embeddings is challenging due to the power law distribution of the indexed users. In this work, we address theses issues by introducing a simple query expansion method in ANN, called FriendSeedSelection, where for each node query, we construct a set of 1-hop embeddings and run ANN search. We highlight our approach does not require any model-level tuning, and is inferred from the data at test-time. This design choice effectively enables our recommendation system to adapt to the changing graph distribution without frequent heavy model retraining. We also discuss how we design our system to efficiently construct such queries online to support 10k+ QPS. For friend recommendation, our method shows improvements of recall, and 11% relative friend reciprocated communication metric gains, now serving over 800 million monthly active users at Snapchat.|在基于图的推荐中嵌入式检索已经显示出对传统的图步检索方法的巨大改进，并且已经被大规模的工业应用如好友推荐所采用[16]。然而，这并非没有挑战: 由于数据的变化而频繁地重新训练图嵌入是缓慢和昂贵的，并且由于索引用户的幂律分布，在这种嵌入上产生高召回的近似最近邻搜索(ANN)是具有挑战性的。在这项工作中，我们通过在人工神经网络中引入一个简单的查询扩展方法，称为 FriendSeedSelection，对于每个节点查询，我们构造一组1跳嵌入并运行人工神经网络搜索来解决这些问题。我们强调我们的方法不需要任何模型级别的调优，并且是从测试时的数据中推断出来的。这种设计选择有效地使我们的推荐系统能够适应变化的图形分布，而不需要频繁的重模型再训练。我们还讨论了如何设计我们的系统，以有效地构建这样的查询在线支持10k + QPS。对于朋友推荐，我们的方法显示了回忆的改进，11% 的亲属朋友回馈了通信指标的收益，现在为 Snapchat 超过8亿的月活跃用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Embedding-Based+Retrieval+in+Friend+Recommendation+with+ANN+Query+Expansion)|0|
|[Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search](https://doi.org/10.1145/3626772.3657815)|Hideaki Joko, Shubham Chatterjee, Andrew Ramsay, Arjen P. de Vries, Jeff Dalton, Faegheh Hasibi|University of Edinburgh; University of Glasgow; Radboud University|The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.|会话代理的未来将为用户提供个性化的信息响应。然而，在开发模型方面的一个重大挑战是缺乏跨越多个会议并反映真实世界用户偏好的大规模对话数据集。以前的方法依赖于难以伸缩的绿色向导设置中的专家，特别是对于个性化任务。我们的方法 LAPS 通过使用大型语言模型(LLM)来指导单个人类工作者生成个性化对话来解决这个问题。这种方法已被证明可以加快创作过程，提高质量。LAPS 可以收集大规模、人工编写、多会话和多域会话，包括提取用户首选项。与现有的数据集相比，LAPS 产生的对话和专家创建的对话一样自然和多样化，这与完全合成的方法形成了对比。采集的数据集适合于训练偏好提取和个性化响应生成。我们的研究结果表明，明确使用提取的偏好生成的响应更好地匹配用户的实际偏好，突出了使用提取的偏好的价值超过简单的对话历史。总的来说，LAPS 引入了一种新的方法，利用 LLM 创建真实的个性化会话数据，比以前的方法更有效率和效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doing+Personal+LAPS:+LLM-Augmented+Dialogue+Construction+for+Personalized+Multi-Session+Conversational+Search)|0|
|[Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3626772.3657733)|Alireza Salemi, Hamed Zamani|University of Massachusetts Amherst|This paper introduces uRAG–a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.|本文介绍了一个统一检索引擎的框架 uRAG，它可以为多个下游检索增强生成(RAG)系统提供服务。每个 RAG 系统都为一个独特的目的使用检索结果，例如开放域问题回答、事实验证、实体链接和关系提取。我们引入了一个通用的培训指导方针，它标准化了搜索引擎与下游 RAG 系统之间的通信，这些 RAG 系统参与优化检索模型。这为我们建立一个大规模实验生态系统奠定了基础，该系统包括18个参与培训的 RAG 系统和18个使用 uRAG 作为搜索引擎新用户的未知 RAG 系统。利用这个实验生态系统，我们回答了许多基础研究问题，这些问题提高了我们对开发机器搜索引擎的承诺和挑战的理解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Search+Engine+for+Machines:+Unified+Ranking+for+Multiple+Retrieval-Augmented+Large+Language+Models)|0|
|[Sequential Recommendation with Collaborative Explanation via Mutual Information Maximization](https://doi.org/10.1145/3626772.3657770)|Yi Yu, Kazunari Sugiyama, Adam Jatowt|University of Innsbruck; Kyoto University; Osaka Seikei University|Current research on explaining sequential recommendations lacks reliable benchmarks and quantitative metrics, making it difficult to compare explanation performance between different models. In this work, we propose a new explanation type, namely, collaborative explanation, into sequential recommendation, allowing a unified approach for modeling user actions and assessing the performance of both recommendation and explanation. We accomplish this by framing the problem as a joint sequential prediction task, which takes a sequence of user's past item-explanation pairs and predicts the next item along with its associated explanation. We propose a pipeline that comprises data preparation and a model adaptation framework called Sequential recommendation with Collaborative Explanation (SCE). This framework can be flexibly applied to any sequential recommendation model for this problem. Furthermore, to address the issue of inconsistency between item and explanation representations when learning both sub-tasks, we propose Sequential recommendation with Collaborative Explanation via Mutual Information Maximization (SCEMIM). Our extensive experiments demonstrate that: (i) SCE framework is effective in enabling sequential models to make recommendations and provide accurate explanations. (ii) Importantly, SCEMIM enhances the consistency between recommendations and explanations, leading to further improvements in the performance of both sub-tasks.|目前关于解释顺序推荐的研究缺乏可靠的基准和量化指标，因此难以比较不同模型之间的解释性能。在这项工作中，我们提出了一个新的解释类型，即协作解释，到顺序推荐，允许一个统一的方法来建模用户的行为和评估两者的性能的推荐和解释。我们通过将问题框架为一个联合的顺序预测任务来完成这个任务，该任务采用用户过去的项目解释对的序列，并预测下一个项目及其相关的解释。我们提出了一个流水线，包括数据准备和模型适应框架称为顺序推荐与协作解释(SCE)。该框架可以灵活地应用于该问题的任何顺序推荐模型。此外，为了解决两个子任务学习过程中项目表征与解释表征不一致的问题，本文提出了基于互信息最大化的协同解释的序贯推荐方法。我们的大量实验表明: (i) SCE 框架能够有效地使序贯模型提出建议并提供准确的解释。(ii)重要的是，SCEMIM 加强了建议和解释之间的一致性，从而进一步改善了这两个子任务的表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Recommendation+with+Collaborative+Explanation+via+Mutual+Information+Maximization)|0|
|[A Learning-to-Rank Formulation of Clustering-Based Approximate Nearest Neighbor Search](https://doi.org/10.1145/3626772.3657931)|Thomas Vecchiato, Claudio Lucchese, Franco Maria Nardini, Sebastian Bruch|Ca' Foscari University of Venice; ISTI-CNR; Pinecone|A critical piece of the modern information retrieval puzzle is approximate nearest neighbor search. Its objective is to return a set of k data points that are closest to a query point, with its accuracy measured by the proportion of exact nearest neighbors captured in the returned set. One popular approach to this question is clustering: The indexing algorithm partitions data points into non-overlapping subsets and represents each partition by a point such as its centroid. The query processing algorithm first identifies the nearest clusters – a process known as routing – then performs a nearest neighbor search over those clusters only. In this work, we make a simple observation: The routing function solves a ranking problem. Its quality can therefore be assessed with a ranking metric, making the function amenable to learning-to-rank. Interestingly, ground-truth is often freely available: Given a query distribution in a top-k configuration, the ground-truth is the set of clusters that contain the exact top-k vectors. We develop this insight and apply it to Maximum Inner Product Search (MIPS). As we demonstrate empirically on various datasets, learning a simple linear function consistently improves the accuracy of clustering-based MIPS.|现代信息检索难题的一个关键部分是近似最近邻搜索。它的目标是返回一组最接近查询点的 k 个数据点，其精度由返回集中捕获的精确最近邻点的比例来衡量。解决这个问题的一种流行方法是聚类: 索引算法将数据分割成不重叠的子集，并用一个点(如其质心)表示每个分区。查询处理算法首先识别最近的集群——一个称为路由的过程——然后仅对这些集群执行最近邻搜索。在这项工作中，我们做了一个简单的观察: 路由函数解决了一个排序问题。因此，它的质量可以评估与排名度量，使功能适合学习到排名。有趣的是，地面真相通常是免费提供的: 给定 top-k 配置中的查询分布，地面真相是包含精确 top-k 向量的集合。我们开发了这种洞察力，并将其应用于最大内部产品搜索(MIPS)。正如我们在各种数据集上的经验证明，学习一个简单的线性函数可以持续地提高基于聚类的 MIPS 的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Learning-to-Rank+Formulation+of+Clustering-Based+Approximate+Nearest+Neighbor+Search)|0|
|[A Surprisingly Simple yet Effective Multi-Query Rewriting Method for Conversational Passage Retrieval](https://doi.org/10.1145/3626772.3657933)|Ivica Kostric, Krisztian Balog|University of Stavanger & Google Research; University of Stavanger|Conversational passage retrieval is challenging as it often requires the resolution of references to previous utterances and needs to deal with the complexities of natural language, such as coreference and ellipsis. To address these challenges, pre-trained sequence-to-sequence neural query rewriters are commonly used to generate a single de-contextualized query based on conversation history. Previous research shows that combining multiple query rewrites for the same user utterance has a positive effect on retrieval performance. We propose the use of a neural query rewriter to generate multiple queries and show how to integrate those queries in the passage retrieval pipeline efficiently. The main strength of our approach lies in its simplicity: it leverages how the beam search algorithm works and can produce multiple query rewrites at no additional cost. Our contributions further include devising ways to utilize multi-query rewrites in both sparse and dense first-pass retrieval. We demonstrate that applying our approach on top of a standard passage retrieval pipeline delivers state-of-the-art performance without sacrificing efficiency.|会话短文检索是一个具有挑战性的问题，因为它往往需要解决对以前话语的引用，并需要处理自然语言的复杂性，如共引和省略。为了应对这些挑战，预先训练的序列到序列神经查询重写器通常用于生成基于会话历史的单个去上下文化查询。以往的研究表明，对同一用户语句进行多次查询重写对检索性能有积极的影响。我们提出使用神经查询重写器来生成多个查询，并说明如何有效地将这些查询集成到文章检索流水线中。我们方法的主要优点在于它的简单性: 它利用了束搜索算法的工作方式，并且可以在不增加成本的情况下产生多个查询重写。我们的贡献还包括设计在稀疏和密集首通检索中利用多查询重写的方法。我们演示了将我们的方法应用于标准通道检索流水线之上，可以在不牺牲效率的情况下提供最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Surprisingly+Simple+yet+Effective+Multi-Query+Rewriting+Method+for+Conversational+Passage+Retrieval)|0|
|[Memory-Efficient Deep Recommender Systems using Approximate Rotary Compositional Embedding](https://doi.org/10.1145/3626772.3657953)|Dongning Ma, Xun Jiao|Villanova University Electrical and Computer Engineering; Villanova University ECE|Embedding tables in deep recommender systems (DRS) process categorical data, which can be memory-intensive due to the high feature cardinality. In this paper, we propose Approximate Rotary Compositional Embedding (ARCE), which intentionally trades off performance to aggressively reduce the size of the embedding tables. Specifically, ARCE uses compositional embedding to split large embedding tables into smaller compositions and replaces index look-ups with vector rotations. To regain the performance loss of this trade-off, ARCE features an input approximation where one index is mapped into multiple indices, creating a larger space for a potential increased learning capability. Experimental results show that using ARCE can reduce the memory overhead of embedding tables in DRS by more than 1000x with less than 3% performance loss, highlighting the potential of using ARCE for less memory intensive DRS designs. We open-source ARCE at https://github.com/VU-DETAIL/arce.|深度推荐系统(DRS)中嵌入表处理分类数据，由于特征基数高，可能会占用大量内存。在本文中，我们提出了近似旋转组合嵌入(ARCE) ，它有意地牺牲性能以积极地减少嵌入表的大小。具体来说，ARCE 使用组合嵌入将大型嵌入表拆分为较小的组合，并用向量旋转替换索引查找。为了重新获得这种折衷的性能损失，ARCE 采用了一种输入近似，其中一个索引映射到多个索引中，为潜在的增强的学习能力创造了更大的空间。实验结果表明，使用 ARCE 可以将 DRS 中嵌入表的内存开销减少1000倍以上，性能损失小于3% ，突出了使用 ARCE 进行内存密集型 DRS 设计的潜力。我们开源的 ARCE  https://github.com/vu-detail/ARCE。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Memory-Efficient+Deep+Recommender+Systems+using+Approximate+Rotary+Compositional+Embedding)|0|
|[Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking](https://doi.org/10.1145/3626772.3657670)|Sara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, Anton Korikov, Scott Sanner|University of Toronto; University of Waterloo|Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., "I'm watching my weight"). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., "classy joint for a date"). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.|会话推荐系统必须理解用户偏好和意图的丰富多样的自然语言(NL)表达，通常以间接的方式进行沟通(例如，“我在减肥”)。这种复杂的语句使得检索相关项目变得具有挑战性，特别是如果仅仅使用不完整或过时的元数据。幸运的是，许多域名都有丰富的项目评论，涵盖标准的元数据类别，并提供可能符合用户兴趣的复杂意见(例如，“优雅的约会联合”)。然而，直到最近才有了大型语言模型(LLM) ，让我们能够在用户生成的评论中解开用户偏好话语和复杂语言之间的常识性联系。此外，LLM 为半结构化对话状态跟踪、复杂意图和偏好理解以及生成建议、解释和问题答案提供了新的范例。因此，我们引入了一种新技术 RA-Rec，这是一种用于 ConvRec 的恢复增强的 LLM 驱动的对话状态跟踪系统，通过一个视频、开源 GitHub 仓库和交互式 Google Colab 笔记本进行了展示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Conversational+Recommendation+with+Prompt-based+Semi-Structured+Natural+Language+State+Tracking)|0|
|[LLMGR: Large Language Model-based Generative Retrieval in Alipay Search](https://doi.org/10.1145/3626772.3661364)|Chen Wei, Yixin Ji, Zeyuan Chen, Jia Xu, Zhongyi Liu|Ant Group Search Recommendation Technology Department; Soochow University School of Computer Science & Technology; Ant Group|The search system aims to help users quickly find items according to queries they enter, which includes the retrieval and ranking modules. Traditional retrieval is a multi-stage process, including indexing and sorting, which cannot be optimized end-to-end. With the real data about mini-apps in the Alipay search, we find that many complex queries fail to display the relevant mini-apps, seriously threatening users' search experience. To address the challenges, we propose a Large Language Model-based Generative Retrieval (LLMGR) approach for retrieving mini-app candidates. The information of the mini-apps is encoded into the large model, and the title of the mini-app is directly generated. Through the online A/B test in Alipay search, LLMGR as a supplementary source has statistically significant improvements in the Click-Through Rate (CTR) of the search system compared to traditional methods. In this paper, we have deployed a novel retrieval method for the Alipay search system and demonstrated that generative retrieval methods based on LLM can improve the performance of search system, particularly for complex queries, which have an average increase of 0.2% in CTR.|该搜索系统旨在帮助用户根据输入的查询快速查找项目，其中包括检索和排序模块。传统的检索是一个多阶段的过程，包括索引和排序，不能实现端到端的优化。通过对支付宝搜索中迷你应用的真实数据进行分析，我们发现许多复杂的查询都无法显示相关的迷你应用，严重威胁了用户的搜索体验。为了应对这些挑战，我们提出了一种基于大语言模型的生成检索(LLMGR)方法来检索迷你应用程序候选者。迷你应用程序的信息被编码到大模型中，并直接生成迷你应用程序的标题。通过支付宝搜索中的在线 A/B 测试，作为补充来源的 LLMGR 在统计学上显著改善了搜索系统的点进率(ctrr) ，而不是传统方法。本文针对支付宝搜索系统提出了一种新的检索方法，并证明了基于 LLM 的生成式检索方法可以提高搜索系统的性能，尤其是对于平均点击率提高0.2% 的复杂查询。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLMGR:+Large+Language+Model-based+Generative+Retrieval+in+Alipay+Search)|0|
|[Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model](https://doi.org/10.1145/3626772.3661343)|Enqiang Xu, Yiming Qiu, Junyang Bai, Ping Zhang, Dadong Miao, Songlin Wang, Guoyu Tang, Lin Liu, Mingming Li|JD.com|In large e-commerce platforms, search systems are typically composed of a series of modules, including recall, pre-ranking, and ranking phases. The pre-ranking phase, serving as a lightweight module, is crucial for filtering out the bulk of products in advance for the downstream ranking module. Industrial efforts on optimizing the pre-ranking model have predominantly focused on enhancing ranking consistency, model structure, and generalization towards long-tail items. Beyond these optimizations, meeting the system performance requirements presents a significant challenge. Contrasting with existing industry works, we propose a novel method: a Generalizable and RAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking consistency by introducing multiple binary classification tasks that predict whether a product is within the top-k results as estimated by the ranking model, which facilitates the addition of learning objectives on common point-wise ranking models; 2) Generalizability through contrastive learning of representation for all products by pre-training on a subset of ranking product embeddings; 3) Ease of implementation in feature construction and online deployment. Our extensive experiments demonstrate significant improvements in both offline metrics and online A/B test: a 0.75 increase in CVR.|在大型电子商务平台中，搜索系统通常由一系列模块组成，包括召回、预排序和排序阶段。预排序阶段作为一个轻量级模块，对于提前过滤掉下游排序模块的大部分产品至关重要。优化预排序模型的工业努力主要集中在增强排序一致性、模型结构和对长尾项目的推广。除了这些优化之外，满足系统性能需求也是一个重大的挑战。与现有的行业工作相比，我们提出了一种新的方法: 一个一般化和排名一致的预排名模型(GRACE) ，它实现了: 1)排名一致性通过引入多个二进制分类任务，预测一个产品是否在由排名模型估计的前 k 结果之内，这有助于增加学习目标的共同点明智的排名模型; 2)通过对比学习的表示对所有产品的一个排名产品嵌入子集的预训练的一般化; 3)易于实施的功能构建和在线部署。我们的大量实验表明，在离线指标和在线 A/B 测试方面都有显著改善: CVR 增加了0.75。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+E-commerce+Search:+Toward+a+Generalizable+and+Rank-Consistent+Pre-Ranking+Model)|0|
|[A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search](https://doi.org/10.1145/3626772.3661359)|Huimu Wang, Mingming Li, Dadong Miao, Songlin Wang, Guoyu Tang, Lin Liu, Sulong Xu, Jinghe Hu|JD.com; JD or JD.com|Re-ranking is a process of rearranging ranking list to more effectively meet user demands by accounting for the interrelationships between items. Existing methods predominantly enhance the precision of search results, often at the expense of diversity, leading to outcomes that may not fulfill the varied needs of users. Conversely, methods designed to promote diversity might compromise the precision of the results, failing to satisfy the users' requirements for accuracy. To alleviate the above problems, this paper proposes a Preference-oriented Diversity Model Based on Mutual-information (PODM-MI), which consider both accuracy and diversity in the re-ranking process. Specifically, PODM-MI adopts Multidimensional Gaussian distributions based on variational inference to capture users' diversity preferences with uncertainty. Then we maximize the mutual information between the diversity preferences of the users and the candidate items using the maximum variational inference lower bound to enhance their correlations. Subsequently, we derive a utility matrix based on the correlations, enabling the adaptive ranking of items in line with user preferences and establishing a balance between the aforementioned objectives. Experimental results on real-world online e-commerce systems demonstrate the significant improvements of PODM-MI, and we have successfully deployed PODM-MI on an e-commerce search platform.|重新排序是通过考虑项目之间的相互关系来重新安排排序列表以更有效地满足用户需求的过程。现有的方法主要是提高搜索结果的精确度，往往以牺牲多样性为代价，导致结果可能无法满足用户的不同需求。相反，旨在促进多样性的方法可能会损害结果的精确性，不能满足用户对精确性的要求。针对上述问题，本文提出了一种基于互信息的偏好导向多样性模型(PODM-MI) ，该模型在重排序过程中同时考虑了准确性和多样性。具体来说，PODM-MI 采用基于变分推理的多维高斯分布来捕获具有不确定性的用户多样性偏好。然后利用最大变分推理下界，最大化用户多样性偏好与候选项之间的相互信息，以增强它们之间的相关性。随后，我们推导出一个基于相关性的效用矩阵，使项目的自适应排序符合用户偏好，并建立上述目标之间的平衡。在实际的在线电子商务系统上的实验结果表明，PODM-MI 算法得到了显著的改进，并成功地在电子商务搜索平台上部署了 PODM-MI 算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Preference-oriented+Diversity+Model+Based+on+Mutual-information+in+Re-ranking+for+E-commerce+Search)|0|
|[Query Performance Prediction for Conversational Search and Beyond](https://doi.org/10.1145/3626772.3657658)|Chuan Meng|University of Amsterdam|Query performance prediction (QPP) is a key task in information retrieval (IR) [1]. The QPP task is to estimate the retrieval quality of a search system for a query without human relevance judgments. In summary, I aim to solve 4 limitations identified in previous QPP studies: I have published 3 papers that address 3 of these limitations, while the remaining one is the focus of my future work. While extensively explored for traditional ad-hoc search, QPP for conversational search (CS) [4] has been little studied. I have identified limitation 1 in previous QPP studies: There is a lack of a comprehensive investigation into how well existing QPP methods designed for ad-hoc search perform in the context of CS. To fill this research gap, I have conducted a comprehensive reproducibility study [5], where I examined various QPP methods that were designed for ad-hoc search in the CS setting. I have made the code and data publicly available on https://github.com/ChuanMeng/QPP4CS. Moreover, I have identified limitation 2 in previous studies on QPP for CS: There is a lack of research in investigating and leveraging the CS-specific features that do not exist in ad-hoc search to improve QPP quality for CS. I have authored a paper to fill this research gap [3]. Specifically, my empirical analysis indicates a correlation between query rewriting quality in CS and the actual retrieval quality. Based on this finding, I have proposed a <u>p</u>er<u>pl</u>exity-based pre-retrieval QPP framework (PPL-QPP) for CS, which integrates query rewriting quality into existing QPP methods. Experimental results show that PPL-QPP improves QPP quality. Beyond the scope of QPP for CS, I have identified drawbacks in general QPP methods. Existing QPP methods typically return a single scalar value that indicates the retrieval quality, which results in two issues: (i) relying on a single value to represent different IR metrics leads to a "one size fits all" issue, and (ii) a single value constraints the interpretability of QPP. Thus, I have identified limitation 3: there is a shortage of QPP methods that are capable of effectively predicting various IR evaluation metrics while maintaining interpretability. To address the limitation, I have proposed a QPP framework using automatically <u>gen</u>erated <u>re</u>evance judgments (QPP-GenRE); it decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query [6]. QPP-GenRE enables the prediction of any IR metric using generated relevance judgments as pseudo-labels, and enables the interpretation of predicted IR metrics based on generated judgments. I have fine-tuned an open-source large language model (LLM) for judging relevance. Experimental results show that QPP-GenRE achieves state-of-the-art QPP quality; my fine-tuned LLM demonstrates a high relevance judgment agreement with human assessors. I have made the code and data publicly available on https://github.com/ChuanMeng/QPP-GenRE. As part of my future work, I plan to solve limitation 4: No study has explored the application of QPP in retrieval-augmented generation (RAG) to predict when not to rely on low-quality retrieved items that have the potential to hurt RAG's text generation.|查询性能预测(QPP)是信息检索(IR)[1]中的一项关键任务。QPP 任务是在没有人类相关性判断的情况下，对查询搜索系统的检索质量进行评估。总之，我的目标是解决在以前的 QPP 研究中发现的4个局限性: 我已经发表了3篇论文，解决了其中的3个局限性，而其余的一个是我未来工作的重点。虽然对传统的自组织搜索进行了广泛的研究，但是对会话搜索的 QPP 研究却很少。我已经在以前的 QPP 研究中确定了局限性1: 缺乏一个全面的调查，以了解现有的 QPP 方法设计的特别搜索在 CS 的情况下表现如何。为了填补这个研究空白，我进行了一个全面的重复性研究[5] ，其中我检查了各种 QPP 方法，这些方法是为在 CS 设置中的特别搜索而设计的。我已经把代码和数据公布在 https://github.com/chuanmeng/qpp4cs 上了。此外，我已经在以前的 CS QPP 研究中确定了局限性2: 缺乏研究调查和利用 CS 特定的特征，这些特征在特别搜索中不存在，以提高 CS 的 QPP 质量。我已经写了一篇论文来填补这个研究空白[3]。具体来说，本文的实证分析表明了 CS 中查询重写质量与实际检索质量之间的相关性。基于这一发现，我提出了一个基于实例的检索前 QPP 框架(PPL-QPP) ，该框架将查询重写质量与现有的 QPP 方法相结合。实验结果表明，PPL-QPP 提高了 QPP 的质量。除了 CS 的 QPP 范围，我已经确定了一般 QPP 方法的缺点。现有的 QPP 方法通常返回指示检索质量的单个标量值，这导致两个问题: (i)依赖于单个值来表示不同的 IR 指标导致“一种尺寸适合所有”问题，以及(ii)单个值限制了 QPP 的可解释性。因此，我已经确定了局限性3: 缺乏能够有效预测各种 IR 评估指标同时保持可解释性的 QPP 方法。为了解决这个局限性，我提出了一个 QPP 框架，它使用了自动的 < u > gen </u > ated < u > re </u > 事件判断(QPP-GenRE) ; 它将 QPP 分解为独立的子任务，判断排序列表中的每个项目与给定查询的相关性[6]。QPP-GenRE 能够使用生成的相关性判断作为伪标签来预测任何 IR 度量，并且能够基于生成的判断来解释预测的 IR 度量。我已经微调了一个用于判断相关性的开源大型语言模型(LLM)。实验结果表明，QPP-GenRE 实现了最先进的 QPP 质量，我的微调 LLM 与人类评估者的相关性判断一致性很高。我已经把代码和数据公布在 https://github.com/chuanmeng/qpp-genre 上了。作为我未来工作的一部分，我计划解决局限性4: 还没有研究探索 QPP 在检索增强生成(RAG)中的应用，以预测何时不依赖于有可能损害 RAG 文本生成的低质量检索项。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Performance+Prediction+for+Conversational+Search+and+Beyond)|0|
|[Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset](https://doi.org/10.1145/3626772.3657892)|Philipp Hager, Romain Deffayet, JeanMichel Renders, Onno Zoeter, Maarten de Rijke|Naver Labs Europe; University of Amsterdam; Booking.com|Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments reveal that ULTR robustly improves click prediction. However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark.|无偏学习排名(ULTR)是一个从用户点击中学习的成熟的框架，用户点击往往受到排名收集数据的影响。ULTR 技术虽然在理论上得到了验证，并在仿真中得到了广泛的测试，但缺乏经验验证，特别是在现代搜索引擎上。从百度搜索引擎收集的2023年 WSDM 杯的数据集提供了一个难得的机会来评估突出的 ULTR 技术在现实世界中的表现。尽管在2023年 WSDM 杯和随后的 NTCIR ULTRE-2任务期间提交了多份申请，但目前尚不清楚观察到的改善是否源于应用 ULTR 或其他学习技术。我们重新审视并扩展现有的实验。我们发现，无偏见的学习排序技术并不能带来明显的性能改善，尤其是与排序丢失和查询文档特性的选择所带来的明显差异相比。我们的实验表明，ULTR 强有力地改善点击预测。然而，在点击预测方面取得的这些进展并不能转化为专家相关性注释排名表现的提高，这意味着结论在很大程度上取决于如何在这一基准中衡量成功。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Learning+to+Rank+Meets+Reality:+Lessons+from+Baidu's+Large-Scale+Search+Dataset)|0|
|[CMCLRec: Cross-modal Contrastive Learning for User Cold-start Sequential Recommendation](https://doi.org/10.1145/3626772.3657839)|Xiaolong Xu, Hongsheng Dong, Lianyong Qi, Xuyun Zhang, Haolong Xiang, Xiaoyu Xia, Yanwei Xu, Wanchun Dou|Nanjing University; RMIT University; Macquarle Unnversity; China University of Petroleum; Nanjing University of Information Science and Technology; College of Intelligence and Computing, Tianjin University; Macquarie University|Sequential recommendation models generate embeddings for items through the analysis of historical user-item interactions and utilize the acquired embeddings to predict user preferences. Despite being effective in revealing personalized preferences for users, these models heavily rely on user-item interactions. However, due to the lack of interaction information, new users face challenges when utilizing sequential recommendation models for predictions, which is recognized as the cold-start problem. Recent studies, while addressing this problem within specific structures, often neglect the compatibility with existing sequential recommendation models, making seamless integration into existing models unfeasible.To address this challenge, we propose CMCLRec, a Cross-Modal Contrastive Learning framework for user cold-start RECommendation. This approach aims to solve the user cold-start problem by customizing inputs for cold-start users that align with the requirements of sequential recommendation models in a cross-modal manner. Specifically, CMCLRec adopts cross-modal contrastive learning to construct a mapping from user features to user-item interactions based on warm user data. It then generates a simulated behavior sequence for each cold-start user in turn for recommendation purposes. In this way, CMCLRec is theoretically compatible with any extant sequential recommendation model. Comprehensive experiments conducted on real-world datasets substantiate that, compared with state-of-the-art baseline models, CMCLRec markedly enhances the performance of conventional sequential recommendation models, particularly for cold-start users.|序贯推荐模型通过分析历史上的用户-项目交互，生成项目的嵌入，并利用获得的嵌入来预测用户偏好。尽管这些模型能够有效地向用户展示个性化偏好，但它们严重依赖于用户项目交互。然而，由于缺乏交互信息，新用户在使用顺序推荐模型进行预测时面临着挑战，这被认为是冷启动问题。最近的研究虽然在特定的结构内解决了这个问题，但往往忽视了与现有顺序推荐模型的兼容性，使得与现有模型的无缝集成变得不可行。为了应对这一挑战，我们提出了 CMCLRec，一个用于用户冷启动推荐的跨模态对比学习框架。这种方法旨在解决用户冷启动问题，为冷启动用户定制输入，以跨模式的方式符合顺序推荐模型的要求。具体来说，CMCLRec 采用跨模态对比学习方法，构建了基于暖用户数据的用户特征到用户项交互的映射关系。然后，它为每个冷启动用户依次生成一个模拟的行为序列，用于推荐目的。这样，CMCLRec 在理论上与任何现存的顺序推荐模型兼容。在真实世界数据集上进行的综合实验证实，与最先进的基线模型相比，CMCLRec 显著提高了传统顺序推荐模型的性能，特别是对于冷启动用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CMCLRec:+Cross-modal+Contrastive+Learning+for+User+Cold-start+Sequential+Recommendation)|0|
|[Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention](https://doi.org/10.1145/3626772.3657829)|Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai, Xiangyu Zhao, Kesen Zhao, Lantao Hu, Peng Jiang, Kun Gai|Kuaishou Technology Strategy Algorithm Department; Unaffiliated; Kuaishou Technology; City University of Hong Kong School of Data Science; City University of Hong Kong|In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication|在推荐系统(RS)应用领域，强化学习(rL)最近已经成为一种强大的工具，这主要是由于它在优化长期回报方面的熟练程度。尽管如此，由于自学、非政策培训和函数逼近之间错综复杂的相互作用，它在学习过程中存在不稳定性。此外，在多奖励推荐场景中，设计一个适当的奖励设置来协调各种任务的内部动态是相当复杂的。为了应对这些挑战，我们引入了 DT4IER，这是一种基于决策转换器的高级推荐模型，不仅旨在提高推荐的有效性，而且还旨在实现直接用户参与和长期保留之间的和谐平衡。DT4IER 采用了一种创新的多奖励设计，能够巧妙地平衡短期和长期奖励与用户特定属性之间的关系，这有助于增强奖励序列的上下文丰富性，确保推荐过程更加知情和个性化。为了增强其预测能力，DT4IER 采用了高维编码器，巧妙地设计识别和利用不同任务之间错综复杂的相互关系。此外，我们在嵌入预测的动作中整合了一种对比学习方法，这种策略显著地提高了模型的整体性能。在三个实际数据集上的实验证明了 DT4IER 对最先进的顺序推荐系统(SRS)和多任务学习(MTL)模型在特定任务的预测准确性和有效性方面的有效性。可以联机访问源代码，以便于复制|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Recommendation+for+Optimizing+Both+Immediate+Feedback+and+Long-term+Retention)|0|
|[Invisible Relevance Bias: Text-Image Retrieval Models Prefer AI-Generated Images](https://doi.org/10.1145/3626772.3657750)|Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, Xueqi Cheng|Gaoling School of Artificial Intelligence, Renmin University of China; Institute of Computing Technology, Chinese Academy of Sciences|With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures. Furthermore, our subsequent exploration reveals that the inclusion of AI-generated images in the training data of the retrieval models exacerbates the invisible relevance bias. The above phenomenon triggers a vicious cycle, which makes the invisible relevance bias become more and more serious. To elucidate the potential causes of invisible relevance and address the aforementioned issues, we introduce an effective training method aimed at alleviating the invisible relevance bias. Subsequently, we apply our proposed debiasing method to retroactively identify the causes of invisible relevance, revealing that the AI-generated images induce the image encoder to embed additional information into their representation. This information exhibits a certain consistency across generated images with different semantics and can make the retriever estimate a higher relevance score.|随着生成模型的进步，人工智能生成的内容(AIGC)正变得越来越现实，充斥着互联网。最近的一项研究表明，这种现象造成源偏见的文本检索的网络搜索。具体来说，神经检索模型对生成文本的排名往往高于人写文本。在本文中，我们将这种偏差的研究扩展到跨模态检索。首先，我们成功地构建了一个合适的基准来研究这种偏差的存在。随后在这个基准上进行的大量实验表明，人工智能生成的图像给文本图像检索模型带来了不可见的相关性偏差。具体来说，我们的实验表明，文本图像检索模型对人工智能生成的图像的排序往往高于真实图像，即使人工智能生成的图像并没有表现出更多的视觉相关特征的查询比真实图像。这种看不见的相关性偏差在具有不同训练数据和结构的检索模型中普遍存在。此外，我们随后的研究表明，在检索模型的训练数据中包含人工智能生成的图像加剧了不可见的相关性偏差。上述现象引发了一个恶性循环，使得无形的关联偏差越来越严重。为了阐明隐性相关产生的潜在原因并解决上述问题，我们引入了一种有效的训练方法来缓解隐性相关偏差。随后，我们应用我们提出的去偏方法来追溯识别不可见相关性的原因，揭示了人工智能生成的图像诱导图像编码器嵌入额外的信息到他们的表示。这些信息在生成的具有不同语义的图像之间表现出一定的一致性，并且可以使检索器估计出更高的相关性得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Invisible+Relevance+Bias:+Text-Image+Retrieval+Models+Prefer+AI-Generated+Images)|0|
|[Fair Sequential Recommendation without User Demographics](https://doi.org/10.1145/3626772.3657703)|Huimin Zeng, Zhankui He, Zhenrui Yue, Julian J. McAuley, Dong Wang|University of California, San Diego; University of Illinois Urbana-Champaign; University of Illinois at Urbana-Champaign|Much existing literature on fair recommendation (i.e., group fairness) leverages users' demographic attributes (e.g., gender) to develop fair recommendation methods. However, in real-world scenarios, due to privacy concerns and convenience considerations, users may not be willing to share their demographic information with the system, which limits the application of many existing methods. Moreover, sequential recommendation (SR) models achieve state-of-the-art performance compared to traditional collaborative filtering (CF) recommenders, and can represent users solely using user-item interactions (user-free). This leaves a wrong impression that SR models are free from group unfairness by design. In this work, we explore a critical question: how can we build a fair sequential recommendation system without even knowing user demographics? To address this problem, we propose Agnostic FairSeqRec (A-FSR): a model-agnostic and demographic-agnostic debiasing framework for sequential recommendation without requiring users' demographic attributes. Firstly, A-FSR reduces the correlation between the potential stereotypical patterns in the input sequences and final recommendations via Dirichlet neighbor smoothing. Secondly, A-FSR estimates an under-represented group of sequences via a gradient-based heuristic, and implicitly moves training focus towards the under-represented group by minimizing a distributionally robust optimization (DRO) based objective. Results on real-world datasets show that A-FSR achieves significant improvements on group fairness in sequential recommendation, while outperforming other state-of-the-art baselines.|关于公平推荐(即群体公平)的许多现有文献利用用户的人口统计特征(如性别)来发展公平推荐方法。然而，在现实世界的情况下，由于隐私问题和方便的考虑，用户可能不愿意与系统共享他们的人口统计信息，这限制了许多现有方法的应用。此外，序贯推荐(SR)模型与传统的协同过滤推荐(CF)模型相比，可以实现最先进的性能，并且可以完全使用用户项交互(用户自由)来代表用户。这就给人留下了一个错误的印象，认为 SR 模型在设计上不存在群体不公平。在这项工作中，我们探讨了一个关键问题: 我们如何建立一个公平的顺序推荐系统，甚至不知道用户的人口统计？为了解决这个问题，我们提出了不可知的 FairSeqRec (A-FSR) : 一个不需要用户人口统计属性的模型不可知和人口统计不可知的连续推荐消偏框架。首先，A-FSR 通过 Dirichlet 邻域平滑降低了输入序列中潜在的常规模式与最终推荐值之间的相关性。其次，A-FSR 通过基于梯度的启发式算法估计一组未被充分表示的序列，并通过最小化基于分布鲁棒优化(DRO)的目标隐式地将训练焦点移向未被充分表示的序列。实际数据集的结果表明，A-FSR 在顺序推荐方面取得了显著的改善，同时优于其他最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Sequential+Recommendation+without+User+Demographics)|0|
|[Negative Sampling Techniques for Dense Passage Retrieval in a Multilingual Setting](https://doi.org/10.1145/3626772.3657854)|Thilina Chaturanga Rajapakse, Andrew Yates, Maarten de Rijke|University of Amsterdam|The bi-encoder transformer architecture has become popular in open-domain retrieval, surpassing traditional sparse retrieval methods. Using hard negatives during training can improve the effectiveness of dense retrievers, and various techniques have been proposed to generate these hard negatives. We investigate the effectiveness of multiple negative sampling methods based on lexical methods (BM25), clustering, and periodically updated dense indices. We examine techniques that were introduced for finding hard negatives in a monolingual setting and reproduce them in a multilingual setting. We discover a gap amongst these techniques that we fill by proposing a novel clustered training method. Specifically, we focus on monolingual retrieval using multilingual dense retrievers across a broad set of diverse languages. We find that negative sampling based on BM25 negatives is surprisingly effective in an in-distribution setting, but this finding does not generalize to out-of-distribution and zero-shot settings, where the newly proposed method achieves the best results. We conclude with recommendations on which negative sampling methods may be the most effective given different multilingual retrieval scenarios.|双编码器变压器结构已经成为开放域检索中的热点，超越了传统的稀疏检索方法。在训练过程中使用硬负片可以提高密集型检索器的效率，人们提出了各种技术来产生这些硬负片。我们研究了基于词汇方法(BM25)、聚类和周期性更新密集指数的多重负抽样方法的有效性。我们研究了在单语环境下寻找硬负面的技术，并在多语环境下重现这些技术。我们通过提出一种新的聚类训练方法来填补这些技术之间的空白。具体来说，我们的重点是使用多语言密集检索器跨多种语言的单语言检索。我们发现基于 BM25负值的负采样在分布内环境中有惊人的效果，但是这一发现并没有推广到分布外环境和零拍环境中，在这两种环境中，新提出的方法取得了最好的效果。最后，我们给出了在不同的多语言检索场景下，哪种负抽样方法可能是最有效的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Negative+Sampling+Techniques+for+Dense+Passage+Retrieval+in+a+Multilingual+Setting)|0|
|[M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework](https://doi.org/10.1145/3626772.3657686)|Zijian Zhang, Shuchang Liu, Jiaao Yu, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Ziru Liu, Qidong Liu, Hongwei Zhao, Lantao Hu, Peng Jiang, Kun Gai||Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling. Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods. To this end, we introduce M3oE, an adaptive multi-domain multi-task mixture-of-experts recommendation framework. M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives. We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner. Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks. The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization. To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively. Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance. The implementation code is available to ensure reproducibility.|多领域推荐和多任务推荐在利用来自不同领域和目标的公共信息进行全面的用户建模方面展示了它们的有效性。尽管如此，实际的推荐通常同时面对多个领域和任务，而这些领域和任务不能被当前的方法很好地处理。为此，我们介绍了一个自适应的多领域多任务混合专家推荐框架 M3oE。M3oE 集成了多领域信息，映射了跨领域和任务的知识，并优化了多个目标。我们利用三个专家混合模块分别学习通用、领域方面和任务方面的用户偏好，以解决多个领域和任务之间的复杂依赖关系。此外，我们设计了一个两级融合机制，用于精确控制不同领域和任务的特征提取和融合。通过应用 AutoML 技术，进一步提高了框架的适应性，实现了动态结构优化。据作者所知，我们的 M3oE 首次尝试自适应地解决多领域多任务推荐问题。针对不同基线的两个基准数据集的大量实验证明了 M3oE 的优越性能。实现代码可用于确保可重复性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M3oE:+Multi-Domain+Multi-Task+Mixture-of+Experts+Recommendation+Framework)|0|
|[NFARec: A Negative Feedback-Aware Recommender Model](https://doi.org/10.1145/3626772.3657809)|Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Dongjin Yu|School of Computer Science and Technology, Hangzhou Dianzi University; Faculty of Engineering, Integrated Graduate School of Medicine, Engineering, and Agricultural Sciences; Graduate Faculty of Interdisciplinary Research, University of Yamanashi|Graph neural network (GNN)-based models have been extensively studied for recommendations, as they can extract high-order collaborative signals accurately which is required for high-quality recommender systems. However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in GNNs, and (2) even when an item vastly deviates from users' preferences, they might still choose it and provide a negative rating. In this paper, we propose a negative feedback-aware recommender model (NFARec) that maximizes the leverage of negative feedback. To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph convolutions (HGCs) to learn users' structural representations. Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the Transformer Hawkes Process. The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions. Extensive experiments demonstrate that NFARec outperforms competitive baselines. Our source code and data are released at https://github.com/WangXFng/NFARec.|基于图形神经网络(GNN)的推荐系统模型能够准确地提取高阶协同信号，是高质量推荐系统所必需的。然而，他们忽视了通过负面反馈获得的有价值的信息在两个方面: (1)不同的用户可能对同一个项目持有相反的反馈，这阻碍了最佳信息在 GNN 中的传播，和(2)即使一个项目大大偏离用户的喜好，他们仍然可能选择它，并提供一个负面评价。在本文中，我们提出了一个负反馈感知的推荐模型(NFARec) ，最大限度地利用负反馈。NFARec 采用反馈感知关联算法，引导超图卷积(HGC)学习用户的结构表示，有效地将信息沿着最优路径传递给多跳邻居。此外，NFARec 还包含了一个辅助任务——预测下一次交互的反馈情绪极性(即正极或负极)——基于变压器霍克斯过程。这项任务有利于了解用户的情绪表达在他们以前的顺序反馈模式和预测未来的交互。大量的实验表明，NFARec 的表现优于竞争基线。我们的源代码和数据在 https://github.com/wangxfng/nfarec 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NFARec:+A+Negative+Feedback-Aware+Recommender+Model)|0|
|[Modeling User Fatigue for Sequential Recommendation](https://doi.org/10.1145/3626772.3657802)|Nian Li, Xin Ban, Cheng Ling, Chen Gao, Lantao Hu, Peng Jiang, Kun Gai, Yong Li, Qingmin Liao|Kuaishou Inc.; Department of Electronic Engineering, Tsinghua University; Independent; Shenzhen International Graduate School, Tsinghua University; Tsinghua University|Recommender systems filter out information that meets user interests. However, users may be tired of the recommendations that are too similar to the content they have been exposed to in a short historical period, which is the so-called user fatigue. Despite the significance for a better user experience, user fatigue is seldom explored by existing recommenders. In fact, there are three main challenges to be addressed for modeling user fatigue, including what features support it, how it influences user interests, and how its explicit signals are obtained. In this paper, we propose to model user Fatigue in interest learning for sequential Recommendations (FRec). To address the first challenge, based on a multi-interest framework, we connect the target item with historical items and construct an interest-aware similarity matrix as features to support fatigue modeling. Regarding the second challenge, built upon feature cross, we propose a fatigue-enhanced multi-interest fusion to capture long-term interest. In addition, we develop a fatigue-gated recurrent unit for short-term interest learning, with temporal fatigue representations as important inputs for constructing update and reset gates. For the last challenge, we propose a novel sequence augmentation to obtain explicit fatigue signals for contrastive learning. We conduct extensive experiments on real-world datasets, including two public datasets and one large-scale industrial dataset. Experimental results show that FRec can improve AUC and GAUC up to 0.026 and 0.019 compared with state-of-the-art models, respectively. Moreover, large-scale online experiments demonstrate the effectiveness of FRec for fatigue reduction. Our codes are released at https://github.com/tsinghua-fib-lab/SIGIR24-FRec.|推荐系统过滤出符合用户兴趣的信息。然而，用户可能会厌倦那些与他们在很短的历史时期内接触到的内容过于相似的推荐，这就是所谓的用户疲劳。尽管这对于更好的用户体验意义重大，但是现有的推荐者很少探讨用户疲劳问题。实际上，建立用户疲劳模型需要解决三个主要问题，包括哪些特性支持用户疲劳，它如何影响用户兴趣，以及如何获得用户疲劳的显性信号。在本文中，我们提出了模型用户疲劳的兴趣学习顺序推荐(FRec)。为了解决第一个问题，我们基于一个多兴趣框架，将目标项目与历史项目连接起来，构造一个感兴趣的相似矩阵作为特征来支持疲劳建模。针对第二个挑战，建立在特征交叉的基础上，我们提出了一种疲劳增强的多兴趣融合来捕获长期兴趣。此外，我们开发了一个用于短期兴趣学习的疲劳门控循环单元，以时间疲劳表示作为构造更新门和复位门的重要输入。针对最后一个挑战，我们提出了一种新的序列增强方法，用于获得用于对比学习的显式疲劳信号。我们对真实世界的数据集进行了广泛的实验，包括两个公共数据集和一个大规模的工业数据集。实验结果表明，与现有模型相比，FRec 可以提高 AUC 和 GAUC，分别达到0.026和0.019。此外，大规模的在线实验证明了 FRec 对疲劳减振的有效性。我们的密码在 https://github.com/tsinghua-fib-lab/sigir24-frec 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Fatigue+for+Sequential+Recommendation)|0|
|[DDPO: Direct Dual Propensity Optimization for Post-Click Conversion Rate Estimation](https://doi.org/10.1145/3626772.3657817)|Hongzu Su, Lichao Meng, Lei Zhu, Ke Lu, Jingjing Li|Tongji University; University of Electronic Science and Technology of China|In online advertising, the sample selection bias problem is a major cause of inaccurate conversion rate estimates. Current mainstream solutions only perform causality-based optimization in the click space since the conversion labels in the non-click space are absent. However, optimization for unclicked samples is equally essential because the non-click space contains more samples and user characteristics than the click space. To exploit the unclicked samples, we propose a Direct Dual Propensity Optimization (DDPO) framework to optimize the model directly in impression space with both clicked and unclicked samples. In this framework, we specifically design a click propensity network and a conversion propensity network. The click propensity network is dedicated to ensuring that optimization in the click space is unbiased. The conversion propensity network is designed to generate pseudo-conversion labels for unclicked samples, thus overcoming the challenge of absent labels in non-click space. With these two propensity networks, we are able to perform causality-based optimization in both click space and non-click space. In addition, to strengthen the causal relationship, we design two causal transfer modules for the conversion rate prediction model with the attention mechanism. The proposed framework is evaluated on five real-world public datasets and one private Tencent advertising dataset. Experimental results verify that our method is able to improve the prediction performance significantly. For instance, our method outperforms the previous state-of-the-art method by 7.0% in terms of the Area Under the Curve on the Ali-CCP dataset.|在网络广告中，样本选择偏差问题是导致转化率估计不准确的主要原因。当前的主流解决方案只在点击空间中执行基于因果关系的优化，因为非点击空间中没有转换标签。然而，对未点击样本的优化同样重要，因为非点击空间比点击空间包含更多的样本和用户特征。为了利用未点击样本，我们提出了一个直接双倾向优化(DDPO)框架，直接在印象空间中对点击样本和未点击样本进行优化。在这个框架中，我们具体设计了一个点击倾向网络和一个转换倾向网络。点击倾向网络致力于确保点击空间的优化是无偏的。转换倾向网络的设计目的是为未点击样本生成伪转换标签，从而克服非点击空间中标签缺失的困难。有了这两个倾向网络，我们就能够在点击空间和非点击空间进行基于因果关系的优化。此外，为了加强因果关系，我们设计了两个具有注意机制的因果传递模块用于转化率预测模型。建议的框架是根据五个真实世界的公共数据集和一个私人腾讯广告数据集进行评估的。实验结果表明，该方法能够显著提高预测性能。例如，在 Ali-CCP 数据集的曲线下面积方面，我们的方法比以前最先进的方法高出7.0% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DDPO:+Direct+Dual+Propensity+Optimization+for+Post-Click+Conversion+Rate+Estimation)|0|
|[A Generic Behavior-Aware Data Augmentation Framework for Sequential Recommendation](https://doi.org/10.1145/3626772.3657682)|Jing Xiao, Weike Pan, Zhong Ming|Shenzhen University|Multi-behavior sequential recommendation (MBSR), which models multi-behavior sequentiality and heterogeneity to better learn users' multifaceted intentions has achieved remarkable success. Though effective, the performance of these approaches may be limited due to the sparsity inherent in a real-world data. Existing data augmentation methods in recommender systems focus solely on a single type of behavior, overlooking the variations in expressing user preferences via different types of behaviors. During the augmentation of samples, it is easy to introduce excessive disturbance or noise, which may mislead the next-item recommendation. To address this limitation, we propose a novel generic framework called multi-behavior data augmentation for sequential recommendation (MBASR). Specifically, we design three behavior-aware data augmentation operations to construct rich training samples. Each augmentation operation takes into account the correlations between behaviors and aligns with the users' behavior patterns. In addition, we introduce a position-based sampling strategy that can effectively reduce the perturbation brought by the augmentation operations to the original data. Note that our model is data-oriented and can thus be embedded in different downstream MBSR models, so the overall framework is generic. Extensive experiments on three real-world datasets demonstrate the effectiveness of our MBASR and its applicability to a wide variety of mainstream MBSR models. Our source code is available at https://github.com/XiaoJing-C/MBASR.|多行为顺序推荐(MBRR)模型对多行为顺序性和异构性进行建模，以更好地了解用户的多方面意图，已取得了显著的成功。尽管这些方法有效，但由于真实世界数据中固有的稀疏性，它们的性能可能会受到限制。推荐系统中现有的数据增强方法只关注单一类型的行为，忽略了通过不同类型的行为表达用户偏好的差异。在样本的增大过程中，容易引入过多的干扰或噪声，从而误导下一项的推荐。为了解决这个问题，我们提出了一种新的通用框架，称为序贯推荐的多行为数据增强(MBASR)。具体来说，我们设计了三个行为感知的数据增强操作来构造丰富的训练样本。每个增强操作都考虑到行为之间的相关性，并与用户的行为模式保持一致。此外，我们还引入了一种基于位置的采样策略，可以有效地减少增广操作对原始数据的干扰。注意，我们的模型是面向数据的，因此可以嵌入到不同的下游 MBSR 模型中，所以总体框架是通用的。在三个实际数据集上的大量实验证明了我们的 MBASR 的有效性及其对各种主流 MBSR 模型的适用性。我们的源代码可以在 https://github.com/xiaojing-c/mbasr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Generic+Behavior-Aware+Data+Augmentation+Framework+for+Sequential+Recommendation)|0|
|[FineRec: Exploring Fine-grained Sequential Recommendation](https://doi.org/10.1145/3626772.3657761)|Xiaokun Zhang, Bo Xu, Youlin Wu, Yuan Zhong, Hongfei Lin, Fenglong Ma|Dalian University of Technology; Pennsylvania State University|Sequential recommendation is dedicated to offering items of interest for users based on their history behaviors. The attribute-opinion pairs, expressed by users in their reviews for items, provide the potentials to capture user preferences and item characteristics at a fine-grained level. To this end, we propose a novel framework FineRec that explores the attribute-opinion pairs of reviews to finely handle sequential recommendation. Specifically, we utilize a large language model to extract attribute-opinion pairs from reviews. For each attribute, a unique attribute-specific user-opinion-item graph is created, where corresponding opinions serve as the edges linking heterogeneous user and item nodes. Afterwards, we devise a diversity-aware convolution operation to aggregate information within the graphs, enabling attribute-specific user and item representation learning. Ultimately, we present an interaction-driven fusion mechanism to integrate attribute-specific user/item representations across all attributes for generating recommendations. Extensive experiments conducted on several real-world datasets demonstrate the superiority of our FineRec over existing state-ofthe-art methods. Further analysis also verifies the effectiveness of our fine-grained manner in handling the task.|序列推荐致力于根据用户的历史行为为他们提供感兴趣的项目。由用户在项目评论中表达的属性-意见对提供了在细粒度水平上捕获用户偏好和项目特征的潜力。为此，我们提出了一个新的框架 FineRec，探索评论的属性-意见对，以精细处理顺序推荐。具体来说，我们利用一个大型的语言模型来从评论中提取属性-意见对。对于每个属性，创建一个惟一的特定于属性的用户意见项图，其中相应的意见作为连接异构用户和项目节点的边。然后，我们设计一个多样性感知的卷积运算来聚集图中的信息，使特定属性的用户和项目表示学习。最后，我们提出了一种交互驱动的融合机制，用于跨所有属性集成特定于属性的用户/项表示，以生成建议。在几个真实世界数据集上进行的大量实验证明了我们的 FineRec 相对于现有最先进的方法的优越性。进一步的分析还验证了我们处理任务的细粒度方式的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FineRec:+Exploring+Fine-grained+Sequential+Recommendation)|0|
|[ReFer: Retrieval-Enhanced Vertical Federated Recommendation for Full Set User Benefit](https://doi.org/10.1145/3626772.3657763)|Wenjie Li, Zhongren Wang, Jinpeng Wang, Shutao Xia, Jile Zhu, Mingjian Chen, Jiangke Fan, Jia Cheng, Jun Lei|Tsinghua University; Meituan|As an emerging privacy-preserving approach to leveraging cross-platform user interactions, vertical federated learning (VFL) has been increasingly applied in recommender systems. However, vanilla VFL is only applicable to overlapped users, ignoring potential universal interest patterns hidden among non-overlapped users and suffers from limited user group benefits, which hinders its application in real-world recommenders. In this paper, we extend the traditional vertical federated recommendation problem (VFR) to a more realistic Fully-Vertical federated recommendation setting (Fully-VFR) which aims to utilize all available data and serve full user groups. To tackle challenges in implementing Fully-VFR, we propose a Retrieval-enhanced Vertical Federated recommender (ReFer), a groundbreaking initiative that explores retrieval-enhanced machine learning approaches in VFL. Specifically, we establish a general "retrieval-and-utilization" algorithm to enhance the quality of representations across all parties. We design a flexible federated retrieval augmentation (RA) mechanism for VFL: (i) Cross-RA to complement field missing and (ii) Local-RA to promote mutual understanding between user groups. We conduct extensive experiments on both public and industry datasets. Results on both sequential and non-sequential CTR prediction tasks demonstrate that our method achieves significant performance improvements over baselines and is beneficial for all user groups.|作为一种新兴的利用跨平台用户交互的隐私保护方法，垂直联邦学习(VFL)在推荐系统中得到了越来越多的应用。然而，普通的 VFL 只适用于重叠用户，忽略了隐藏在非重叠用户之间的潜在通用兴趣模式，并且受到用户组好处的限制，这阻碍了它在实际推荐中的应用。本文将传统的垂直联邦推荐问题(VFR)扩展到一个更加现实的全垂直联邦推荐设置(Full-VFR) ，其目的是利用所有可用的数据，为全用户组提供服务。为了解决在实施完全 VFR 的挑战，我们提出了一个检索增强垂直联邦推荐(参考) ，一个突破性的倡议，探索检索增强机器学习方法在 VFL。具体来说，我们建立了一个通用的“检索和利用”算法，以提高所有各方的表示质量。我们设计了一个灵活的 VFL 联邦检索增强(RA)机制: (i)交叉 RA 来补充字段缺失; (ii)本地 RA 来促进用户组之间的相互理解。我们在公共和行业数据集上进行广泛的实验。在顺序和非顺序 CTR 预测任务中的结果表明，我们的方法比基线性能有了显著的提高，并且对所有用户组都有利。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReFer:+Retrieval-Enhanced+Vertical+Federated+Recommendation+for+Full+Set+User+Benefit)|0|
|[Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation](https://doi.org/10.1145/3626772.3657710)|Chung Park, Taesan Kim, Hyungjun Yoon, Junui Hong, Yelim Yu, Mincheol Cho, Minsung Choi, Jaegul Choo|SK Telelcom / KAIST; SK Telelcom; Korea Advanced Institute of Science and Technology; SK Telecom / KAIST|Cross-Domain Sequential Recommendation (CDSR) improves recommendation performance by utilizing information from multiple domains, which contrasts with Single-Domain Sequential Recommendation (SDSR) that relies on a historical interaction within a specific domain. However, CDSR may underperform compared to the SDSR approach in certain domains due to negative transfer, which occurs when there is a lack of relation between domains or different levels of data sparsity. To address the issue of negative transfer, our proposed CDSR model estimates the degree of negative transfer of each domain and adaptively assigns it as a weight factor to the prediction loss, to control gradient flows through domains with significant negative transfer. To this end, our model compares the performance of a model trained on multiple domains (CDSR) with a model trained solely on the specific domain (SDSR) to evaluate the negative transfer of each domain using our asymmetric cooperative network. In addition, to facilitate the transfer of valuable cues between the SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis. This cooperative learning between SDSR and CDSR tasks is similar to the collaborative dynamics between pacers and runners in a marathon. Our model outperformed numerous previous works in extensive experiments on two real-world industrial datasets across ten service domains. We also have deployed our model in the recommendation system of our personal assistant app service, resulting in 21.4% increase in click-through rate compared to existing models, which is valuable to real-world business1.|跨域序列推荐(CDSR)通过利用来自多个域的信息来提高推荐性能，这与依赖于特定域内的历史交互的单域序列推荐(SDSR)形成了鲜明的对比。然而，CDSR 方法在某些领域的表现可能不如 SDSR 方法，这是由于负迁移，这种负迁移发生在领域之间缺乏联系或不同层次的数据稀疏时。为了解决负迁移问题，我们提出的 CDSR 模型估计每个域的负迁移程度，并自适应地将其作为预测损失的权重因子，以控制梯度流通过具有显著负迁移的域。为此，我们的模型比较了在多域(CDSR)训练的模型和单独在特定域(SDSR)训练的模型的性能，以评估使用我们的非对称合作网络的每个域的负迁移。此外，为了促进 SDSR 和 CDSR 任务之间有价值线索的传递，我们开发了一个辅助损失模型，该模型在每个领域的基础上最大化两个任务表征对之间的相互信息。SDSR 和 CDSR 任务之间的协作学习类似于马拉松中步行者和跑步者之间的协作动力学。我们的模型在十个服务领域的两个实际工业数据集上进行了广泛的实验，其性能优于以前的许多工作。我们也在个人助理应用程序服务的推荐系统中使用了我们的模型，与现有模型相比，点进率增加了21.4% ，这对于现实世界的商业来说是很有价值的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pacer+and+Runner:+Cooperative+Learning+Framework+between+Single-+and+Cross-Domain+Sequential+Recommendation)|0|
|[Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3657713)|Hanyu Li, Weizhi Ma, Peijie Sun, Jiayu Li, Cunxiang Yin, Yancheng He, Guoqiang Xu, Min Zhang, Shaoping Ma|Tencent; Tsinghua University|Cross-domain recommender (CDR) systems aim to enhance the performance of the target domain by utilizing data from other related domains. However, irrelevant information from the source domain may instead degrade target domain performance, which is known as the negative transfer problem. There have been some attempts to address this problem, mostly by designing adaptive representations for overlapped users. Whereas, representation adaptions solely rely on the expressive capacity of the CDR model, lacking explicit constraint to filter the irrelevant source-domain collaborative information for the target domain. In this paper, we propose a novel Collaborative information regularized User Transformation (CUT) framework to tackle the negative transfer problem by directly filtering users' collaborative information. In CUT, user similarity in the target domain is adopted as a constraint for user transformation learning to filter the user collaborative information from the source domain. CUT first learns user similarity relationships from the target domain. Then, source-target information transfer is guided by the user similarity, where we design a user transformation layer to learn target-domain user representations and a contrastive loss to supervise the user collaborative information transferred. The results show significant performance improvement of CUT compared with SOTA single and cross-domain methods. Further analysis of the target-domain results illustrates that CUT can effectively alleviate the negative transfer problem.|跨域推荐(CDR)系统旨在通过利用其他相关域的数据来提高目标域的性能。然而，来自源域的不相关信息反而会降低目标域的性能，这就是所谓的负迁移问题。已经有一些尝试来解决这个问题，主要是通过为重叠用户设计自适应表示。然而，表示适配仅仅依赖于 CDR 模型的表达能力，缺乏明确的约束来过滤不相关的源域协同信息。本文提出了一种新的协同信息规范化用户转换(CUT)框架，通过直接过滤用户的协同信息来解决负迁移问题。在 CUT 中，采用目标域中的用户相似度作为用户转换学习的约束条件，对源域中的用户协作信息进行过滤。CUT 首先从目标域学习用户相似性关系。然后，以用户相似性为指导，设计了用户转换层来学习目标域用户表示，并通过对比度损失来监督用户协同信息的传输。结果表明，与 SOTA 单域和跨域方法相比，CUT 的性能有了显著的提高。对目标域结果的进一步分析表明，CUT 可以有效地缓解负迁移问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aiming+at+the+Target:+Filter+Collaborative+Information+for+Cross-Domain+Recommendation)|0|
|[On the Negative Perception of Cross-domain Recommendations and Explanations](https://doi.org/10.1145/3626772.3657735)|Denis Kotkov, Alan Medlar, Yang Liu, Dorota Glowacka|University of Helsinki|Recommender systems typically operate within a single domain, for example, recommending books based on users' reading habits. If such data is unavailable, it may be possible to make cross-domain recommendations and recommend books based on user preferences from another domain, such as movies. However, despite considerable research on cross-domain recommendations, no studies have investigated their impact on users' behavioural intentions or system perceptions compared to single-domain recommendations. Similarly, while single-domain explanations have been shown to improve users' perceptions of recommendations, there are no comparable studies for the cross-domain case. In this article, we present a between-subject study (N=237) of users' behavioural intentions and perceptions of book recommendations. The study was designed to disentangle the effects of whether recommendations were single- or cross-domain from whether explanations were present or not. Our results show that cross-domain recommendations have lower trust and interest than single-domain recommendations, regardless of their quality. While these negative effects can be ameliorated by cross-domain explanations, they are still perceived as inferior to single-domain recommendations without explanations. Last, we show that explanations decrease interest in the single-domain case, but increase perceived transparency and scrutability in both single- and cross-domain recommendations. Our findings offer valuable insights into the impact of recommendation provenance on user experience and could inform the future development of cross-domain recommender systems.|推荐系统通常在单一领域内运作，例如，根据用户的阅读习惯推荐书籍。如果这样的数据是不可用的，它可能会作出跨领域的建议，并推荐书籍的基础上用户喜好从另一个领域，如电影。然而，尽管对跨领域建议进行了大量的研究，但没有研究调查它们对用户行为意图或系统感知的影响，与单领域建议相比。同样，虽然单一领域的解释已被证明可以改善用户对推荐的看法，但是对于跨领域的案例没有可比较的研究。在这篇文章中，我们提出了一个主题间的研究(N = 237)用户的行为意图和感知的书籍推荐。这项研究的目的是将建议是单一还是跨领域的影响与解释是否存在区分开来。我们的研究结果表明，无论其质量如何，跨域建议比单域建议具有更低的信任度和兴趣。虽然这些负面影响可以通过跨领域的解释得到改善，但它们仍然被认为不如没有解释的单领域建议。最后，我们表明，解释降低兴趣的单一领域的情况下，但增加感知的透明度和审查在单一和跨领域的建议。我们的研究结果为推荐来源对用户体验的影响提供了有价值的见解，并且可以为跨域推荐系统的未来发展提供信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Negative+Perception+of+Cross-domain+Recommendations+and+Explanations)|0|
|[Multi-Domain Sequential Recommendation via Domain Space Learning](https://doi.org/10.1145/3626772.3657685)|Junyoung Hwang, Hyunjun Ju, SeongKu Kang, Sanghwan Jang, Hwanjo Yu|Pohang University of Science and Technology; 42dot; University of Illinois Urbana-Champaign|This paper explores Multi-Domain Sequential Recommendation (MDSR), an advancement of Multi-Domain Recommendation that incorporates sequential context. Recent MDSR approach exploits domain-specific sequences, decoupled from mixed-domain histories, to model domain-specific sequential preference, and use mixeddomain histories to model domain-shared sequential preference. However, the approach faces challenges in accurately obtaining domain-specific sequential preferences in the target domain, especially when users only occasionally engage with it. In such cases, the history of users in the target domain is limited or not recent, leading the sequential recommender system to capture inaccurate domain-specific sequential preferences. To address this limitation, this paper introduces Multi-Domain Sequential Recommendation via Domain Space Learning (MDSR-DSL). Our approach utilizes cross-domain items to supplement missing sequential context in domain-specific sequences. It involves creating a "domain space" to maintain and utilize the unique characteristics of each domain and a domain-to-domain adaptation mechanism to transform item representations across domain spaces. To validate the effectiveness of MDSR-DSL, this paper extensively compares it with state-of-the-art MD(S)R methods and provides detailed analyses.|多域顺序推荐(MDSR)是结合顺序上下文的多域推荐的一种进步。最近的 MDSR 方法利用领域特定的序列，从混合领域历史解耦，建模领域特定的顺序偏好，并使用混合领域历史建模领域共享的顺序偏好。然而，该方法在准确获取目标域中特定于领域的顺序首选项时面临挑战，特别是当用户只是偶尔使用它时。在这种情况下，目标域的用户历史是有限的或不是最近的，导致顺序推荐系统捕获不准确的领域特定的顺序首选项。针对这一局限性，本文引入了基于领域空间学习的多领域序贯推荐(MDSR-DSL)。我们的方法利用跨领域的项目来补充领域特定序列中缺少的顺序上下文。它包括创建一个“域空间”来维护和利用每个域的独特特征，以及一个域到域的适应机制来跨域空间转换项表示。为了验证 MDSR-DSL 的有效性，本文将其与最新的 MD (S) R 方法进行了广泛的比较，并给出了详细的分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Domain+Sequential+Recommendation+via+Domain+Space+Learning)|0|
|[Behavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommendation Systems](https://doi.org/10.1145/3626772.3657924)|Dayu Yang, Fumian Chen, Hui Fang|University of Delaware|Large Language Models (LLMs) have demonstrated great potential in Conversational Recommender Systems (CRS). However, the application of LLMs to CRS has exposed a notable discrepancy in behavior between LLM-based CRS and human recommenders: LLMs often appear inflexible and passive, frequently rushing to complete the recommendation task without sufficient inquiry.This behavior discrepancy can lead to decreased accuracy in recommendations and lower user satisfaction. Despite its importance, existing studies in CRS lack a study about how to measure such behavior discrepancy. To fill this gap, we propose Behavior Alignment, a new evaluation metric to measure how well the recommendation strategies made by a LLM-based CRS are consistent with human recommenders'. Our experiment results show that the new metric is better aligned with human preferences and can better differentiate how systems perform than existing evaluation metrics. As Behavior Alignment requires explicit and costly human annotations on the recommendation strategies, we also propose a classification-based method to implicitly measure the Behavior Alignment based on the responses. The evaluation results confirm the robustness of the method.|大语言模型(LLM)在会话推荐系统(CRS)中显示出巨大的潜力。然而，LLM 在 CRS 中的应用暴露了基于 LLM 的 CRS 和人类推荐者之间显着的行为差异: LLM 往往显得不灵活和被动，经常在没有充分询问的情况下匆忙完成推荐任务。这种行为差异可能导致推荐的准确性下降和用户满意度降低。尽管 CRS 具有重要意义，但是现有的研究缺乏如何测量这种行为差异的研究。为了填补这个空白，我们提出了行为校准，一个新的评估指标，以衡量如何以 LLM 为基础的 CRS 的推荐策略是一致的人类推荐者的。我们的实验结果表明，与现有的评估指标相比，新的指标更符合人类的偏好，能够更好地区分系统的执行情况。由于行为对齐需要对推荐策略进行明确而昂贵的人工注释，我们还提出了一种基于分类的方法来隐式地度量基于响应的行为对齐。评价结果证实了该方法的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior+Alignment:+A+New+Perspective+of+Evaluating+LLM-based+Conversational+Recommendation+Systems)|0|
|[Bi-Objective Negative Sampling for Sensitivity-Aware Search](https://doi.org/10.1145/3626772.3657895)|Jack McKechnie, Graham McDonald, Craig Macdonald|University of Glasgow|Cross-encoders leverage fine-grained interactions between documents and queries for effective relevance ranking. Such ranking models are typically trained to satisfy the single objective of providing relevant information to the users. However, not all information should be made available. For example, documents containing sensitive information, such as personal or confidential information, should not be returned in the search results. Sensitivity-aware search (SAS) aims to develop retrieval models that can satisfy two objectives, namely: (1) providing the user with relevant search results, while (2) ensuring that no documents that contain sensitive information are included in the ranking. In this work, we propose three novel negative sampling strategies that enable cross-encoders to be trained to satisfy the bi-objective task of SAS. Additionally, we investigate and compare with filtering sensitive documents in ranking pipelines. Our experiments on a collection labelled for sensitivity show that our proposed negative sampling strategies lead to a ~37% increase in terms of cost-sensitive nDCG (nCSDCG) for SAS.|交叉编码器利用文档和查询之间的细粒度交互来进行有效的相关性排序。这种排名模型通常经过训练，以满足向用户提供相关信息的单一目标。然而，并非所有的信息都应该提供。例如，包含敏感信息(如个人或机密信息)的文档不应在搜索结果中返回。敏感性搜索(SAS)旨在开发能够满足两个目标的检索模型，即: (1)为用户提供相关的搜索结果，同时(2)确保没有包含敏感信息的文档被包含在排名中。在这项工作中，我们提出了三种新颖的负采样策略，使交叉编码器的训练，以满足 SAS 的双目标任务。此外，我们还研究和比较了在排序管道中过滤敏感文档的方法。我们对标记为敏感性的集合的实验表明，我们提出的阴性采样策略导致 SAS 的成本敏感性 nDCG (nCSDCG)增加约37% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-Objective+Negative+Sampling+for+Sensitivity-Aware+Search)|0|
|[Relevance Feedback Method For Patent Searching Using Vector Subspaces](https://doi.org/10.1145/3626772.3661365)|Sebastian Björkqvist|IPRally Technologies Oy|Searching for novelty-destroying prior art is an important part of patent application drafting and invalidation. The task is challenging due to the detailed information needed to determine whether a document is novelty-destroying or simply closely related, resulting in the original search results not always being fully on target. Allowing the user to provide feedback on the relevance of the initial search results and iterating on the search may thus improve the results significantly. We present a relevance feedback method based on computing the affine vector subspace spanned by the relevant document vectors. The method can be used with any dense retrieval system, and we demonstrate its effectiveness in improving recall in prior art searches. We compare the subspace-based method to the Rocchio algorithm and show that the method is less sensitive to changes in hyperparameters when the number of relevant documents increases.|查找毁新技术是专利申请起草和失效的重要组成部分。这项任务具有挑战性，因为确定一份文件是否具有新颖性或仅仅是密切相关所需的详细信息，导致原始搜索结果并不总是完全符合目标。因此，允许用户就初始搜索结果的相关性提供反馈并对搜索进行迭代，可以大大改进搜索结果。我们提出了一种基于计算相关文档向量所跨越的仿射向量子空间的关联反馈方法。该方法可以应用于任何密集检索系统，并证明了该方法在提高现有技术检索中的召回率方面的有效性。我们比较了基于子空间的方法和 Rocchio 算法，发现当相关文档数量增加时，该方法对超参数的变化不太敏感。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relevance+Feedback+Method+For+Patent+Searching+Using+Vector+Subspaces)|0|
|[Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations](https://doi.org/10.1145/3626772.3657769)|Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini|Dipartimento di Informatica, Università di Pisa; ISTI-CNR; Pinecone|Learned sparse representations form an attractive class of contextual embeddings for text retrieval. That is so because they are effective models of relevance and are interpretable by design. Despite their apparent compatibility with inverted indexes, however, retrieval over sparse embeddings remains challenging. That is due to the distributional differences between learned embeddings and term frequency-based lexical models of relevance such as BM25. Recognizing this challenge, a great deal of research has gone into, among other things, designing retrieval algorithms tailored to the properties of learned sparse representations, including approximate retrieval systems. In fact, this task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on a large benchmark dataset by throughput and recall. In this work, we propose a novel organization of the inverted index that enables fast yet effective approximate retrieval over learned sparse embeddings. Our approach organizes inverted lists into geometrically-cohesive blocks, each equipped with a summary vector. During query processing, we quickly determine if a block must be evaluated using the summaries. As we show experimentally, single-threaded query processing using our method, Seismic, reaches sub-millisecond per-query latency on various sparse embeddings of the MS MARCO dataset while maintaining high recall. Our results indicate that Seismic is one to two orders of magnitude faster than state-of-the-art inverted index-based solutions and further outperforms the winning (graph-based) submissions to the BigANN Challenge by a significant margin.|学习的稀疏表示形成了一类有吸引力的文本检索上下文嵌入。之所以如此，是因为它们是有效的相关性模型，可以通过设计加以解释。然而，尽管它们与反向索引具有明显的兼容性，但是通过稀疏嵌入进行检索仍然具有挑战性。这是由于学习嵌入和基于词汇频率的关联词汇模型(如 BM25)之间的分布差异造成的。认识到这一挑战，大量的研究已经进入，除其他事项外，设计检索算法适合于学习稀疏表示的属性，包括近似检索系统。事实上，这项任务在 NeurIPS 2023最新的 BigANN 挑战中占有显著地位，在这个挑战中，通过吞吐量和召回率对大型基准数据集上的近似算法进行了评估。在这项工作中，我们提出了一种新的组织倒排索引，使快速而有效的近似检索学习稀疏嵌入。我们的方法将倒排的列表组织成具有几何内聚性的块，每个块配备一个汇总向量。在查询处理过程中，我们快速确定是否必须使用摘要计算块。正如我们的实验表明，使用我们的方法，地震，单线程查询处理达到亚毫秒每查询延迟各种稀疏嵌入的 MS MARCO 数据集，同时保持高召回率。我们的研究结果表明，地震数量级比最先进的基于倒排索引的解决方案快一到两倍，并进一步优于 BigANN 挑战赛的获胜者(基于图表的)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Inverted+Indexes+for+Approximate+Retrieval+over+Learned+Sparse+Representations)|0|
|[Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and Relevance](https://doi.org/10.1145/3626772.3657832)|Theresia Veronika Rampisela, Tuukka Ruotsalo, Maria Maistro, Christina Lioma|University of Copenhagen|Relevance and fairness are two major objectives of recommender systems (RSs). Recent work proposes measures of RS fairness that are either independent from relevance (fairness-only) or conditioned on relevance (joint measures). While fairness-only measures have been studied extensively, we look into whether joint measures can be trusted. We collect all joint evaluation measures of RS relevance and fairness, and ask: How much do they agree with each other? To what extent do they agree with relevance/fairness measures? How sensitive are they to changes in rank position, or to increasingly fair and relevant recommendations? We empirically study for the first time the behaviour of these measures across 4 real-world datasets and 4 recommenders. We find that most of these measures: i) correlate weakly with one another and even contradict each other at times; ii) are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures; and iii) tend to compress scores at the low end of their range, meaning that they are not very expressive. We counter the above limitations with a set of guidelines on the appropriate usage of such measures, i.e., they should be used with caution due to their tendency to contradict each other and of having a very small empirical range.|相关性和公平性是推荐系统的两个主要目标。最近的研究提出了 RS 公平性的测量方法，这些测量方法要么独立于相关性(仅仅是公平性) ，要么以相关性(联合测量)为条件。虽然只有公平的措施已经得到了广泛的研究，但是我们研究的是联合措施是否可以信任。我们收集了所有 RS 相关性和公平性的联合评价指标，并问: 它们之间有多大程度的一致性？它们在多大程度上同意相关性/公平性措施？他们对职位的变化，或者对越来越公平和相关的建议有多敏感？我们首次实证研究了这些措施的行为在4个真实世界的数据集和4个推荐。我们发现这些测量中的大多数: i)彼此之间相关性很弱，有时甚至相互矛盾; ii)对排名位置变化的敏感性低于相关性和公平性测量，这意味着它们比传统的 RS 测量粒度更小; iii)倾向于压缩其范围的低端分数，这意味着它们不是非常具有表现力。针对上述限制，我们制定了一套关于适当使用此类措施的指导方针，即应谨慎使用这些措施，因为它们往往相互矛盾，而且经验范围很小。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+We+Trust+Recommender+System+Fairness+Evaluation?+The+Role+of+Fairness+and+Relevance)|0|
|[Sequential Recommendation with Latent Relations based on Large Language Model](https://doi.org/10.1145/3626772.3657762)|Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, Min Zhang|Tsinghua University; Meituan|Sequential recommender systems predict items that may interest users by modeling their preferences based on historical interactions. Traditional sequential recommendation methods rely on capturing implicit collaborative filtering signals among items. Recent relation-aware sequential recommendation models have achieved promising performance by explicitly incorporating item relations into the modeling of user historical sequences, where most relations are extracted from knowledge graphs. However, existing methods rely on manually predefined relations and suffer the sparsity issue, limiting the generalization ability in diverse scenarios with varied item relations. In this paper, we propose a novel relation-aware sequential recommendation framework with Latent Relation Discovery (LRD). Different from previous relation-aware models that rely on predefined rules, we propose to leverage the Large Language Model (LLM) to provide new types of relations and connections between items. The motivation is that LLM contains abundant world knowledge, which can be adopted to mine latent relations of items for recommendation. Specifically, inspired by that humans can describe relations between items using natural language, LRD harnesses the LLM that has demonstrated human-like knowledge to obtain language knowledge representations of items. These representations are fed into a latent relation discovery module based on the discrete state variational autoencoder (DVAE). Then the self-supervised relation discovery tasks and recommendation tasks are jointly optimized. Experimental results on multiple public datasets demonstrate our proposed latent relations discovery method can be incorporated with existing relation-aware sequential recommendation models and significantly improve the performance. Further analysis experiments indicate the effectiveness and reliability of the discovered latent relations.|顺序推荐系统通过基于历史交互对用户偏好进行建模来预测用户可能感兴趣的项目。传统的顺序推荐方法依赖于捕捉项目之间隐含的协同过滤信号。最近的关系感知序列推荐模型已经取得了良好的性能，明确地结合项目关系到用户历史序列的建模，其中大多数关系是从知识图提取。然而，现有的方法依赖于人工预定义的关系，并且存在稀疏性问题，限制了在不同项目关系的不同场景中的泛化能力。本文提出了一种新的基于潜在关系发现(LRD)的关系感知序列推荐框架。与以前依赖于预定义规则的关系感知模型不同，我们建议利用大语言模型(LLM)来提供新类型的关系和项之间的连接。其动机是 LLM 包含了丰富的世界知识，可以用来挖掘推荐项目的潜在关系。具体来说，受到人类可以使用自然语言描述项目之间关系的启发，LRD 利用已经证明类似于人类的知识的 LLM 来获得项目的语言知识表示。这些表示被反馈到基于离散状态变分自动编码器(DVAE)的潜在关系发现模块中。然后对自监督关系发现任务和推荐任务进行联合优化。在多个公共数据集上的实验结果表明，本文提出的潜在关系发现方法可以与现有的关系感知顺序推荐模型相结合，从而显著提高推荐性能。进一步的分析实验表明了所发现的潜在关系的有效性和可靠性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Recommendation+with+Latent+Relations+based+on+Large+Language+Model)|0|
|[Enhancing Sequential Recommenders with Augmented Knowledge from Aligned Large Language Models](https://doi.org/10.1145/3626772.3657782)|Yankun Ren, Zhongde Chen, Xinxing Yang, Longfei Li, Cong Jiang, Lei Cheng, Bo Zhang, Linjian Mo, Jun Zhou|Ant Group|Recommender systems are widely used in various online platforms. In the context of sequential recommendation, it is essential to accurately capture the chronological patterns in user activities to generate relevant recommendations. Conventional ID-based sequential recommenders have shown promise but lack comprehensive real-world knowledge about items, limiting their effectiveness. Recent advancements in Large Language Models (LLMs) offer the potential to bridge this gap by leveraging the extensive real-world knowledge encapsulated in LLMs. However, integrating LLMs into sequential recommender systems comes with its own challenges, including inadequate representation of sequential behavior patterns and long inference latency. In this paper, we propose SeRALM (Enhancing <u>Se</u>quential <u>R</u>ecommenders with Augmented Knowledge from <u>A</u>ligned Large <u>L</u>anguage <u>M</u>odels) to address these challenges. SeRALM integrates LLMs with conventional ID-based sequential recommenders for sequential recommendation tasks. We combine text-format knowledge generated by LLMs with item IDs and feed this enriched data into ID-based recommenders, benefitting from the strengths of both paradigms. Moreover, we develop a theoretically underpinned alignment training method to refine LLMs' generation using feedback from ID-based recommenders for better knowledge augmentation. We also present an asynchronous technique to expedite the alignment training process. Experimental results on public benchmarks demonstrate that SeRALM significantly improves the performances of ID-based sequential recommenders. Further, a series of ablation studies and analyses corroborate SeRALM's proficiency in steering LLMs to generate more pertinent and advantageous knowledge across diverse scenarios.|推荐系统广泛应用于各种在线平台。在顺序推荐的背景下，准确地捕获用户活动中的顺序模式以生成相关的推荐是至关重要的。传统的基于 ID 的顺序推荐已经显示出希望，但是缺乏关于项目的全面的现实世界知识，限制了它们的有效性。大型语言模型(LLM)中的最新进展提供了通过利用 LLM 中封装的广泛的现实世界知识来弥补这一差距的潜力。然而，将 LLM 集成到顺序推荐系统中也有其自身的挑战，包括顺序行为模式的不充分表示和长的推理延迟。在这篇论文中，我们提出了 SeRALM (增强 < u > Se </u > 量 < u > R </u > 从 < u > A </u > 线性大 < u > L </u > 语言 < u > M </u > 模型的增强知识推荐)来解决这些挑战。SerRALM 将 LLM 与传统的基于 ID 的顺序推荐器集成在一起，用于顺序推荐任务。我们将 LLM 生成的文本格式知识与项目 ID 结合起来，并将这些丰富的数据提供给基于 ID 的推荐程序，这两种范例的优势使我们受益匪浅。此外，我们开发了一个理论上支持的对齐训练方法来细化 LLM 的生成，使用基于 ID 的推荐者的反馈来更好地增强知识。我们还提出了一种异步技术，以加快对准训练过程。对公共基准测试的实验结果表明，基于 ID 的顺序推荐算法的性能得到了明显的改善。此外，一系列的消融研究和分析证实了 SerRALM 在指导 LLM 方面的能力，以便在不同的情况下产生更相关和更有利的知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Sequential+Recommenders+with+Augmented+Knowledge+from+Aligned+Large+Language+Models)|0|
|[Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment](https://doi.org/10.1145/3626772.3657709)|Xinyu Zhu, Lilin Zhang, Ning Yang|Sichuan University|Personalized fairness in recommendations has been attracting increasing attention from researchers. The existing works often treat a fairness requirement, represented as a collection of sensitive attributes, as a hyper-parameter, and pursue extreme fairness by completely removing information of sensitive attributes from the learned fair embedding, which suffer from two challenges: huge training cost incurred by the explosion of attribute combinations, and the suboptimal trade-off between fairness and accuracy. In this paper, we propose a novel Adaptive Fair Representation Learning (AFRL) model, which achieves a real personalized fairness due to its advantage of training only one model to adaptively serve different fairness requirements during inference phase. Particularly, AFRL treats fairness requirements as inputs and can learn an attribute-specific embedding for each attribute from the unfair user embedding, which endows AFRL with the adaptability during inference phase to determine the non-sensitive attributes under the guidance of the user's unique fairness requirement. To achieve a better trade-off between fairness and accuracy in recommendations, AFRL conducts a novel Information Alignment to exactly preserve discriminative information of non-sensitive attributes and incorporate a debiased collaborative embedding into the fair embedding to capture attribute-independent collaborative signals, without loss of fairness. Finally, the extensive experiments conducted on real datasets together with the sound theoretical analysis demonstrate the superiority of AFRL.|推荐的个性化公平性越来越受到研究者的关注。现有的公平需求表示为一组敏感属性，是一个超参数，通过从学习公平嵌入中完全去除敏感属性的信息来追求极端公平，这种方法面临着两个挑战: 属性组合爆炸所带来的巨大训练成本，以及公平性和准确性之间的次优权衡。本文提出了一种新的自适应公平表示学习(AFRL)模型，该模型由于在推理阶段只训练一个模型来适应不同的公平需求，从而实现了真正的个性化公平。特别地，AFRL 将公平性要求视为输入，可以从不公平的用户嵌入中学习每个属性的特定属性嵌入，从而赋予 AFRL 在推理阶段在用户唯一公平性要求指导下确定非敏感属性的适应性。为了在推荐的公平性和准确性之间取得更好的平衡，AFRL 进行了一种新的信息对齐，以精确地保留非敏感属性的区分信息，并在公平嵌入中加入去偏见的协作嵌入，以捕获与属性无关的协作信号，而不会损失公平性。最后，在实际数据集上进行了广泛的实验，结合可靠的理论分析，验证了 AFRL 的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Fair+Representation+Learning+for+Personalized+Fairness+in+Recommendations+via+Information+Alignment)|0|
|[MealRec+: A Meal Recommendation Dataset with Meal-Course Affiliation for Personalization and Healthiness](https://doi.org/10.1145/3626772.3657857)|Ming Li, Lin Li, Xiaohui Tao, Jimmy Xiangji Huang|Wuhan University of Technology; York University; University of Southern Queensland|Meal recommendation, as a typical health-related recommendation task, contains complex relationships between users, courses, and meals. Among them, meal-course affiliation associates user-meal and user-course interactions. However, an extensive literature review demonstrates that there is a lack of publicly available meal recommendation datasets including meal-course affiliation. Meal recommendation research has been constrained in exploring the impact of cooperation between two levels of interaction on personalization and healthiness. To pave the way for meal recommendation research, we introduce a new benchmark dataset called MealRec^+. Due to constraints related to user health privacy and meal scenario characteristics, the collection of data that includes both meal-course affiliation and two levels of interactions is impeded. Therefore, a simulation method is adopted to derive meal-course affiliation and user-meal interaction from the user's dining sessions simulated based on user-course interaction data. Then, two well-known nutritional standards are used to calculate the healthiness scores of meals. Moreover, we experiment with several baseline models, including separate and cooperative interaction learning methods. Our experiment demonstrates that cooperating the two levels of interaction in appropriate ways is beneficial for meal recommendations. Furthermore, in response to the less healthy recommendation phenomenon found in the experiment, we explore methods to enhance the healthiness of meal recommendations. The dataset is available on GitHub (https://github.com/WUT-IDEA/MealRecPlus).|膳食推荐作为一项典型的与健康相关的推荐任务，包含用户、课程和膳食之间的复杂关系。其中，用餐过程的联系将用户-用餐和用户-过程的交互联系起来。然而，一个广泛的文献回顾表明，有缺乏公开可用的膳食推荐数据集，包括膳食过程的联系。饮食推荐研究在探讨两个互动水平之间的合作对个性化和健康的影响方面受到了限制。为了为膳食推荐研究铺平道路，我们引入了一个新的基准数据集 MealRec ^ + 。由于与用户健康隐私和用餐场景特征有关的限制，收集包括用餐过程关联和两个层次的互动的数据受到阻碍。为此，采用一种仿真方法，从基于用户-过程交互数据的用户用餐会话模拟中，推导出用户-过程关联关系和用户-用餐交互关系。然后，使用两个众所周知的营养标准来计算膳食的健康评分。此外，我们还实验了几个基线模型，包括分离式和合作式交互学习方法。我们的实验表明，以适当的方式协调两个层次的互动对于推荐用餐是有益的。此外，针对实验中发现的不太健康的推荐现象，我们探讨了提高膳食推荐健康性的方法。该数据集可在 GitHub ( https://GitHub.com/wut-idea/mealrecplus )上获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MealRec+:+A+Meal+Recommendation+Dataset+with+Meal-Course+Affiliation+for+Personalization+and+Healthiness)|0|
|[IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT](https://doi.org/10.1145/3626772.3657725)|Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Jie Wang, Joemon M. Jose|Shandong University; University of Glasgow; Telefonica Research; Amazon; University of Glasgow school pf computing science|Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation. IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training. Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that "parameter efficiency represents overall efficiency". TPME provides more comprehensive insights into practical efficiency comparisons between different methods. Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN. We release our codes and other materials at https://github.com/GAIR-Lab/IISAN.|多模态基础模型在顺序推荐系统中具有变革性，利用了强大的表示学习能力。虽然参数有效微调(PEFT)通常用于为推荐任务调整基础模型，但大多数研究优先考虑参数有效性，往往忽略了 GPU 内存效率和训练速度等关键因素。针对这一差距，本文介绍了 IISAN (Intra-and Inter-modal Side Adapted Network for Multimodal Reform) ，这是一个简单的即插即用的结构，采用了解耦 PEFT 结构，同时利用了模式内和模式间的自适应。IISAN 匹配全微调(FFT)和最先进的 PEFT 的性能。更重要的是，它显著降低了 GPU 内存使用量——对于多通道顺序推荐任务，从47GB 降至仅3GB。此外，与 FFT 相比，它将每个历元的训练时间从443秒提高到22秒。与 Adapter 和 LoRA 相比，这也是一个显著的改进，后者需要37-39 GB 的 GPU 内存和350-380秒每个纪元的训练时间。此外，我们提出了一个新的组合效率度量，TPME (训练时间，参数和 GPU 内存效率) ，以缓解流行的误解“参数效率代表整体效率”。TPME 为不同方法之间的实际效率比较提供了更全面的见解。此外，我们还对所有 PEFT 和 FFT 方法进行了有效性分析，从而验证了 IISAN 方法的优越性。我们在 https://github.com/gair-lab/iisan 公布我们的代码和其他材料。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IISAN:+Efficiently+Adapting+Multimodal+Representation+for+Sequential+Recommendation+with+Decoupled+PEFT)|0|
|[FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation](https://doi.org/10.1145/3626772.3657853)|Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon|The University of Queensland School of Information Technology and Electrical Engineering; CSIRO; The University of Queensland ITEE|Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines.|联邦搜索系统聚合来自多个搜索引擎的结果，选择适当的来源，以提高结果质量，并与用户意图保持一致。随着检索增强生成(RAG)流水线的日益普及，联邦搜索在跨异构数据源获取相关信息以产生知情响应方面可以发挥关键作用。然而，现有的数据集，比如在过去 TREC 联邦网络跟踪中开发的数据集，早于 RAG 范式转变，缺乏对现代信息检索挑战的描述。为了弥补这一差距，我们提出了 FeB4RAG，一个专门为 RAG 框架内的联邦搜索而设计的新型数据集。该数据集来源于广泛使用的基准测试集合的16个子集，包括为聊天机器人应用程序量身定制的790个信息请求(类似于对话查询) ，以及每个资源返回的最高结果和相关的 LLM 衍生的相关性判断。此外，为了支持对这个集合的需求，我们演示了 RAG 的高质量联邦搜索系统与联邦搜索的简单方法相比对响应生成的影响。我们通过定性的并行比较来比较通过 RAG 管道产生的答案。我们的集合支持开发和评估新的联邦搜索方法，特别是在 RAG 管道上下文中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FeB4RAG:+Evaluating+Federated+Search+in+the+Context+of+Retrieval+Augmented+Generation)|0|
|[Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation](https://doi.org/10.1145/3626772.3657695)|Zhe Xu, Daoyuan Chen, Jiayi Kuang, Zihao Yi, Yaliang Li, Ying Shen|Alibaba Group; Alibaba group; Sun Yat-sen University|Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences. In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively. We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (), a novel approach that synergizes these elements to improve the quality of support provided in ESCs. By leveraging in-context learning and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs. We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to deepen situational awareness of help-seekers' mental states. Our supportive decoder integrates information from diverse knowledge sources, underpinning response generation that is both empathetic and cognitively aware. The effectiveness of is demonstrated through extensive automatic and human evaluations, revealing substantial improvements over numerous state-of-the-art models, with up to 13.79% enhancement in overall performance of ten metrics. Our codes are available for public access to facilitate further research and development.|情绪支持对话(ESC)系统是提供移情互动的关键，帮助用户通过理解和处理他们独特的经验的消极情绪状态。本文研究了 ESC 中的两个关键问题: 通过动态实证检索来提高情境相关性和同理心反应的产生; 通过提高认知理解来全面掌握内隐心理状态。我们介绍了动态演示检索和认知方面情境理解() ，一种新的方法，协同这些要素，以提高质量的支持提供在胚胎干细胞。通过利用上下文学习和人物角色信息，我们引入了一种创新的检索机制，选择信息丰富和个性化的演示对。我们还提出了一个认知理解模块，该模块利用来自 ATOMIC 知识源的四种认知关系来加深求助者心理状态的情势察觉。我们的支持性解码器整合了来自不同知识来源的信息，支持同理心和认知意识的反应生成。其有效性通过广泛的自动和人工评估得到了证实，显示出在许多最先进的模型上有了实质性的改进，10个指标的总体性能提高了13.79% 。我们的守则可供公众查阅，以促进进一步的研究和发展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Demonstration+Retrieval+and+Cognitive+Understanding+for+Emotional+Support+Conversation)|0|
|[Broadening the View: Demonstration-augmented Prompt Learning for Conversational Recommendation](https://doi.org/10.1145/3626772.3657755)|Huy Dao, Yang Deng, Dung D. Le, Lizi Liao|National University of Singapore; Singapore Management University; College of Engineering and Computer Science, VinUniversity|Conversational Recommender Systems (CRSs) leverage natural language dialogues to provide tailored recommendations. Traditional methods in this field primarily focus on extracting user preferences from isolated dialogues. It often yields responses with a limited perspective, confined to the scope of individual conversations. Recognizing the potential in collective dialogue examples, our research proposes an expanded approach for CRS models, utilizing selective analogues from dialogue histories and responses to enrich both generation and recommendation processes. This introduces significant research challenges, including: (1) How to secure high-quality collections of recommendation dialogue exemplars? (2) How to effectively leverage these exemplars to enhance CRS models? To tackle these challenges, we introduce a novel Demonstration-enhanced Conversational Recommender System (DCRS), which aims to strengthen its understanding on the given dialogue contexts by retrieving and learning from demonstrations. In particular, we first propose a knowledge-aware contrastive learning method that adeptly taps into the mentioned entities and the dialogue's contextual essence for pretraining the demonstration retriever. Subsequently, we further develop two adaptive demonstration-augmented prompt learning approaches, involving contextualized prompt learning and knowledge-enriched prompt learning, to bridge the gap between the retrieved demonstrations and the two end tasks of CRS, i.e., response generation and item recommendation, respectively. Rigorous evaluations on two established benchmark datasets underscore DCRS's superior performance over existing CRS methods in both item recommendation and response generation.|会话推荐系统(CRS)利用自然语言对话提供量身定制的推荐。该领域的传统方法主要侧重于从孤立对话中提取用户首选项。它常常产生一个有限的视角的回应，局限于个人对话的范围。认识到集体对话实例的潜力，我们的研究提出了一种扩展的 CRS 模型方法，利用对话历史和回应中的选择性类比来丰富生成和推荐过程。这引入了重大的研究挑战，包括: (1)如何保证高质量的推荐对话样本集？(2)如何有效地利用这些范例来增强 CRS 模型？为了应对这些挑战，我们引入了一个新颖的示范增强会话推荐系统(dCRS) ，目的是通过检索和学习示范来加强对特定对话背景的理解。特别地，我们首先提出了一种知识感知的对比学习方法，该方法能够很好地利用上述实体和对话的语境本质来预先训练示范检索器。随后，我们进一步开发了两种适应性示范增强的及时学习方法，包括上下文化的及时学习和知识丰富的及时学习，以弥合检索的示范和 CRS 的两个最终任务之间的差距，即响应生成和项目推荐。对两个已建立的基准数据集的严格评估强调了 DCRS 在项目推荐和响应生成方面优于现有 CRS 方法的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Broadening+the+View:+Demonstration-augmented+Prompt+Learning+for+Conversational+Recommendation)|0|
|[ProCIS: A Benchmark for Proactive Retrieval in Conversations](https://doi.org/10.1145/3626772.3657869)|Chris Samarinas, Hamed Zamani|University of Massachusetts Amherst|The field of conversational information seeking, which is rapidly gaining interest in both academia and industry, is changing how we interact with search engines through natural language interactions. Existing datasets and methods are mostly evaluating reactive conversational information seeking systems that solely provide response to every query from the user. We identify a gap in building and evaluating proactive conversational information seeking systems that can monitor a multi-party human conversation and proactively engage in the conversation at an opportune moment by retrieving useful resources and suggestions. In this paper, we introduce a large-scale dataset for proactive document retrieval that consists of over 2.8 million conversations. We conduct crowdsourcing experiments to obtain high-quality and relatively complete relevance judgments through depth-k pooling. We also collect annotations related to the parts of the conversation that are related to each document, enabling us to evaluate proactive retrieval systems. We introduce normalized proactive discounted cumulative gain (npDCG) for evaluating these systems, and further provide benchmark results for a wide range of models, including a novel model we developed for this task. We believe that the developed dataset, called ProCIS, paves the path towards developing proactive conversational information seeking systems.|会话信息搜索领域正在迅速引起学术界和工业界的兴趣，它正在改变我们通过自然语言交互与搜索引擎进行互动的方式。现有的数据集和方法主要是评估反应式会话信息搜索系统，这种系统只对用户的每个查询提供响应。我们发现在建立和评估积极主动的会话信息搜索系统方面存在差距，这种系统可以监控多方的人类会话，并通过检索有用的资源和建议，在适当的时候积极主动地参与会话。在这篇文章中，我们介绍了一个大型的主动文献检索数据集，包括超过280万次对话。我们进行众包实验，以获得高质量和相对完整的相关性判断通过深度 k 池。我们还收集与会话中与每个文档相关的部分相关的注释，使我们能够评估主动检索系统。我们引入标准化的前瞻性折扣累积增益(npDCG)来评估这些系统，并进一步提供基准结果的范围广泛的模型，包括一个新的模型，我们开发的这项任务。我们相信，所开发的数据集，称为 ProCIS，铺平了发展前瞻性会话信息搜索系统的道路。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProCIS:+A+Benchmark+for+Proactive+Retrieval+in+Conversations)|0|
|[An Empirical Analysis on Multi-turn Conversational Recommender Systems](https://doi.org/10.1145/3626772.3657893)|Lu Zhang, Chen Li, Yu Lei, Zhu Sun, Guanfeng Liu|Macquarie University; Yanshan University; Chengdu University of Information Technology; Agency for Science, Technology and Research, Singapore|The rise of conversational recommender systems (CRSs) brings the evolution of the recommendation paradigm, which enables users to interact with the system and achieve dynamic recommendations. As one essential branch, multi-turn CRSs, built on the user simulator paradigm, have attracted great attention due to their powerful ability to accomplish recommendations without real dialogue resources. Recent multi-turn CRS models, equipped with various delicately designed components (e.g., conversation module), achieve state-of-the-art (SOTA) performance. We, for the first time, propose a comprehensive experimental evaluation for existing SOTA multi-turn CRSs to investigate three research questions: (1) reproducibility - are the designed components beneficial to target multi-turn CRSs? (2) scenario-specific adaptability - how do these components perform in various scenarios? and (3) generality - can the effective components from the target CRS be effectively transferred to other multi-turn CRSs? To answer these questions, we design and conduct experiments under different settings, including carefully selected SOTA baselines, components of CRSs, datasets, and evaluation metrics, thus providing an experimental aspect overview of multi-turn CRSs. As a result, we derive several significant insights whereby effective guidelines are provided for future multi-turn CRS model designs across diverse scenarios.|会话推荐系统(CRS)的兴起带来了推荐范式的演变，使得用户能够与系统进行交互，实现动态推荐。作为一个重要的分支，建立在用户模拟器范式之上的多回合 CRS 由于其在没有真实对话资源的情况下完成推荐的强大能力而引起了人们的极大关注。最近的多回转 CRS 模型，配备了各种精心设计的组件(例如，会话模块) ，实现了最先进的(SOTA)性能。我们首次对现有的 SOTA 多回转 CRS 进行了全面的实验评价，以探讨三个研究问题: (1)可重复性——所设计的部件是否有利于靶向多回转 CRS？(2)特定场景的适应性——这些组件在各种场景中如何执行？(3)通用性——目标 CRS 的有效部件能否有效地转移到其他多回路 CRS 上？为了回答这些问题，我们在不同的设置下设计和进行实验，包括精心选择的 SOTA 基线，CRS 的组成部分，数据集和评估指标，从而提供多回合 CRS 的实验方面的概述。因此，我们得出了几个重要的见解，从而为未来多回合 CRS 模型设计提供了有效的指导方针，跨越不同的情景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Analysis+on+Multi-turn+Conversational+Recommender+Systems)|0|
|[SM-RS: Single- and Multi-Objective Recommendations with Contextual Impressions and Beyond-Accuracy Propensity Scores](https://doi.org/10.1145/3626772.3657863)|Patrik Dokoupil, Ladislav Peska, Ludovico Boratto|Faculty of Mathematics and Physics, Charles University, Prague, Czechia; University of Cagliari|Recommender systems (RS) rely on interaction data between users and items to generate effective results. Historically, RS aimed to deliver the most consistent (i.e., accurate) items to the trained user profiles. However, the attention towards additional (beyond-accuracy) quality criteria has increased tremendously in recent years. Both the research and applied models are being optimized for diversity, novelty, or fairness, to name a few. Naturally, the proper functioning of such optimization methods depends on the knowledge of users' propensities towards interacting with recommendations having certain quality criteria. However, so far, no dataset that captures such propensities exists. To bridge this research gap, we present SM-RS (single-objective + multi-objective recommendations dataset) that links users' self-declared propensity toward relevance, novelty, and diversity criteria with impressions and corresponding item selections. After presenting the dataset's collection procedure and basic statistics, we propose three tasks that are rarely available to conduct using existing RS datasets: impressions-aware click prediction, users' propensity scores prediction, and construction of recommendations proportional to the users' propensity scores. For each task, we also provide detailed evaluation procedures and competitive baselines. The dataset is available at https://osf.io/hkzje/.|推荐系统(RS)依赖于用户和项目之间的交互数据来生成有效的结果。从历史上看，RS 的目标是向训练有素的用户配置文件提供最一致(即准确)的条目。然而，对于额外的(超精确度)质量标准的关注在最近几年已经大大增加。研究和应用模型都在为多样性、新颖性或公平性而进行优化。当然，这种优化方法的正确功能取决于用户对具有某些质量标准的建议的交互倾向的了解。然而，到目前为止，还没有数据集能够捕捉到这种倾向。为了弥合这一研究差距，我们提出了 SM-RS (单目标 + 多目标推荐数据集) ，将用户自我声明的相关性，新颖性和多样性标准与印象和相应的项目选择联系起来。在介绍了数据集的收集过程和基本统计数据之后，我们提出了三个使用现有 RS 数据集很少可用的任务: 印象感知的点击预测，用户倾向得分预测，以及与用户倾向得分成比例的建议的构建。对于每项任务，我们还提供了详细的评估程序和竞争基线。数据集可在 https://osf.io/hkzje/下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SM-RS:+Single-+and+Multi-Objective+Recommendations+with+Contextual+Impressions+and+Beyond-Accuracy+Propensity+Scores)|0|
|[To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process](https://doi.org/10.1145/3626772.3657732)|Zhongxiang Sun, Zihua Si, Xiao Zhang, Xiaoxue Zang, Yang Song, Hongteng Xu, Jun Xu|Renmin Unversity of China; Renmin Unversity of China Gaoling School of Artificial Intelligence; Kuaishou Technology Co., Ltd. Recommendation; Kuaishou Technology Co., Ltd.; Renmin University of China Gaoling School of Artificial Intelligence|Incorporating Search and Recommendation (S R) services within a singularapplication is prevalent in online platforms, leading to a new task termedopen-app motivation prediction, which aims to predict whether users initiatethe application with the specific intent of information searching, or toexplore recommended content for entertainment. Studies have shown thatpredicting users' motivation to open an app can help to improve user engagementand enhance performance in various downstream tasks. However, accuratelypredicting open-app motivation is not trivial, as it is influenced byuser-specific factors, search queries, clicked items, as well as their temporaloccurrences. Furthermore, these activities occur sequentially and exhibitintricate temporal dependencies. Inspired by the success of the Neural HawkesProcess (NHP) in modeling temporal dependencies in sequences, this paperproposes a novel neural Hawkes process model to capture the temporaldependencies between historical user browsing and querying actions. The model,referred to as Neural Hawkes Process-based Open-App Motivation prediction model(NHP-OAM), employs a hierarchical transformer and a novel intensity function toencode multiple factors, and open-app motivation prediction layer to integratetime and user-specific information for predicting users' open-app motivations.To demonstrate the superiority of our NHP-OAM model and construct a benchmarkfor the Open-App Motivation Prediction task, we not only extend the public S Rdataset ZhihuRec but also construct a new real-world Open-App MotivationDataset (OAMD). Experiments on these two datasets validate NHP-OAM'ssuperiority over baseline models. Further downstream application experimentsdemonstrate NHP-OAM's effectiveness in predicting users' Open-App Motivation,highlighting the immense application value of NHP-OAM.|将搜索和推荐(S R)服务整合到一个单一的应用程序中在在线平台中非常普遍，这导致了一个新的任务，即开放应用程序动机预测，其目的是预测用户是以信息搜索的特定意图启动应用程序，还是为娱乐探索推荐的内容。研究表明，预测用户打开应用程序的动机有助于提高用户参与度，并提高各种下游任务的性能。然而，准确预测开放应用程序的动机并非易事，因为它受到用户特定因素、搜索查询、点击项以及它们的时间出现的影响。此外，这些活动发生顺序和表现出复杂的时间依赖性。受到神经霍克斯过程(NHP)在序列时间依赖性建模方面的成功启发，提出了一种新的神经霍克斯过程模型来捕捉历史用户浏览和查询操作之间的时间依赖性。该模型被称为基于神经霍克斯过程的开放应用动机预测模型(NHP-OAM) ，采用分层变换器和新颖的强度函数对多个因素进行编码，并使用开放应用动机预测层整合时间和用户特定信息来预测用户的开放应用动机。为了证明我们的 NHP-OAM 模型的优越性，构建开放应用动机预测任务的基准，我们不仅扩展了公共 S 数据集 ZhhuRec，而且构建了一个新的现实世界的开放应用动机数据集(OAMD)。在这两个数据集上的实验验证了 NHP-OAM 算法相对于基线模型的优越性。进一步的下游应用实验证明了 NHP-OAM 在预测用户开放应用动机方面的有效性，突出了 NHP-OAM 的巨大应用价值。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Search+or+to+Recommend:+Predicting+Open-App+Motivation+with+Neural+Hawkes+Process)|0|
|[Counterfactual Ranking Evaluation with Flexible Click Models](https://doi.org/10.1145/3626772.3657810)|Alexander Buchholz, Ben London, Giuseppe Di Benedetto, Jan Malte Lichtenberg, Yannik Stein, Thorsten Joachims|Amazon Music, Berlin, Germany; Amazon Music, Seattle, WA, USA; Amazon Music, Ithaca, NY, NY, USA|Evaluating a new ranking policy using data logged by a previously deployed policy requires a counterfactual (off-policy) estimator that corrects for presentation and selection biases. Some estimators (e.g., the position-based model) perform this correction by making strong assumptions about user behavior, which can lead to high bias if the assumptions are not met. Other estimators (e.g., the item-position model) rely on randomization to avoid these assumptions, but they often suffer from high variance. In this paper, we develop a new counterfactual estimator, called Interpol, that provides a tunable trade-off in the assumptions it makes, thus providing a novel ability to optimize the bias-variance trade-off. We analyze the bias of our estimator, both theoretically and empirically, and show that it achieves lower error than both the position-based model and the item-position model, on both synthetic and real datasets. This improvement in accuracy not only benefits offline evaluation of ranking policies, we also find that Interpol improves learning of new ranking policies when used as the training objective for learning-to-rank.|使用先前部署的策略记录的数据来评估新的排序策略需要一个反事实(非策略)估计器来纠正表示和选择偏差。一些估计量(例如，基于位置的模型)通过对用户行为做出强有力的假设来执行这种修正，如果假设不能满足，就会导致高偏差。其他估计量(例如，项目位置模型)依赖于随机化来避免这些假设，但是它们经常受到高方差的影响。在本文中，我们发展了一个新的反事实估计器，称为国际刑警组织，它提供了一个可调的权衡，它所做的假设，从而提供了一个新的能力，优化偏差-方差权衡。从理论和实证两个方面分析了估计器的误差，结果表明，无论是在合成数据集上还是在实际数据集上，该估计器都比基于位置的模型和项目位置的模型具有更低的误差。这种准确性的提高不仅有利于排序策略的离线评估，我们还发现国际刑警组织在将新的排序策略作为学习排序的培训目标时改善了学习效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Ranking+Evaluation+with+Flexible+Click+Models)|0|
|[Deep Pattern Network for Click-Through Rate Prediction](https://doi.org/10.1145/3626772.3657777)|Hengyu Zhang, Junwei Pan, Dapeng Liu, Jie Jiang, Xiu Li|Tencent; Tsinghua University; Tsinghua University|Click-through rate (CTR) prediction tasks play a pivotal role in real-worldapplications, particularly in recommendation systems and online advertising. Asignificant research branch in this domain focuses on user behavior modeling.Current research predominantly centers on modeling co-occurrence relationshipsbetween the target item and items previously interacted with by users in theirhistorical data. However, this focus neglects the intricate modeling of userbehavior patterns. In reality, the abundance of user interaction recordsencompasses diverse behavior patterns, indicative of a spectrum of habitualparadigms. These patterns harbor substantial potential to significantly enhanceCTR prediction performance. To harness the informational potential within userbehavior patterns, we extend Target Attention (TA) to Target Pattern Attention(TPA) to model pattern-level dependencies. Furthermore, three criticalchallenges demand attention: the inclusion of unrelated items within behaviorpatterns, data sparsity in behavior patterns, and computational complexityarising from numerous patterns. To address these challenges, we introduce theDeep Pattern Network (DPN), designed to comprehensively leverage informationfrom user behavior patterns. DPN efficiently retrieves target-related userbehavior patterns using a target-aware attention mechanism. Additionally, itcontributes to refining user behavior patterns through a pre-training paradigmbased on self-supervised learning while promoting dependency learning withinsparse patterns. Our comprehensive experiments, conducted across three publicdatasets, substantiate the superior performance and broad compatibility of DPN.|点进率(ctrl)预测任务在现实世界的应用程序中扮演着关键角色，特别是在推荐系统和在线广告中。该领域的一个重要研究分支是用户行为建模。目前的研究主要集中在建模共现关系之间的目标项目和项目以前互动的用户在他们的历史数据。然而，这种关注忽略了用户行为模式的复杂建模。实际上，用户交互记录的丰富性包含了不同的行为模式，表明了一系列的习惯范式。这些模式具有显著提高 CTR 预测性能的巨大潜力。为了利用用户行为模式中的信息潜力，我们将目标注意力(TA)扩展到目标模式注意力(TPA) ，以建立模式级别的依赖关系。此外，三个关键的挑战需要注意: 在行为模式中包含不相关的项目，行为模式中的数据稀疏，以及由许多模式引起的计算复杂性。为了应对这些挑战，我们引入了深度模式网络(Deep Pattern Network，DPN) ，它旨在全面利用来自用户行为模式的信息。DPN 使用目标感知注意机制有效地检索与目标相关的用户行为模式。此外，它有助于细化用户的行为模式，通过预训练范式的基础上自我监督学习，同时促进稀疏模式的依赖性学习。我们在三个公共数据集上进行的全面实验证实了 DPN 的优越性能和广泛的兼容性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Pattern+Network+for+Click-Through+Rate+Prediction)|0|
|[AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations](https://doi.org/10.1145/3626772.3657724)|Wei Wu, Chao Wang, Dazhong Shen, Chuan Qin, Liyi Chen, Hui Xiong|University of Science and Technology of China; The Hong Kong University of Science and Technology (Guangzhou); HKUST Fok Ying Tung Research Institute, The Hong Kong University of Science and Technology (Guangzhou); BOSS Zhipin; Shanghai Artificial Intelligence Laboratory|Collaborative filtering methods based on graph neural networks (GNNs) havewitnessed significant success in recommender systems (RS), capitalizing ontheir ability to capture collaborative signals within intricate user-itemrelationships via message-passing mechanisms. However, these GNN-based RSinadvertently introduce excess linear correlation between user and itemembeddings, contradicting the goal of providing personalized recommendations.While existing research predominantly ascribes this flaw to the over-smoothingproblem, this paper underscores the critical, often overlooked role of theover-correlation issue in diminishing the effectiveness of GNN representationsand subsequent recommendation performance. Up to now, the over-correlationissue remains unexplored in RS. Meanwhile, how to mitigate the impact ofover-correlation while preserving collaborative filtering signals is asignificant challenge. To this end, this paper aims to address theaforementioned gap by undertaking a comprehensive study of the over-correlationissue in graph collaborative filtering models. Firstly, we present empiricalevidence to demonstrate the widespread prevalence of over-correlation in thesemodels. Subsequently, we dive into a theoretical analysis which establishes apivotal connection between the over-correlation and over-smoothing issues.Leveraging these insights, we introduce the Adaptive Feature De-correlationGraph Collaborative Filtering (AFDGCF) framework, which dynamically appliescorrelation penalties to the feature dimensions of the representation matrix,effectively alleviating both over-correlation and over-smoothing issues. Theefficacy of the proposed framework is corroborated through extensiveexperiments conducted with four representative graph collaborative filteringmodels across four publicly available datasets.|基于图形神经网络(GNN)的协同过滤方法在推荐系统(RS)中取得了巨大的成功，利用了它们通过消息传递机制在复杂的用户-项目关系中捕获协作信号的能力。然而，这些基于 GNN 的 RSN 无意中在用户和项目嵌入之间引入了过多的线性相关性，与提供个性化推荐的目标相矛盾。虽然现有的研究主要把这个缺陷归因于过度平滑问题，但本文强调了过度相关问题在降低 GNN 表示的有效性和随后的推荐性能方面的关键作用，往往被忽视。到目前为止，过度相关性问题在 RS 中仍然没有得到探讨。同时，如何在保留协同过滤信号的同时减轻过度相关的影响是一个重大挑战。为此，本文旨在通过对图形协同过滤模型中的过度相关问题进行全面研究来弥补上述差距。首先，我们提出的经验证据表明，在这些模型中过度相关的广泛流行。随后，我们深入进行了理论分析，建立了过度相关和过度平滑问题之间的关键联系。利用这些见解，我们引入了自适应特征去相关图协同过滤(AFDGCF)框架，该框架动态地将相关惩罚应用于表示矩阵的特征维度，有效地缓解了过度相关和过度平滑的问题。该框架的有效性通过四个具有代表性的图形协同过滤模型在四个公开数据集上进行的广泛实验得到了证实。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AFDGCF:+Adaptive+Feature+De-correlation+Graph+Collaborative+Filtering+for+Recommendations)|0|
|[TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems](https://doi.org/10.1145/3626772.3657721)|Peiyan Zhang, Yuchen Yan, Xi Zhang, Chaozhuo Li, Senzhang Wang, Feiran Huang, Sunghun Kim|Fuzhou University Interdisciplinary Institute for Medical Engineering; Peking University School of Intelligence Science and Technology; Hong Kong University of Science and Technology; Microsoft Research Asia; Jinan University; Central South University|Graph Neural Networks (GNNs) have emerged as promising solutions forcollaborative filtering (CF) through the modeling of user-item interactiongraphs. The nucleus of existing GNN-based recommender systems involvesrecursive message passing along user-item interaction edges to refine encodedembeddings. Despite their demonstrated effectiveness, current GNN-based methodsencounter challenges of limited receptive fields and the presence of noisy"interest-irrelevant" connections. In contrast, Transformer-based methods excelin aggregating information adaptively and globally. Nevertheless, theirapplication to large-scale interaction graphs is hindered by inherentcomplexities and challenges in capturing intricate, entangled structuralinformation. In this paper, we propose TransGNN, a novel model that integratesTransformer and GNN layers in an alternating fashion to mutually enhance theircapabilities. Specifically, TransGNN leverages Transformer layers to broadenthe receptive field and disentangle information aggregation from edges, whichaggregates information from more relevant nodes, thereby enhancing the messagepassing of GNNs. Additionally, to capture graph structure informationeffectively, positional encoding is meticulously designed and integrated intoGNN layers to encode such structural knowledge into node attributes, thusenhancing the Transformer's performance on graphs. Efficiency considerationsare also alleviated by proposing the sampling of the most relevant nodes forthe Transformer, along with two efficient sample update strategies to reducecomplexity. Furthermore, theoretical analysis demonstrates that TransGNN offersincreased expressiveness compared to GNNs, with only a marginal increase inlinear complexity. Extensive experiments on five public datasets validate theeffectiveness and efficiency of TransGNN.|图形神经网络(GNN)通过对用户项交互图的建模，成为协同过滤(CF)的有效解决方案。现有的基于 GNN 的推荐系统的核心涉及递归消息传递沿用户项交互边缘细化编码解码。尽管已经证明了这些方法的有效性，但是目前基于 GNN 的方法遇到了接受域有限和存在噪声“兴趣无关”连接的挑战。相比之下，基于 Transform- 的方法优于自适应和全局聚合信息。然而，在获取错综复杂、纠缠不清的结构信息方面，它们在大尺度相互作用图中的应用受到了固有的复杂性和挑战性的阻碍。在本文中，我们提出了 TransGNN，一个新颖的模型，集成变压器和 GNN 层在一个交替的方式，以相互增强他们的能力。具体来说，TransGNN 利用 TransGNN 层来扩展接收字段，并从边界中分离信息聚合，从而从更相关的节点聚合信息，从而增强 GNN 的消息传递。此外，为了有效地获取图结构信息，位置编码被精心设计并集成到 GNN 层中，将这些结构知识编码到节点属性中，从而提高了变压器在图上的性能。通过提出变压器最相关节点的抽样，以及两种有效的样本更新策略来降低复杂性，也减轻了效率方面的考虑。此外，理论分析表明，与 GNN 相比，TransGNN 提供了更高的表达能力，只是略微增加了非线性复杂度。通过对五个公共数据集的大量实验验证了 TransGNN 的有效性和高效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransGNN:+Harnessing+the+Collaborative+Power+of+Transformers+and+Graph+Neural+Networks+for+Recommender+Systems)|0|
|[Lightweight Embeddings for Graph Collaborative Filtering](https://doi.org/10.1145/3626772.3657820)|Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, Hongzhi Yin|Shandong University; Hefei University of Technology; The University of Queensland School of Electrical Engineering and Computer Science|Graph neural networks (GNNs) are currently one of the most performantcollaborative filtering methods. Meanwhile, owing to the use of an embeddingtable to represent each user/item as a distinct vector, GNN-based recommendershave inherited the long-standing defect of parameter inefficiency. As a commonpractice for scalable embeddings, parameter sharing enables the use of fewerembedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, mostexisting methods are a heuristically designed, predefined mapping from eachuser's/item's ID to the corresponding meta-embedding indexes, thus simplifyingthe optimization problem into learning only the meta-embeddings. However, inthe context of GNN-based collaborative filtering, such a fixed mapping omitsthe semantic correlations between entities that are evident in the user-iteminteraction graph, leading to suboptimal recommendation performance. To thisend, we propose Lightweight Embeddings for Graph Collaborative Filtering(LEGCF), a parameter-efficient embedding framework dedicated to GNN-basedrecommenders. LEGCF innovatively introduces an assignment matrix as an extralearnable component on top of meta-embeddings. To jointly optimize these twoheavily entangled components, aside from learning the meta-embeddings byminimizing the recommendation loss, LEGCF further performs efficient assignmentupdate by enforcing a novel semantic similarity constraint and finding itsclosed-form solution based on matrix pseudo-inverse. The meta-embeddings andassignment matrix are alternately updated, where the latter is sparsified onthe fly to ensure negligible storage overhead. Extensive experiments on threebenchmark datasets have verified LEGCF's smallest trade-off between size andperformance, with consistent accuracy gain over state-of-the-art baselines. Thecodebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.|图神经网络(GNN)是目前性能最好的协同过滤方法之一。同时，由于使用嵌入表将每个用户/项目表示为一个独立的向量，基于 GNN 的推荐器继承了长期以来参数低效的缺陷。作为可伸缩嵌入的常见实践，参数共享使嵌入向量的使用更少(即元嵌入)。当分配元嵌入时，大多数现有的方法都是启发式设计的，预定义的从每个用户/项目的 ID 到相应元嵌入索引的映射，从而简化了最佳化问题，只学习元嵌入。然而，在基于 GNN 的协同过滤中，这种固定的映射忽略了在用户-项目/交互图中显而易见的实体之间的语义相关性，导致了次优的推荐性能。为此，我们提出了图形协同过滤的轻量级嵌入(legCF) ，这是一个专门针对基于 GNN 的推荐程序的参数高效嵌入框架。LEGCF 在元嵌入的基础上创新地引入了指派矩阵作为可学习的组件。为了联合优化这两个高度纠缠的组件，LEGCF 除了通过最小化推荐丢失来学习元嵌入之外，还通过强制执行一种新的语义相似性约束并基于矩阵伪逆寻找其封闭形式解来进一步执行有效的赋值更新。元嵌入和分配矩阵交替更新，其中后者被动态稀疏化，以确保可以忽略存储开销。在三个基准数据集上的大量实验已经证实了 LEGCF 在大小和性能之间的最小权衡，在最先进的基准上有一致的准确性增益。LEgCF 的代码库有 https://github.com/xurong-liang/LEGCF。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+Embeddings+for+Graph+Collaborative+Filtering)|0|
|[Graded Relevance Scoring of Written Essays with Dense Retrieval](https://doi.org/10.1145/3626772.3657744)|Salam Albatarni, Sohaila Eltanbouly, Tamer Elsayed|Qatar University Computer Science and Engineering Department|Automated Essay Scoring automates the grading process of essays, providing agreat advantage for improving the writing proficiency of students. Whileholistic essay scoring research is prevalent, a noticeable gap exists inscoring essays for specific quality traits. In this work, we focus on therelevance trait, which measures the ability of the student to stay on-topicthroughout the entire essay. We propose a novel approach for graded relevancescoring of written essays that employs dense retrieval encoders. Denserepresentations of essays at different relevance levels then form clusters inthe embeddings space, such that their centroids are potentially separate enoughto effectively represent their relevance levels. We hence use the simple1-Nearest-Neighbor classification over those centroids to determine therelevance level of an unseen essay. As an effective unsupervised dense encoder,we leverage Contriever, which is pre-trained with contrastive learning anddemonstrated comparable performance to supervised dense retrieval models. Wetested our approach on both task-specific (i.e., training and testing on sametask) and cross-task (i.e., testing on unseen task) scenarios using the widelyused ASAP++ dataset. Our method establishes a new state-of-the-art performancein the task-specific scenario, while its extension for the cross-task scenarioexhibited a performance that is on par with the state-of-the-art model for thatscenario. We also analyzed the performance of our approach in a more practicalfew-shot scenario, showing that it can significantly reduce the labeling costwhile sacrificing only 10|自动化论文评分自动化了论文的评分过程，为提高学生的写作水平提供了巨大的优势。虽然整体论文评分研究是普遍存在的，一个明显的差距存在评分论文的具体质量特征。在这项工作中，我们关注的是关联特质，它衡量的是学生在整篇文章中始终保持主题的能力。我们提出了一种新的方法分级相关护航的书面文章，使用密集检索编码器。随后，在嵌入空间中，不同关联水平的密集表示形成聚类，这样它们的质心可能足够分离，以有效地表示它们的关联水平。因此，我们使用这些质心上的简单1-最近邻分类来确定一篇看不见的文章的关联水平。作为一个有效的无监督密集编码器，我们利用捐助者，这是预先训练与对比学习，并显示了可比性能的监督密集检索模型。使用广泛使用的 ASAP + + 数据集，对我们的方法进行了任务特定(即同一任务的培训和测试)和跨任务(即未知任务的测试)场景的测试。我们的方法在任务特定场景中建立了一种新的最先进的表现，而它在跨任务场景中的扩展表现出了与该场景的最先进模型相当的表现。我们还分析了我们的方法在一个更实用的少镜头场景中的性能，表明它可以显著降低标签成本，同时只牺牲10|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graded+Relevance+Scoring+of+Written+Essays+with+Dense+Retrieval)|0|
|[Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models](https://doi.org/10.1145/3626772.3657841)|Catherine Chen, Jack Merullo, Carsten Eickhoff|University of Tübingen; Brown University|Neural models have demonstrated remarkable performance across diverse rankingtasks. However, the processes and internal mechanisms along which theydetermine relevance are still largely unknown. Existing approaches foranalyzing neural ranker behavior with respect to IR properties rely either onassessing overall model behavior or employing probing methods that may offer anincomplete understanding of causal mechanisms. To provide a more granularunderstanding of internal model decision-making processes, we propose the useof causal interventions to reverse engineer neural rankers, and demonstrate howmechanistic interpretability methods can be used to isolate componentssatisfying term-frequency axioms within a ranking model. We identify a group ofattention heads that detect duplicate tokens in earlier layers of the model,then communicate with downstream heads to compute overall document relevance.More generally, we propose that this style of mechanistic analysis opens upavenues for reverse engineering the processes neural retrieval models use tocompute relevance. This work aims to initiate granular interpretability effortsthat will not only benefit retrieval model development and training, butultimately ensure safer deployment of these models.|神经模型在不同的排序任务中表现出了显著的性能。然而，它们决定相关性的过程和内部机制在很大程度上仍然是未知的。现有的方法分析神经排序行为方面的 IR 特性依赖于评估整体模型行为或使用探测方法，可能提供不完整的因果机制的理解。为了提供对内部模型决策过程的更细粒度的理解，我们提出使用因果干预来逆向工程神经排序器，并演示如何使用机械可解释性方法在排序模型中分离满足术语频率公理的组件。我们识别出一组注意头，它们检测模型早期层中的重复标记，然后与下游头通信以计算整个文档的相关性。更一般地说，我们认为这种类型的机械分析为神经检索模型用于计算相关性的过程开辟了逆向工程。这项工作的目的是启动细粒度的可解释性工作，这不仅有利于检索模型的开发和培训，而且最终确保这些模型的更安全的部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Axiomatic+Causal+Interventions+for+Reverse+Engineering+Relevance+Computation+in+Neural+Retrieval+Models)|0|
|[Optimizing Learning-to-Rank Models for Ex-Post Fair Relevance](https://doi.org/10.1145/3626772.3657751)|Sruthi Gorantla, Eshaan Bhansali, Amit Deshpande, Anand Louis|Microsoft; Indian Institute of Science; University of Wisconsin-Madison; Indian Institute of Science Bangalore|Learning-to-rank (LTR) models rank items based on specific features, aiming to maximize ranking utility by prioritizing highly relevant items. However, optimizing only for ranking utility can lead to representational harm and may fail to address implicit bias in relevance scores. Prior studies introduced algorithms to train stochastic ranking models, such as the Plackett-Luce ranking model, that maximize expected ranking utility while achieving fairness in expectation (ex-ante fairness). Still, every sampled ranking may not satisfy group fairness (ex-post fairness). Post-processing methods ensure ex-post fairness; however, the LTR model lacks awareness of this step, creating a mismatch between the objective function the LTR model optimizes and the one it is supposed to optimize. In this paper, we first propose a novel objective where the relevance (or the expected ranking utility) is computed over only those rankings that satisfy given representation constraints for groups of items. We call this the ex-post fair relevance. We then give a framework for training Group-Fair LTR models to maximize our proposed ranking objective. Leveraging an efficient sampler for ex-post group-fair rankings and efficient algorithms to train the Plackett-Luce LTR model, we demonstrate their use in training the Group-Fair Plackett-Luce model in our framework. Experiments on MovieLens and Kiva datasets reveal improved fairness and relevance with our group-fair Plackett-Luce model compared to post-processing. In scenarios with implicit bias, our algorithm generally outperforms existing LTR baselines in both fairness and relevance.|学习排序(LTR)模型根据特定的特征对项目进行排序，目的是通过对高度相关的项目进行优先排序，使排序效用最大化。然而，仅仅为了排名效用而优化可能会导致代表性损害，并且可能无法解决相关性得分中的隐性偏差。先前的研究介绍了训练随机排名模型的算法，例如 Plackett-Luce 排名模型，这种算法在实现预期公平(事前公平)的同时最大化预期排名效用。尽管如此，每个抽样排名可能不满足组公平性(事后公平性)。后处理方法确保了事后公平性，但是 LTR 模型缺乏对这一步骤的意识，使得 LTR 模型优化的目标函数与其应该优化的目标函数不匹配。在本文中，我们首先提出一个新的目标，其中的相关性(或期望的排名效用)是计算只有那些满足给定的表示约束的项目组的排名。我们称之为事后公平相关。然后，我们给出了一个训练集团公平的长期资产负债率模型的框架，以最大限度地提出我们的排名目标。我们利用一个有效的后集团公平排名采样器和有效的算法来训练 Plackett-Luce LTR 模型，在我们的框架中演示了它们在训练集团公平 Plackett-Luce 模型中的应用。在 MovieLens 和 Kiva 数据集上进行的实验表明，与后处理相比，我们的组公平 Plackett-Luce 模型提高了公平性和相关性。在存在隐性偏差的情况下，我们的算法通常在公平性和相关性方面都优于现有的 LTR 基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Learning-to-Rank+Models+for+Ex-Post+Fair+Relevance)|0|
|[Scaling Sequential Recommendation Models with Transformers](https://doi.org/10.1145/3626772.3657816)|Pablo Zivic, Hernán Ceferino Vázquez, Jorge Sánchez|Mercado Libre Inc., Córdoba, Argentina; Mercado Libre Inc., Buenos Aires, Argentina|Modeling user preferences has been mainly addressed by looking at users' interaction history with the different elements available in the system. Tailoring content to individual preferences based on historical data is the main goal of sequential recommendation. The nature of the problem, as well as the good performance observed across various domains, has motivated the use of the transformer architecture, which has proven effective in leveraging increasingly larger amounts of training data when accompanied by an increase in the number of model parameters. This scaling behavior has brought a great deal of attention, as it provides valuable guidance in the design and training of even larger models. Taking inspiration from the scaling laws observed in training large language models, we explore similar principles for sequential recommendation. Addressing scalability in this context requires special considerations as some particularities of the problem depart from the language modeling case. These particularities originate in the nature of the content catalogs, which are significantly larger than the vocabularies used for language and might change over time. In our case, we start from a well-known transformer-based model from the literature and make two crucial modifications. First, we pivot from the traditional representation of catalog items as trainable embeddings to representations computed with a trainable feature extractor, making the parameter count independent of the number of items in the catalog. Second, we propose a contrastive learning formulation that provides us with a better representation of the catalog diversity. We demonstrate that, under this setting, we can train our models effectively on increasingly larger datasets under a common experimental setup. We use the full Amazon Product Data dataset, which has only been partially explored in other studies, and reveal scaling behaviors similar to those found in language models. Compute-optimal training is possible but requires a careful analysis of the compute-performance trade-offs specific to the application. We also show that performance scaling translates to downstream tasks by fine-tuning larger pre-trained models on smaller task-specific domains. Our approach and findings provide a strategic roadmap for model training and deployment in real high-dimensional preference spaces, facilitating better training and inference efficiency. We hope this paper bridges the gap between the potential of transformers and the intrinsic complexities of high-dimensional sequential recommendation in real-world recommender systems. Code and models can be found at https://github.com/mercadolibre/srt.|建模用户偏好主要通过查看用户的交互历史和系统中可用的不同元素来解决。根据历史数据为个人偏好定制内容是顺序推荐的主要目标。这一问题的性质以及在各个领域观察到的良好性能促使使用了变压器结构，事实证明，在模型参数数量增加的同时，变压器结构有效地利用了越来越多的培训数据。这种缩放行为引起了广泛的关注，因为它为更大型模型的设计和训练提供了有价值的指导。从训练大型语言模型中观察到的比例规律中获得启发，我们探索了顺序推荐的相似原则。在这种情况下处理可伸缩性需要特别的考虑，因为问题的一些特殊性与语言建模情况不同。这些特殊性源于内容目录的性质，它们明显大于用于语言的词汇表，并且可能随着时间的推移而变化。在我们的例子中，我们从文献中的一个著名的基于变压器的模型开始，并进行了两个关键的修改。首先，我们将目录项的传统表示从可训练的嵌入转向使用可训练的特征提取器计算的表示，使得参数计数独立于目录项的数量。其次，我们提出了一个对比学习公式，为我们提供了一个更好的表示目录多样性。我们证明，在这种设置下，我们可以在一个通用的实验设置下，在越来越大的数据集上有效地训练我们的模型。我们使用完整的亚马逊产品数据集，这只是在其他研究中部分探索，并揭示了类似于在语言模型中发现的缩放行为。计算优化训练是可能的，但是需要对特定于应用程序的计算性能权衡进行仔细分析。我们还表明，通过在较小的特定于任务的领域上微调较大的预先训练的模型，性能伸缩转化为下游任务。我们的方法和研究结果提供了一个战略路线图的模型训练和部署在真正的高维偏好空间，促进更好的训练和推理效率。我们希望本文能够弥补现实推荐系统中变压器的潜力和高维顺序推荐的内在复杂性之间的差距。代码和模型可以在 https://github.com/mercadolibre/srt 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Sequential+Recommendation+Models+with+Transformers)|0|
|[SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation](https://doi.org/10.1145/3626772.3657716)|Yuxi Liu, Lianghao Xia, Chao Huang|; University of Hong Kong|Sequential recommendation effectively addresses information overload bymodeling users' temporal and sequential interaction patterns. To overcome thelimitations of supervision signals, recent approaches have adoptedself-supervised learning techniques in recommender systems. However, there arestill two critical challenges that remain unsolved. Firstly, existingsequential models primarily focus on long-term modeling of individualinteraction sequences, overlooking the valuable short-term collaborativerelationships among the behaviors of different users. Secondly, real-world dataoften contain noise, particularly in users' short-term behaviors, which canarise from temporary intents or misclicks. Such noise negatively impacts theaccuracy of both graph and sequence models, further complicating the modelingprocess. To address these challenges, we propose a novel framework calledSelf-Supervised Graph Neural Network (SelfGNN) for sequential recommendation.The SelfGNN framework encodes short-term graphs based on time intervals andutilizes Graph Neural Networks (GNNs) to learn short-term collaborativerelationships. It captures long-term user and item representations at multiplegranularity levels through interval fusion and dynamic behavior modeling.Importantly, our personalized self-augmented learning structure enhances modelrobustness by mitigating noise in short-term graphs based on long-term userinterests and personal stability. Extensive experiments conducted on fourreal-world datasets demonstrate that SelfGNN outperforms variousstate-of-the-art baselines. Our model implementation codes are available athttps://github.com/HKUDS/SelfGNN.|顺序推荐通过建模用户的时间和顺序交互模式有效地解决了信息超载问题。为了克服监督信号的局限性，最近的方法在推荐系统中采用了自监督学习技术。然而，仍然有两个关键的挑战没有得到解决。首先，现有的序贯模型主要侧重于个体交互序列的长期建模，忽视了不同用户行为之间有价值的短期协作关系。其次，真实世界的数据往往包含噪音，特别是在用户的短期行为，这可能是由于临时意图或错误点击。这种噪声对图模型和序列模型的精度都有负面影响，使得建模过程更加复杂。为了应对这些挑战，我们提出了一个新的框架，称为自我监督图神经网络(SelfGNN)的顺序推荐。自组织神经网络(SelfGNN)框架根据时间间隔对短期图形进行编码，并利用图形神经网络(GNN)学习短期协作关系。它通过区间融合和动态行为建模，在多粒度级别捕获长期用户和项目表示。重要的是，我们的个性化自增强学习结构通过减少基于长期用户兴趣和个人稳定性的短期图中的噪声来增强模型鲁棒性。在四个真实世界数据集上进行的大量实验表明，SelfGNN 的性能优于各种最先进的基线。我们的模型实现代码可以通过 https:// github.com/hkuds/selfgnn 获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SelfGNN:+Self-Supervised+Graph+Neural+Networks+for+Sequential+Recommendation)|0|
|[Revisit Targeted Model Poisoning on Federated Recommendation: Optimize via Multi-objective Transport](https://doi.org/10.1145/3626772.3657764)|Jiajie Su, Chaochao Chen, Weiming Liu, Zibin Lin, Shuheng Shen, Weiqiang Wang, Xiaolin Zheng|Zhejiang University, Hangzhou, China; Ant Group, Hangzhou, China|Federated Recommendation (FedRec) is popularly investigated in personalized recommenders for preserving user privacy. However, due to the distributed training paradigm, FedRec is vulnerable to model poisoning attacks. In this paper, we focus on the targeted model poisoning attack against FedRec, which aims at effectively attacking the FedRec via uploading poisoned gradients to raise the exposure ratio of a multi-target item set. Previous attack methods excel with fewer target items but suffer performance decline as the amount of target items increases, which reveals two perennially neglected issues: (i) The simple promotion of prediction scores without considering intrinsic collaborations between users and items is ineffective in multi-target cases. (ii) Target items are heterogeneous, which requires discriminative attacking users and strategies for different targets. To address the issues, we propose a novel Heterogeneous Multi-target Transfer Attack framework named HMTA which consists of two stages, i.e., (1) diverse user agent generation and (2) optimal multi-target transport attack. The former stage leverages collaboration-aware manifold learning to extract latent associations among users and items, and develops a differentiable contrastive sorting to generate user agents from both difficulty and diversity scale. The latter stage conducts poisoning in a fine-grained and distinguishing way, which first completes distribution mapping from target items to generated user agents and then achieves a hybrid multi-target attack. Extensive experiments on benchmark datasets demonstrate the effectiveness of HMTA.|为了保护用户隐私，联邦推荐(FedRec)在个性化推荐中得到了广泛的应用。然而，由于分布式训练范例，FedRec 容易受到模型中毒攻击。本文研究了针对 FedRec 的目标模型中毒攻击，目的是通过上传中毒梯度来有效地攻击 FedRec，以提高多目标项目集的暴露率。以往的攻击方法优于较少的目标项目，但随着目标项目数量的增加性能下降，这揭示了两个长期被忽视的问题: (i)在多目标情况下，不考虑用户和项目之间的内在协作的简单预测得分的提升是无效的。(ii)目标项是异构的，需要针对不同目标的区分性攻击用户和策略。针对这一问题，提出了一种新的异构多目标传输攻击框架 HMTA，该框架由两个阶段组成，即(1)多用户代理生成阶段和(2)最优多目标传输攻击阶段。前一阶段利用协作感知流形学习来提取用户和项目之间的潜在关联，并开发了一种可微对比排序方法来从难度和多样性尺度生成用户代理。后一阶段采用细粒度和区分的方式进行中毒，首先完成目标项到生成用户代理的分布映射，然后实现混合多目标攻击。在基准数据集上的大量实验证明了 HMTA 算法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisit+Targeted+Model+Poisoning+on+Federated+Recommendation:+Optimize+via+Multi-objective+Transport)|0|
|[LoRec: Combating Poisons with Large Language Model for Robust Sequential Recommendation](https://doi.org/10.1145/3626772.3657684)|Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng|Institute of Computing Technology, CAS|Sequential recommender systems stand out for their ability to capture users' dynamic interests and the patterns of item transitions. However, the inherent openness of sequential recommender systems renders them vulnerable to poisoning attacks, where fraudsters are injected into the training data to manipulate learned patterns. Traditional defense methods predominantly depend on predefined assumptions or rules extracted from specific known attacks, limiting their generalizability to unknown attacks. To solve the above problems, considering the rich open-world knowledge encapsulated in Large Language Models (LLMs), we attempt to introduce LLMs into defense methods to broaden the knowledge beyond limited known attacks. We propose LoRec, an innovative framework that employs LLM-Enhanced Calibration to strengthen the robustness of sequential Recommender systems against poisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that refines the training process of sequential recommender systems with knowledge derived from LLMs, applying a user-wise reweighting to diminish the impact of attacks. Incorporating LLMs' open-world knowledge, the LCT effectively converts the limited, specific priors or rules into a more general pattern of fraudsters, offering improved defenses against poisons. Our comprehensive experiments validate that LoRec, as a general framework, significantly strengthens the robustness of sequential recommender systems.|顺序推荐系统因其捕捉用户动态兴趣和项目转换模式的能力而脱颖而出。然而，顺序推荐系统固有的开放性使它们容易受到中毒攻击，欺诈者被注入培训数据以操纵学到的模式。传统的防御方法主要依赖于预定义的假设或从特定的已知攻击中提取的规则，限制了它们对未知攻击的普遍性。为了解决上述问题，考虑到大语言模型(LLM)中包含的丰富的开放世界知识，我们尝试将 LLM 引入防御方法中，以扩展已知攻击范围之外的知识。我们提出 LoRec，一个创新的框架，使用 LLM 增强校准，以加强顺序推荐系统对中毒攻击的健壮性。LoRec 集成了一个 LLM 增强的 CalibraTor (LCT) ，它利用从 LLM 获得的知识完善了顺序推荐系统的训练过程，应用了用户明智的重新加权来减少攻击的影响。结合 LLM 的开放世界知识，LCT 有效地将有限的、特定的前科或规则转化为更普遍的欺诈者模式，提供了针对有毒物质的更好的防御。我们的综合实验验证了 LoRec 作为一个通用框架，显著增强了顺序推荐系统的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LoRec:+Combating+Poisons+with+Large+Language+Model+for+Robust+Sequential+Recommendation)|0|
|[Treatment Effect Estimation for User Interest Exploration on Recommender Systems](https://doi.org/10.1145/3626772.3657736)|Jiaju Chen, Wenjie Wang, Chongming Gao, Peng Wu, Jianxiong Wei, Qingsong Hua|Meituan; University of Science and Technology of China; National University of Singapore; Beijing Technology and Business University|Recommender systems learn personalized user preferences from user feedback like clicks. However, user feedback is usually biased towards partially observed interests, leaving many users' hidden interests unexplored. Existing approaches typically mitigate the bias, increase recommendation diversity, or use bandit algorithms to balance exploration-exploitation trade-offs. Nevertheless, they fail to consider the potential rewards of recommending different categories of items and lack the global scheduling of allocating top-N recommendations to categories, leading to suboptimal exploration. In this work, we propose an Uplift model-based Recommender (UpliftRec) framework, which regards top-N recommendation as a treatment optimization problem. UpliftRec estimates the treatment effects, i.e., the click-through rate (CTR) under different category exposure ratios, by using observational user feedback. UpliftRec calculates group-level treatment effects to discover users' hidden interests with high CTR rewards and leverages inverse propensity weighting to alleviate confounder bias. Thereafter, UpliftRec adopts a dynamic programming method to calculate the optimal treatment for overall CTR maximization. We implement UpliftRec on different backend models and conduct extensive experiments on three datasets. The empirical results validate the effectiveness of UpliftRec in discovering users' hidden interests while achieving superior recommendation accuracy.|推荐系统通过用户反馈(如点击)学习个性化用户偏好。然而，用户反馈通常偏向于部分观察到的兴趣，使得许多用户的隐藏兴趣未被探索。现有的方法通常会减轻偏差，增加推荐多样性，或者使用盗贼算法来平衡勘探与开发的权衡。然而，他们没有考虑推荐不同类别项目的潜在回报，缺乏将前 N 项推荐分配给类别的全球时间表，导致次优探索。在这项工作中，我们提出了一个基于 UpliftRec (UpliftRec)模型的推荐框架，它将 top-N 推荐作为一个治疗最佳化问题。UpliftRec 通过观察用户反馈来估计治疗效果，即不同类别暴露比例下的点进率。UpliftRec 计算组级治疗效果，以发现用户的高点击率奖励隐藏的兴趣，并利用反倾向加权，以减轻混杂偏见。然后，UpliftRec 采用动态规划方法来计算总体 CTR 最大化的最优处理。我们在不同的后端模型上实现 UpliftRec，并在三个数据集上进行了广泛的实验。实证结果验证了 UpliftRec 在发现用户隐藏兴趣的同时达到更高的推荐准确率的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Treatment+Effect+Estimation+for+User+Interest+Exploration+on+Recommender+Systems)|0|
|[Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization](https://doi.org/10.1145/3626772.3657705)|Pancheng Wang, Shasha Li, Dong Li, Kehan Long, Jintao Tang, Ting Wang|National University of Defense Technology|Automatically condensing multiple topic-related scientific papers into asuccinct and concise summary is referred to as Multi-Document ScientificSummarization (MDSS). Currently, while commonly used abstractive MDSS methodscan generate flexible and coherent summaries, the difficulty in handling globalinformation and the lack of guidance during decoding still make it challengingto generate better summaries. To alleviate these two shortcomings, this paperintroduces summary candidates into MDSS, utilizing the global information ofthe document set and additional guidance from the summary candidates to guidethe decoding process. Our insights are twofold: Firstly, summary candidates canprovide instructive information from both positive and negative perspectives,and secondly, selecting higher-quality candidates from multiple optionscontributes to producing better summaries. Drawing on the insights, we proposea summary candidates fusion framework – Disentangling Instructive informationfrom Ranked candidates (DIR) for MDSS. Specifically, DIR first uses aspecialized pairwise comparison method towards multiple candidates to pick outthose of higher quality. Then DIR disentangles the instructive information ofsummary candidates into positive and negative latent variables with ConditionalVariational Autoencoder. These variables are further incorporated into thedecoder to guide generation. We evaluate our approach with three differenttypes of Transformer-based models and three different types of candidates, andconsistently observe noticeable performance improvements according to automaticand human evaluation. More analyses further demonstrate the effectiveness ofour model in handling global information and enhancing decodingcontrollability.|将多篇与主题相关的科学论文自动压缩成简洁、简洁的摘要，称为多文档科学摘要(MDSS)。目前，虽然常用的抽象 MDSS 方法可以生成灵活和连贯的摘要，但全球信息处理的困难和解码过程中缺乏指导仍然使得生成更好的摘要具有挑战性。为了克服这两个缺点，本文将摘要候选集引入 MDSS，利用文档集的全局信息和摘要候选集的额外指导来指导解码过程。我们的见解是双重的: 首先，总结候选人可以从正面和负面的角度提供有益的信息，其次，从多个选项中选择更高质量的候选人有助于产生更好的总结。在此基础上，我们提出了一个综合候选人融合框架——从排名候选人(DIR)中分离指导性信息，用于 MDSS。具体来说，DIR 首先对多个候选人使用专门的成对比较方法来挑选那些质量较高的候选人。然后用条件变分自动编码器将总结候选人的指导信息分解为正变量和负变量。这些变量进一步合并到解码器中以指导生成。我们评估我们的方法与三个不同类型的变压器为基础的模型和三个不同类型的候选人，并一致地观察显着的性能改善，根据自动和人工评估。更多的分析进一步证明了该模型在处理全局信息和提高译码可控性方面的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangling+Instructive+Information+from+Ranked+Multiple+Candidates+for+Multi-Document+Scientific+Summarization)|0|
|[Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check](https://doi.org/10.1145/3626772.3657980)|Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He|East China Normal University|Retrieval-Augmented Generation (RAG) aims to generate more reliable andaccurate responses, by augmenting large language models (LLMs) with theexternal vast and dynamic knowledge. Most previous work focuses on using RAGfor single-round question answering, while how to adapt RAG to the complexconversational setting wherein the question is interdependent on the precedingcontext is not well studied. In this paper, we propose a conversation-level RAGapproach, which incorporates fine-grained retrieval augmentation and self-checkfor conversational question answering (CQA). In particular, our approachconsists of three components, namely conversational question refiner,fine-grained retriever and self-check based response generator, which workcollaboratively for question understanding and relevant information acquisitionin conversational settings. Extensive experiments demonstrate the greatadvantages of our approach over the state-of-the-art baselines. Moreover, wealso release a Chinese CQA dataset with new features including reformulatedquestion, extracted keyword, retrieved paragraphs and their helpfulness, whichfacilitates further researches in RAG enhanced CQA.|检索增强生成(RAG)旨在通过扩充大型语言模型(LLM)和外部海量动态知识来产生更可靠、更准确的响应。以往的研究主要集中在 RAG 在单轮问答中的应用，而如何将 RAG 应用到复杂的会话环境中，使问题相互依赖于前面的上下文，这方面的研究还不多。在本文中，我们提出了一种会话级的 RAG 方法，该方法结合了细粒度检索增强和会话问题回答(CQA)的自我检查。特别是，我们的方法由三个部分组成，即会话问题细化，细粒度检索和基于自我检查的响应生成器，它们协同工作的问题理解和相关信息的获取在会话环境中。大量的实验证明了我们的方法相对于最先进的基线的巨大优势。此外，我们还发布了一个中文 CQA 数据集，该数据集具有重构问题、提取关键词、检索段落及其有用性等新特征，为进一步研究 RAG 增强 CQA 提供了方便。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boosting+Conversational+Question+Answering+with+Fine-Grained+Retrieval-Augmentation+and+Self-Check)|0|
|[Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?](https://doi.org/10.1145/3626772.3657979)|Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky|Google Research; University of Waterloo|Query expansion has been widely used to improve the search results offirst-stage retrievers, yet its influence on second-stage, cross-encoderrankers remains under-explored. A recent work of Weller et al. [44] shows thatcurrent expansion techniques benefit weaker models such as DPR and BM25 butharm stronger rankers such as MonoT5. In this paper, we re-examine thisconclusion and raise the following question: Can query expansion improvegeneralization of strong cross-encoder rankers? To answer this question, wefirst apply popular query expansion methods to state-of-the-art cross-encoderrankers and verify the deteriorated zero-shot performance. We identify twovital steps for cross-encoders in the experiment: high-quality keywordgeneration and minimal-disruptive query modification. We show that it ispossible to improve the generalization of a strong neural ranker, by promptengineering and aggregating the ranking results of each expanded query viafusion. Specifically, we first call an instruction-following language model togenerate keywords through a reasoning chain. Leveraging self-consistency andreciprocal rank weighting, we further combine the ranking results of eachexpanded query dynamically. Experiments on BEIR and TREC Deep Learning2019/2020 show that the nDCG@10 scores of both MonoT5 and RankT5 followingthese steps are improved, which points out a direction for applying queryexpansion to strong cross-encoder rankers.|查询扩展已被广泛用于改善第一阶段检索者的搜索结果，但其对第二阶段交叉编码者的影响尚未得到充分研究。Weller 等人最近的一项工作[44]表明，目前的扩展技术有利于较弱的模型，如 DPR 和 BM25，但损害较强的排名，如 MonoT5。在本文中，我们重新审视这个结论，并提出以下问题: 查询扩展能否改善强交叉编码器排序器的泛化？为了回答这个问题，我们首先将流行的查询扩展方法应用于最先进的交叉编码器，并验证恶化的零射击性能。在实验中，我们确定了交叉编码器的两个重要步骤: 高质量的关键字生成和最小干扰的查询修改。我们表明，通过提示工程和聚合每个扩展查询的排序结果，提高一个强神经排序器的泛化是可能的。具体来说，我们首先调用一个指令遵循语言模型来通过一个推理链生成关键字。利用自相容性和相互排序加权，进一步动态组合每个扩展查询的排序结果。BEIR 和 TREC Deep Learning2019/2020的实验表明，遵循这些步骤的 MonoT5和 RankT5的 nDCG@10分数得到了改善，这为将查询扩展应用于强交叉编码器排序器指明了方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Query+Expansion+Improve+Generalization+of+Strong+Cross-Encoder+Rankers?)|0|
|[EASE-DR: Enhanced Sentence Embeddings for Dense Retrieval](https://doi.org/10.1145/3626772.3657925)|Xixi Zhou, Yang Gao, Xin Jie, Xiaoxu Cai, Jiajun Bu, Haishuai Wang|Zhejiang University, Hangzhou, China|Recent neural information retrieval models using dense text representations generated by pre-trained models commonly face two issues. First, a pre-trained model (e.g., BERT) usually truncates a long document before giving its representation, which may cause the loss of some important semantic information. Second, although pre-training models like BERT have been widely used in generating sentence embeddings, a substantial body of literature has shown that the pre-training models often represent sentence embeddings in a homogeneous and narrow space, known as the problem of representation anisotropy, which hurts the quality of dense vector retrieval. In this paper, we split the query and the document in information retrieval into two sets of natural sentences and generate their sentence embeddings with BERT, the most popular pre-trained model. Before aggregating the sentence embeddings to get the entire embedding representations of the input query and document, to alleviate the usual representation degeneration problem of sentence embeddings from BERT, we sample the variational auto-encoder's latent space distribution to obtain isotropic sentence embeddings and utilize supervised contrastive learning to uniform the distribution of these sentence embeddings in the representation space. Our proposed model undergoes training optimization for both the query and the document in the abovementioned aspects. Our model performs well in evaluating three extensively researched neural information retrieval datasets.|最近的神经信息检索模型使用由预先训练的模型产生的密集文本表示，通常面临两个问题。首先，预先训练的模型(例如 BERT)在给出表示之前通常会截断一个长文档，这可能会导致一些重要语义信息的丢失。其次，尽管像 BERT 这样的预训练模型已经被广泛应用于生成句子嵌入，但是大量的文献表明，预训练模型往往代表同质和狭窄空间中的句子嵌入，即所谓的表示各向异性问题，这损害了密集向量检索的质量。在本文中，我们将查询和信息检索中的文档分成两组自然句子，并使用 BERT (最流行的预训练模型)生成它们的句子嵌入。在对输入查询和文档的句子嵌入进行聚合得到完整的嵌入表示之前，为了缓解 BERT 中常见的句子嵌入表示退化问题，采用变分自动编码器的潜空间分布来获得各向同性的句子嵌入，并利用监督对比学习来统一这些句子嵌入在表示空间中的分布。我们提出的模型在上述方面对查询和文档进行了训练优化。我们的模型在评估三个广泛研究的神经信息检索数据集方面表现良好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EASE-DR:+Enhanced+Sentence+Embeddings+for+Dense+Retrieval)|0|
|[Explainable Uncertainty Attribution for Sequential Recommendation](https://doi.org/10.1145/3626772.3657900)|Carles Balsells Rodas, Fan Yang, Zhishen Huang, Yan Gao|Imperial College London, London, United Kingdom; Amazon.com Inc, Seattle, WA, USA|Sequential recommendation systems suggest products based on users' historical behaviours. The inherent sparsity of user-item interactions in a vast product space often leads to unreliable recommendations. Recent research addresses this challenge by leveraging auxiliary product relations to mitigate recommendation uncertainty, and quantifying uncertainty in recommendation scores to modify the candidates selection. However, such approaches may not be efficient due to the requirement of additional side information or providing suboptimal recommendations. To enhance sequential recommendation performance by leveraging uncertainty information, we introduce Explainable Uncertainty Attribution (ExUA). We employ gradient-based saliency attribution to identify sources of uncertainty stemming from sequential interactions. Experimental findings on Amazon and MovieLens datasets demonstrate ExUA's effectiveness in identifying interactions that induce uncertainty, resulting in a 6%+ improvement in NDCG@20 scores when the uncertainty information is integrated into a post-hoc training phase.|连续推荐系统根据用户的历史行为推荐产品。在广阔的产品空间中，用户项交互的固有稀疏性常常导致不可靠的建议。最近的研究通过利用辅助产品关系来减少推荐的不确定性，并量化推荐分数的不确定性来修改候选人的选择，从而解决了这一挑战。然而，由于需要额外的辅助信息或提供次优的建议，这种方法可能不是有效的。为了利用不确定性信息提高序贯推荐的性能，我们引入了可解释的不确定性归因(ExUA)。我们使用基于梯度的显著性归因来识别源于序列相互作用的不确定性。Amazon 和 MovieLens 数据集上的实验结果证明了 ExUA 在识别诱导不确定性的相互作用方面的有效性，当不确定性信息被整合到事后训练阶段时，导致 NDCG@20分数提高6% + 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Uncertainty+Attribution+for+Sequential+Recommendation)|0|
|[FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction](https://doi.org/10.1145/3626772.3657941)|Wentao Ouyang, Rui Dong, Ri Tao, Xiangzheng Liu|Alibaba Group, Beijing, China|Click-through rate (CTR) prediction plays an important role in online advertising platforms. Most existing methods use data from the advertising platform itself for CTR prediction. As user behaviors also exist on many other platforms, e.g., media platforms, it is beneficial to further exploit such complementary information for better modeling user interest and for improving CTR prediction performance. However, due to privacy concerns, data from different platforms cannot be uploaded to a server for centralized model training. Vertical federated learning (VFL) provides a possible solution which is able to keep the raw data on respective participating parties and learn a collaborative model in a privacy-preserving way. However, traditional VFL methods only utilize aligned data with common keys across parties, which strongly restricts their application scope. In this paper, we propose FedUD, which is able to exploit unaligned data, in addition to aligned data, for more accurate federated CTR prediction. FedUD contains two steps. In the first step, FedUD utilizes aligned data across parties like traditional VFL, but it additionally includes a knowledge distillation module. This module distills useful knowledge from the guest party's high-level representations and guides the learning of a representation transfer network. In the second step, FedUD applies the learned knowledge to enrich the representations of the host party's unaligned data such that both aligned and unaligned data can contribute to federated model training. Experiments on two real-world datasets demonstrate the superior performance of FedUD for federated CTR prediction.|点进率预测在在线广告平台中扮演着重要的角色。大多数现有的方法使用来自广告平台本身的数据进行点击率预测。由于用户行为也存在于许多其他平台上，如媒体平台，因此进一步利用这些互补信息有利于更好地建立用户兴趣模型和提高 CTR 预测性能。然而，由于隐私问题，不同平台的数据不能上传到服务器进行集中的模型培训。垂直联邦学习(VFL)提供了一种可能的解决方案，它能够保存各参与方的原始数据，并以保护隐私的方式学习协作模型。然而，传统的 VFL 方法只利用跨各方公共密钥的对齐数据，这严重限制了它们的应用范围。在本文中，我们提出了 FedUD，它能够利用未对齐的数据，除了对齐的数据，更准确的联邦点击率预测。FedUD 包含两个步骤。在第一个步骤中，FedUD 利用传统 VFL 等各方之间的对齐数据，但是它还包括一个知识提取模块。该模块从客方的高层次表示中提取有用的知识，并指导表示传递网络的学习。在第二步中，FedUD 应用所学到的知识来丰富主机方的未对齐数据的表示，这样对齐和未对齐的数据都可以有助于联邦模型的训练。在两个实际数据集上的实验表明，FedUD 在联邦 CTR 预测方面具有优越的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedUD:+Exploiting+Unaligned+Data+for+Cross-Platform+Federated+Click-Through+Rate+Prediction)|0|
|[Generalizable Tip-of-the-Tongue Retrieval with LLM Re-ranking](https://doi.org/10.1145/3626772.3657917)|Luís Borges, Rohan Jha, Jamie Callan, Bruno Martins|The University of Texas at Austin, Austin, Texas, USA; Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; Instituto Superior Técnico and INESC-ID, Lisbon, Portugal|Tip-of-the-Tongue (ToT) retrieval is challenging for search engines because the queries are usually natural-language, verbose, and contain uncertain and inaccurate information. This paper studies the generalization capabilities of existing retrieval methods with ToT queries in multiple domains. We curate a multi-domain dataset and evaluate the effectiveness of recall-oriented first-stage retrieval methods across the different domains, considering in-domain, out-of-domain, and multi-domain training settings. We further explore the use of a Large Language Model (LLM), i.e. GPT-4, for zero-shot re-ranking in various ToT domains, relying solely on the item titles. Results show that multi-domain training enhances recall, and that LLMs are strong zero-shot re-rankers, especially for popular items, outperforming direct GPT-4 prompting without first-stage retrieval. Datasets and code can be found on GitHub https://github.com/LuisPB7/TipTongue|舌尖检索(ToT)对于搜索引擎来说是一个挑战，因为查询通常是自然语言的，冗长的，并且包含不确定和不准确的信息。本文研究了现有的多领域 ToT 查询检索方法的泛化能力。我们策划一个多领域的数据集，并评估面向召回的第一阶段检索方法在不同领域的有效性，考虑域内，域外和多领域的训练设置。我们进一步探索了大型语言模型(LLM)的使用，即 GPT-4，用于在各种 ToT 域中重新排序，仅仅依赖于项目标题。结果表明，多领域训练有助于提高记忆力，LLM 具有较强的零击重排能力，尤其是对于热门项目，其表现优于没有第一阶段提取的直接 GPT-4提示。数据集和代码可以在 gitHub  https://GitHub.com/luispb7/tiptongue 上找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizable+Tip-of-the-Tongue+Retrieval+with+LLM+Re-ranking)|0|
|[Grasping Both Query Relevance and Essential Content for Query-focused Summarization](https://doi.org/10.1145/3626772.3657958)|Ye Xiong, Hidetaka Kamigaito, Soichiro Murakami, Peinan Zhang, Hiroya Takamura, Manabu Okumura|Tokyo Institute of Technology, Tokyo, Japan; CyberAgent, Inc., Tokyo, Japan|Numerous effective methods have been developed to improve query-focused summarization (QFS) performance, e.g., pre-trained model-based and query-answer relevance-based methods. However, these methods still suffer from missing or redundant information due to the inability to capture and effectively utilize the interrelationship between the query and the source document, as well as between the source document and its generated summary, resulting in the summary being unable to answer the query or containing additional unrequired information. To mitigate this problem, we propose an end-to-end hierarchical two-stage summarization model, that first predicts essential content, and then generates a summary by emphasizing the predicted important sentences while maintaining separate encodings for the query and the source, so that it can comprehend not only the query itself but also the essential information in the source. We evaluated the proposed model on two QFS datasets, and the results indicated its overall effectiveness and that of each component.|为了提高查询聚焦摘要(QFS)的性能，已经开发了许多有效的方法，例如基于预训练模型的方法和基于查询-回答相关性的方法。然而，由于无法捕获和有效利用查询与源文档之间以及源文档与其生成的摘要之间的相互关系，这些方法仍然存在信息缺失或多余的问题，导致摘要无法回答查询或包含额外的不必要信息。为了解决这个问题，我们提出了一种端到端的层次化两阶段摘要模型，该模型首先对重要内容进行预测，然后通过强调预测的重要句子来生成摘要，同时对查询和源代码保持单独的编码，从而不仅能够理解查询本身，而且能够理解源代码中的重要信息。我们在两个 QFS 数据集上对所提出的模型进行了评估，结果表明了该模型的整体有效性和每个组件的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grasping+Both+Query+Relevance+and+Essential+Content+for+Query-focused+Summarization)|0|
|[MoME: Mixture-of-Masked-Experts for Efficient Multi-Task Recommendation](https://doi.org/10.1145/3626772.3657922)|Jiahui Xu, Lu Sun, Dengji Zhao|ShanghaiTech University, Shanghai, China|Multi-task learning techniques have attracted great attention in recommendation systems because they can meet the needs of modeling multiple perspectives simultaneously and improve recommendation performance. As promising multi-task recommendation system models, Mixture-of-Experts (MoE) and related methods use an ensemble of expert sub-networks to improve generalization and have achieved significant success in practical applications. However, they still face key challenges in efficient parameter sharing and resource utilization, especially when they are applied to real-world datasets and resource-constrained devices. In this paper, we propose a novel framework called Mixture-of-Masked-Experts (MoME) to address the challenges. Unlike MoE, expert sub-networks in MoME are extracted from an identical over-parameterized base network by learning binary masks. It utilizes a binary mask learning mechanism composed of neuron-level model masking and weight-level expert masking to achieve coarse-grained base model pruning and fine-grained expert pruning, respectively. Compared to existing MoE-based models, MoME achieves efficient parameter sharing and requires significantly less sub-network storage since it actually only trains a base network and a mixture of partially overlapped binary expert masks. Experimental results on real-world datasets demonstrate the superior performance of MoME in terms of recommendation accuracy and computational efficiency. Our code is available at https://https://github.com/Xjh0327/MoME.|多任务学习技术能够满足同时建立多视角模型的需要，提高推荐系统的性能，因而受到推荐系统的广泛关注。专家混合推荐系统作为一种有前途的多任务推荐系统模型，利用专家子网络集成技术提高推荐系统的泛化能力，在实际应用中取得了显著的成功。然而，它们在有效的参数共享和资源利用方面仍然面临着关键的挑战，特别是当它们应用于真实世界的数据集和资源受限的设备时。在本文中，我们提出了一个新的框架，称为蒙版专家混合(MoME) ，以解决这一挑战。与 MoE 不同的是，MoME 中的专家子网络是通过学习二进制掩码从相同的过参数化基网络中提取出来的。该方法利用神经元级模型掩蔽和权重级专家掩蔽组成的二元掩蔽学习机制，分别实现了粗粒度基模型和细粒度专家模型的修剪。与现有的基于 MoE 的模型相比，MoME 实现了有效的参数共享，并且需要的子网存储量明显减少，因为它实际上只训练一个基本网络和部分重叠的二进制专家掩码的混合。在实际数据集上的实验结果表明，MoME 在推荐精度和计算效率方面具有优越的性能。我们的代码可以在 https://https://github.com/xjh0327/mome 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MoME:+Mixture-of-Masked-Experts+for+Efficient+Multi-Task+Recommendation)|0|
|[Multi-Layer Ranking with Large Language Models for News Source Recommendation](https://doi.org/10.1145/3626772.3657966)|Wenjia Zhang, Lin Gui, Rob Procter, Yulan He|University of Warwick; The University of Warwick; King's College London|To seek reliable information sources for news events, we introduce a noveltask of expert recommendation, which aims to identify trustworthy sources basedon their previously quoted statements. To achieve this, we built a noveldataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourcedfrom a collection of news articles. We formulate the recommendation task as theretrieval of experts based on their likelihood of being associated with a givenquery. We also propose a multi-layer ranking framework employing Large LanguageModels to improve the recommendation performance. Our results show thatemploying an in-context learning based LLM ranker and a multi-layerranking-based filter significantly improve both the predictive quality andbehavioural quality of the recommender system.|为了为新闻事件寻找可靠的信息来源，本文提出了一种新颖的专家推荐任务，该任务的目的是根据新闻事件的可靠信息来源的先前引用的陈述来确定其可靠性。为了实现这一点，我们建立了一个名为 NewsQuote 的新颖数据集，其中包含23,571对引用说话者，这些引用说话者来自一系列新闻文章。我们根据专家与特定查询相关联的可能性来制定推荐任务。我们还提出了一个使用大型语言模型的多层次排序框架，以提高推荐性能。我们的研究结果表明，使用基于上下文学习的 LLM 排名器和基于多层次排名的过滤器可以显著提高推荐系统的预测质量和行为质量。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Layer+Ranking+with+Large+Language+Models+for+News+Source+Recommendation)|0|
|[Neural Click Models for Recommender Systems](https://doi.org/10.1145/3626772.3657939)|Mikhail Shirokikh, Ilya Shenbin, Anton Alekseev, Anna Volodkevich, Alexey Vasilev, Andrey V. Savchenko, Sergey I. Nikolenko|Sber AI Lab; Sber AI Lab, Moscow, Russian Federation; PDMI RAS & St. Petersburg University, St. Petersburg, Russian Federation; PDMI RAS, St. Petersburg, Russian Federation; Steklov Mathematical Institute, St. Petersburg; St. Petersburg State University, St. Petersburg, Russian Federation|We develop and evaluate neural architectures to model the user behavior in recommender systems (RS) inspired by click models for Web search but going beyond standard click models. Proposed architectures include recurrent networks, Transformer-based models that alleviate the quadratic complexity of self-attention, adversarial and hierarchical architectures. Our models outperform baselines on the ContentWise and RL4RS datasets and can be used in RS simulators to model user response for RS evaluation and pretraining.|我们开发和评估神经结构，以模拟推荐系统(RS)中的用户行为，该系统受到 Web 搜索的点击模型的启发，但超越了标准的点击模型。建议的体系结构包括循环网络，基于变压器的模型，减轻自我注意的二次复杂性，对手和分层体系结构。我们的模型优于 ContentWise 和 RL4RS 数据集的基线，可以用于 RS 模拟器，为 RS 评估和预训练建立用户响应模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Click+Models+for+Recommender+Systems)|0|
|[SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval](https://doi.org/10.1145/3626772.3657910)|Zihao Li, Yuyi Ao, Jingrui He|University of Illinois at Urbana-Champaign|Knowledge graphs (KGs), which store an extensive number of relational facts(head, relation, tail), serve various applications. While many downstream taskshighly rely on the expressive modeling and predictive embedding of KGs, most ofthe current KG representation learning methods, where each entity is embeddedas a vector in the Euclidean space and each relation is embedded as atransformation, follow an entity ranking protocol. On one hand, such anembedding design cannot capture many-to-many relations. On the other hand, inmany retrieval cases, the users wish to get an exact set of answers without anyranking, especially when the results are expected to be precise, e.g., whichgenes cause an illness. Such scenarios are commonly referred to as "setretrieval". This work presents a pioneering study on the KG set retrievalproblem. We show that the set retrieval highly depends on expressive modelingof many-to-many relations, and propose a new KG embedding model SpherE toaddress this problem. SpherE is based on rotational embedding methods, but eachentity is embedded as a sphere instead of a vector. While inheriting the highinterpretability of rotational-based models, our SpherE can more expressivelymodel one-to-many, many-to-one, and many-to-many relations. Through extensiveexperiments, we show that our SpherE can well address the set retrieval problemwhile still having a good predictive ability to infer missing facts. The codeis available at https://github.com/Violet24K/SpherE.|知识图(KGs)存储了大量的关系事实(头部、关系、尾部) ，服务于各种应用。虽然许多下游任务高度依赖于 KG 的表达式建模和预测嵌入，但目前大多数 KG 表示学习方法都遵循实体排序协议，其中每个实体嵌入在欧几里德空间中作为一个向量，每个关系嵌入作为变换。一方面，这样的嵌入式设计不能捕获多对多的关系。另一方面，在许多检索案例中，用户希望在没有任何排名的情况下得到一组精确的答案，特别是当结果被期望是精确的时候，例如，哪些基因导致疾病。这样的场景通常被称为“设置检索”。本文对 KG 集检索问题进行了开创性的研究。我们证明了集合检索高度依赖于多对多关系的表达式建模，并提出了一种新的 KG 嵌入模型 SpherE 来解决这一问题。球面嵌入是基于旋转嵌入的方法，但是每个实体都是球面嵌入而不是向量嵌入。在继承了基于旋转的模型的高可解释性的同时，我们的 SpherE 可以更有表现力地建立一对多、多对一和多对多的关系模型。通过大量的实验表明，我们的 SphereE 能够很好地解决集合检索问题，同时仍然具有很好的预测能力来推断丢失的事实。密码可以在 https://github.com/violet24k/sphere 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SpherE:+Expressive+and+Interpretable+Knowledge+Graph+Embedding+for+Set+Retrieval)|0|
|[CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models](https://doi.org/10.1145/3626772.3657672)|Peiyuan Gong, Jiamian Li, Jiaxin Mao|Renmin University of China Gaoling School of Artificial Intelligence|Collaborative search supports multiple users working together to accomplish aspecific search task. Research has found that designing lightweightcollaborative search plugins within instant messaging platforms aligns betterwith users' collaborative habits. However, due to the complexity of multi-userinteraction scenarios, it is challenging to implement a fully functioninglightweight collaborative search system. Therefore, previous studies onlightweight collaborative search had to rely on the Wizard of Oz paradigm. Inrecent years, large language models (LLMs) have been demonstrated to interactnaturally with users and achieve complex information-seeking tasks throughLLM-based agents. Hence, to better support the research in collaborativesearch, in this demo, we propose CoSearchAgent, a lightweight collaborativesearch agent powered by LLMs. CoSearchAgent is designed as a Slack plugin thatcan support collaborative search during multi-party conversations on thisplatform. Equipped with the capacity to understand the queries and context inmulti-user conversations and the ability to search the Web for relevantinformation via APIs, CoSearchAgent can respond to user queries with answersgrounded on the relevant search results. It can also ask clarifying questionswhen the information needs are unclear. The proposed CoSearchAgent is highlyflexible and would be useful for supporting further research on collaborativesearch. The code and demo video are accessible.|协同搜索支持多个用户协同工作来完成特定的搜索任务。研究发现，在即时通讯平台上设计轻量级/协作式搜索插件更符合用户的协作习惯。然而，由于多用户交互场景的复杂性，实现一个全功能的轻量级协同搜索系统是一个挑战。因此，之前对轻量级协作搜索的研究必须依赖于绿野仙踪范式。近年来，大型语言模型(LLM)已被证明可以与用户进行自然交互，并通过基于 LLM 的代理实现复杂的信息搜索任务。因此，为了更好地支持协作搜索的研究，在本演示中，我们提出了 CoSearchAgent，一个由 LLM 支持的轻量级协作搜索代理。CoSearchAgent 被设计成一个 Slack 插件，可以在这个平台上支持多方对话的协作搜索。CoSearchAgent 具有理解多用户对话中的查询和上下文的能力，并能够通过 API 在网上搜索相关信息，CoSearchAgent 可以根据相关搜索结果回答用户的查询。当信息需求不明确时，它也可以提出澄清问题。提出的 CoSearchAgent 具有高度灵活性，将有助于支持协作研究的进一步研究。代码和演示视频是可访问的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoSearchAgent:+A+Lightweight+Collaborative+Search+Agent+with+Large+Language+Models)|0|
|[MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation](https://doi.org/10.1145/3626772.3657662)|Zijie J. Wang, Duen Horng Chau|Georgia Tech|Retrieval-augmented text generation (RAG) addresses the common limitations of large language models (LLMs), such as hallucination, by retrieving information from an updatable external knowledge base. However, existing approaches often require dedicated backend servers for data storage and retrieval, thereby limiting their applicability in use cases that require strict data privacy, such as personal finance, education, and medicine. To address the pressing need for client-side dense retrieval, we introduce MeMemo, the first open-source JavaScript toolkit that adapts the state-of-the-art approximate nearest neighbor search technique HNSW to browser environments. Developed with modern and native Web technologies, such as IndexedDB and Web Workers, our toolkit leverages client-side hardware capabilities to enable researchers and developers to efficiently search through millions of high-dimensional vectors in the browser. MeMemo enables exciting new design and research opportunities, such as private and personalized content creation and interactive prototyping, as demonstrated in our example application RAG Playground. Reflecting on our work, we discuss the opportunities and challenges for on-device dense retrieval. MeMemo is available at https://github.com/poloclub/mememo.|检索增强型文本生成(RAG)通过从可更新的外部知识库中检索信息，解决了大型语言模型(LLM)的常见局限性，如幻觉。然而，现有的方法通常需要专用的后端服务器来存储和检索数据，从而限制了它们在需要严格数据隐私的用例中的适用性，例如个人理财、教育和医疗。为了满足客户端密集检索的迫切需求，我们引入了 MeMemo，这是第一个开源 JavaScript 工具包，它将最先进的近似最近邻搜索技术 HNSW 应用于浏览器环境。我们的工具包利用 IndexedDB 和 Web Workers 等现代和本地 Web 技术开发，利用客户端硬件能力，使研究人员和开发人员能够在浏览器中有效地搜索数百万个高维向量。MeMemo 提供了令人兴奋的新设计和研究机会，如私人和个性化的内容创建和交互式原型制作，如我们的示例应用程序 RAG Playground 所示。回顾我们的工作，我们讨论了设备上密集检索的机会和挑战。备忘录可在 https://github.com/poloclub/MeMemo 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MeMemo:+On-device+Retrieval+Augmentation+for+Private+and+Personalized+Text+Generation)|0|
|[Monitoring the Evolution of Behavioural Embeddings in Social Media Recommendation](https://doi.org/10.1145/3626772.3661368)|Srijan Saket, Olivier Jeunen, Md. Danish Kalim|ShareChat; Sharechat|Emerging short-video platforms like TikTok, Instagram Reels, and ShareChatpresent unique challenges for recommender systems, primarily originating from acontinuous stream of new content. ShareChat alone receives approximately 2million pieces of fresh content daily, complicating efforts to assess quality,learn effective latent representations, and accurately match content with theappropriate user base, especially given limited user feedback. Embedding-basedapproaches are a popular choice for industrial recommender systems because theycan learn low-dimensional representations of items, leading to effectiverecommendation that can easily scale to millions of items and users. Our work characterizes the evolution of such embeddings in short-videorecommendation systems, comparing the effect of batch and real-time updates tocontent embeddings. We investigate how embeddings change with subsequentupdates, explore the relationship between embeddings and popularity bias, andhighlight their impact on user engagement metrics. Our study unveils thecontrast in the number of interactions needed to achieve mature embeddings in abatch learning setup versus a real-time one, identifies the point of highestinformation updates, and explores the distribution of ℓ_2-norms across thetwo competing learning modes. Utilizing a production system deployed on alarge-scale short-video app with over 180 million users, our findings offerinsights into designing effective recommendation systems and enhancing usersatisfaction and engagement in short-video applications.|新兴的短视频平台，如 TikTok、 Instagram Reels 和 ShareChat.com 对推荐系统提出了独特的挑战，这些平台主要来源于源源不断的新内容。ShareChat 每天接收大约200万条新内容，这使得评估质量、学习有效的潜在表现形式以及将内容与合适的用户群准确匹配的工作变得复杂，特别是在用户反馈有限的情况下。基于嵌入的方法是工业推荐系统的一个流行选择，因为它们可以学习项目的低维表示，导致有效的推荐，可以轻松地扩展到数百万个项目和用户。我们的工作描述了这种嵌入在短视频推荐系统中的演变，比较了批量和实时更新对内容嵌入的影响。我们调查嵌入如何随着后续更新而改变，探索嵌入与流行偏差之间的关系，并强调它们对用户参与度量的影响。我们的研究揭示了在批量学习设置中实现成熟嵌入与实时嵌入所需的交互数量的对比，确定了最高信息更新的点，并探索了在两种竞争学习模式中 l _ 2-规范的分布。我们的研究结果利用一个部署在拥有超过1.8亿用户的大规模短视频应用程序上的生产系统，为设计有效的推荐系统、提高用户满意度和参与短视频应用程序提供了见解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Monitoring+the+Evolution+of+Behavioural+Embeddings+in+Social+Media+Recommendation)|0|
|[Embedding Based Deduplication in E-commerce AutoComplete](https://doi.org/10.1145/3626772.3661373)|Shaodan Zhai, Yuwei Chen, Yixue Li|Coupang Inc., Mountain View, USA; Coupang Inc., Mountain View, CA, USA|Query AutoComplete (QAC) is an important feature in e-commerce search engines, aimed at enhancing user experience by offering relevant query suggestions. However, these suggestions often include semantically duplicate entries derived from user logs. While the existing literature has made significant progress in query similarity learning for e-commerce applications, the specific challenge of query deduplication has received less attention. To address this issue, this paper presents a new industry-scale framework for QAC deduplication at Coupang, utilizing diverse data augmentation techniques to enhance deduplication accuracy effectively. Our results reveal that this approach substantially outperforms existing query similarity methods, providing valuable insights into the utility of various pre-trained models and data augmentation strategies. Online A/B testing further validates the significant impact of our deduplication framework on improving the e-commerce search experience, highlighting the importance of addressing semantic duplicates in QAC suggestions and offering a practical solution with proven effectiveness in a live e-commerce environment.|查询自动完成(Query AutoComplete，QAC)是电子商务搜索引擎的一个重要特性，旨在通过提供相关的查询建议来增强用户体验。但是，这些建议通常包括来自用户日志的语义重复条目。虽然现有的文献在电子商务应用中的查询相似性学习方面取得了显著的进展，但是查询重复数据删除这一具体挑战却没有得到足够的重视。为了解决这个问题，本文提出了一个新的行业规模的质保局在 Coupang 重复数据删除框架，利用不同的数据增强技术，以有效地提高重复数据删除的准确性。我们的研究结果表明，这种方法大大优于现有的查询相似性方法，为各种预先训练的模型和数据增强策略的实用性提供了有价值的见解。在线 A/B 测试进一步验证了我们的重复数据删除框架对改善电子商务搜索体验的重要影响，突出了解决质量保证委员会建议中的语义重复的重要性，并提供了一个实用的解决方案，在实时电子商务环境中被证明是有效的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Based+Deduplication+in+E-commerce+AutoComplete)|0|
|[Using and Evaluating Quantum Computing for Information Retrieval and Recommender Systems](https://doi.org/10.1145/3626772.3661378)|Maurizio Ferrari Dacrema, Andrea Pasin, Paolo Cremonesi, Nicola Ferro|Politecnico di Milano, Milano, Italy; Università degli Studi di Padova, Padova, Italy|The field of Quantum Computing (QC) has gained significant popularity in recent years, due to its potential to provide benefits in terms of efficiency and effectiveness when employed to solve certain computationally intensive tasks. In both Information Retrieval (IR) and Recommender Systems (RS) we are required to build methods that apply complex processing on large and heterogeneous datasets, it is natural therefore to wonder whether QC could also be applied to boost their performance. The tutorial aims to provide first an introduction to QC for an audience that is not familiar with the technology, then to show how to apply the QC paradigm of Quantum Annealing (QA) to solve practical problems that are currently faced by IR and RS systems. During the tutorial, participants will be provided with the fundamentals required to understand QC and to apply it in practice by using a real D-Wave quantum annealer through APIs.|近年来，量子计算(QC)因其在解决某些计算密集型任务时在效率和有效性方面的潜力而广受欢迎。在信息检索(IR)和推荐系统(RS)中，我们都被要求建立在大型异构数据集上应用复杂处理的方法，因此很自然地想知道是否也可以应用质量控制来提高它们的性能。本教程旨在首先为不熟悉这项技术的观众介绍质量控制，然后展示如何应用质量控制量子退火(QA)范例来解决当前 IR 和 RS 系统所面临的实际问题。在本教程期间，参与者将提供必要的基本知识，以了解质量控制，并应用它在实践中使用一个真正的 D-Wave 量子退火器通过 API。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+and+Evaluating+Quantum+Computing+for+Information+Retrieval+and+Recommender+Systems)|0|
|[Reinforcing Long-Term Performance in Recommender Systems with User-Oriented Exploration Policy](https://doi.org/10.1145/3626772.3657714)|Changshuo Zhang, Sirui Chen, Xiao Zhang, Sunhao Dai, Weijie Yu, Jun Xu|Gaoling School of AI, Renmin University of China, Beijing, China; University of International Business and Economics School of Information Technology and Management; University of Illinois at Urbana-Champaign, Champaign, USA|Reinforcement learning (RL) has gained popularity in recommender systems for improving long-term performance by effectively exploring users' interests. However, modern recommender systems face the challenge of different user behavioral patterns among millions of items, making exploration more difficult. For example, users with varying activity levels require different exploration intensities. Unfortunately, previous studies often overlook this aspect and apply a uniform exploration strategy to all users, which ultimately hampers long-term user experiences. To tackle these challenges, we propose User-Oriented Exploration Policy (UOEP), a novel approach that enables fine-grained exploration among user groups. We first construct a distributional critic that allows policy optimization based on varying quantile levels of cumulative reward feedback from users, representing user groups with different activity levels. Using this critic as a guide, we design a population of distinct actors dedicated to effective and fine-grained exploration within their respective user groups. To simultaneously enhance diversity and stability during the exploration process, we also introduce a population-level diversity regularization term and a supervision module. Experimental results on public recommendation datasets validate the effectiveness of our approach, as it outperforms all other baselines in terms of long-term performance. Moreover, further analyses reveal the benefits of our approach, including improved performance for low-activity users and increased fairness among users.|在推荐系统中，强化学习(RL)通过有效地探索用户的兴趣来改善长期表现，这种做法已经越来越受欢迎。然而，现代推荐系统面临着数以百万计的项目中不同的用户行为模式的挑战，使得探索变得更加困难。例如，活动级别不同的用户需要不同的探索强度。不幸的是，以往的研究往往忽视了这一方面，对所有用户采用统一的探索策略，这最终阻碍了长期的用户体验。为了应对这些挑战，我们提出了面向用户的探索策略(UOEP) ，这是一种新颖的方法，能够在用户组之间进行细粒度的探索。我们首先构造一个分布式批评，允许基于来自用户的累积奖励反馈的不同分位数级别的策略优化，代表具有不同活动级别的用户组。以这个批评家为指导，我们设计了一组不同的参与者，致力于在他们各自的用户群中进行有效和细粒度的探索。为了在勘探过程中同时增强多样性和稳定性，我们还引入了一个种群级多样性正则化项和一个监督模块。对公共推荐数据集的实验结果验证了我们方法的有效性，因为它在长期性能方面优于所有其他基线。此外，进一步的分析揭示了我们的方法的好处，包括改善低活动用户的性能和增加用户之间的公平性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcing+Long-Term+Performance+in+Recommender+Systems+with+User-Oriented+Exploration+Policy)|0|
|[Unsupervised Cross-Domain Image Retrieval with Semantic-Attended Mixture-of-Experts](https://doi.org/10.1145/3626772.3657826)|Kai Wang, Jiayang Liu, Xing Xu, Jingkuan Song, Xin Liu, Heng Tao Shen|College of Computer Science and Technology, Huaqiao University, Xiamen, China; College of Electronic and Information Engineering, Tongji University, Shanghai, China; University of Electronic Science and Technology of China|Unsupervised cross-domain image retrieval is designed to facilitate the retrieval between images in different domains in an unsupervised way. Without the guidance of labels, both intra-domain semantic learning and inter-domain semantic alignment pose significant challenges to the model's learning process. The resolution of these challenges relies on the accurate capture of domain-invariant semantic features by the model. Based on this consideration, we propose our Semantic-Attended Mixture of Experts (SA-MoE) model. Leveraging the proficiency of MoE network in capturing visual features, we enhance the model's focus on semantically relevant features through a series of strategies. We first utilize the self-attention mechanism of Vision Transformer to adaptively collect information with different weights on instances from different domains. In addition, we introduce contextual semantic association metrics to more accurately measure the semantic relatedness between instances. By utilizing the association metrics, secondary clustering is performed in the feature space to reinforce semantic relationships. Finally, we employ the metrics for information selection on the fused data to remove the semantic noise. We conduct extensive experiments on three widely used datasets. The consistent comparison results with existing methods indicate that our model possesses the state-of-the-art performance.|无监督跨域图像检索是为了方便不同域间图像的无监督检索而设计的。在没有标签指导的情况下，域内语义学习和域间语义对齐都给模型的学习过程带来了巨大的挑战。这些挑战的解决依赖于模型对领域不变语义特征的准确捕获。基于这一考虑，我们提出了语义参与的专家混合模型(SA-MoE)。利用 MoE 网络捕获视觉特征的能力，我们通过一系列策略提高了模型对语义相关特征的关注度。首先利用视觉变压器的自注意机制对不同领域的实例进行不同权重的信息自适应采集。此外，我们引入上下文语义关联度量来更准确地度量实例之间的语义关联。利用关联度量，在特征空间中进行二次聚类以增强语义关系。最后，利用融合数据的信息选择度量去除语义噪声。我们在三个广泛使用的数据集上进行了广泛的实验。与现有方法的一致性比较结果表明，我们的模型具有最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Cross-Domain+Image+Retrieval+with+Semantic-Attended+Mixture-of-Experts)|0|
|[Multilingual Meta-Distillation Alignment for Semantic Retrieval](https://doi.org/10.1145/3626772.3657812)|Meryem M'hamdi, Jonathan May, Franck Dernoncourt, Trung Bui, Seunghyun Yoon|Adobe Research, San Jose, CA, USA; Microsoft & University of Southern California, Redmond, WA, USA; University of Southern California, Los Angeles, CA, USA; Adobe Research, Seattle, WA, USA|Multilingual semantic retrieval involves retrieving semantically relevant content to a query irrespective of the language. Compared to monolingual and bilingual semantic retrieval, multilingual semantic retrieval requires a stronger alignment approach to pull the contents to be retrieved close to the representation of their corresponding queries, no matter their language combinations. Traditionally, this is achieved through more supervision in the form of multilingual parallel resources, which are expensive to obtain, especially for low-resource languages. In this work, on top of an optimization-based Model-Agnostic Meta-Learner (MAML), we propose a data-efficient meta-distillation approach: MAML-Align,1 specifically for low-resource multilingual semantic retrieval. Our approach simulates a gradual feedback loop from monolingual to bilingual and from bilingual to multilingual semantic retrieval. We systematically compare multilingual meta-distillation learning to different baselines and conduct ablation studies on the role of different sampling approaches in the meta-task construction. We show that MAML-Align's gradual feedback loop boosts the generalization to different languages, including zero-shot ones, better than naive fine-tuning and vanilla MAML.|多语言语义检索包括检索与查询语言无关的语义相关内容。与单语言和双语语义检索相比，多语言语义检索需要一种更强的对齐方法来将要检索的内容拉近其相应查询的表示，而不管它们的语言组合如何。传统上，这是通过以多语言并行资源的形式进行更多的监督来实现的，这些资源是昂贵的，特别是对于资源较少的语言。本文在基于优化的模型不可知元学习器(MAML)的基础上，提出了一种数据高效的元精馏方法: MAML-Align，1，专门用于低资源的多语言语义检索。我们的方法模拟了一个从单语到双语，从双语到多语的语义检索的渐进反馈循环。我们系统地比较了不同基线的多语言元精馏学习，并对不同抽样方法在元任务构建中的作用进行了消融研究。我们展示了 MAML-Align 的渐进反馈循环提高了对不同语言的泛化能力，包括0-shot 语言，这比单纯的微调和普通的 MAML 要好。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multilingual+Meta-Distillation+Alignment+for+Semantic+Retrieval)|0|
|[Dataset and Models for Item Recommendation Using Multi-Modal User Interactions](https://doi.org/10.1145/3626772.3657881)|Simone Borg Bruun, Krisztian Balog, Maria Maistro|Tenure Track Assistant Professor; PhD; Dr. Scient.|While recommender systems with multi-modal item representations (image,audio, and text), have been widely explored, learning recommendations frommulti-modal user interactions (e.g., clicks and speech) remains an openproblem. We study the case of multi-modal user interactions in a setting whereusers engage with a service provider through multiple channels (website andcall center). In such cases, incomplete modalities naturally occur, since notall users interact through all the available channels. To address thesechallenges, we publish a real-world dataset that allows progress in thisunder-researched area. We further present and benchmark various methods forleveraging multi-modal user interactions for item recommendations, and proposea novel approach that specifically deals with missing modalities by mappinguser interactions to a common feature space. Our analysis reveals importantinteractions between the different modalities and that a frequently occurringmodality can enhance learning from a less frequent one.|虽然具有多模态项目表示(图像、音频和文本)的推荐系统已经得到了广泛的探索，但是从多模态用户交互(例如点击和语音)中学习推荐仍然是一个尚未解决的问题。我们研究的情况下，多模式的用户交互设置，其中用户与服务提供商通过多个渠道(网站和呼叫中心)。在这种情况下，不完整的模式自然会出现，因为没有用户通过所有可用的渠道进行交互。为了应对这些挑战，我们发布了一个真实世界的数据集，允许在这个研究不足的领域取得进展。我们进一步介绍和基准利用多模态用户交互的项目推荐的各种方法，并提出新的方法，具体处理缺失的模式映射到一个共同的功能空间的用户交互。我们的分析揭示了不同模式之间的重要相互作用，一个频繁出现的模式可以增强从一个不太频繁的学习。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+and+Models+for+Item+Recommendation+Using+Multi-Modal+User+Interactions)|0|
|[Behavior-Contextualized Item Preference Modeling for Multi-Behavior Recommendation](https://doi.org/10.1145/3626772.3657696)|Mingshi Yan, Fan Liu, Jing Sun, Fuming Sun, Zhiyong Cheng, Yahong Han|Dalian Minzu University; Hefei University of Technology; Tianjin University; National University of Singapore|In recommender systems, multi-behavior methods have demonstrated theireffectiveness in mitigating issues like data sparsity, a common challenge intraditional single-behavior recommendation approaches. These methods typicallyinfer user preferences from various auxiliary behaviors and apply them to thetarget behavior for recommendations. However, this direct transfer canintroduce noise to the target behavior in recommendation, due to variations inuser attention across different behaviors. To address this issue, this paperintroduces a novel approach, Behavior-Contextualized Item Preference Modeling(BCIPM), for multi-behavior recommendation. Our proposedBehavior-Contextualized Item Preference Network discerns and learns users'specific item preferences within each behavior. It then considers only thosepreferences relevant to the target behavior for final recommendations,significantly reducing noise from auxiliary behaviors. These auxiliarybehaviors are utilized solely for training the network parameters, therebyrefining the learning process without compromising the accuracy of the targetbehavior recommendations. To further enhance the effectiveness of BCIPM, weadopt a strategy of pre-training the initial embeddings. This step is crucialfor enriching the item-aware preferences, particularly in scenarios where datarelated to the target behavior is sparse. Comprehensive experiments conductedon four real-world datasets demonstrate BCIPM's superior performance comparedto several leading state-of-the-art models, validating the robustness andefficiency of our proposed approach.|在推荐系统中，多行为方法已经证明了它们在缓解诸如数据稀疏等问题上的有效性，这是传统的单行为推荐方法所面临的共同挑战。这些方法通常从各种辅助行为推断出用户偏好，并将其应用于目标行为以获得推荐。然而，由于不同行为的用户注意力的差异，这种直接转移会给推荐中的目标行为带来噪声。为了解决这个问题，本文提出了一种新的方法，行为上下文项目偏好建模(BCIPM) ，用于多行为推荐。我们提出的上下文化项目偏好网络识别和学习用户的特定项目偏好在每个行为。然后它只考虑那些与目标行为相关的偏好作为最终建议，显著减少辅助行为的噪音。这些辅助行为仅用于训练网络参数，从而在不影响目标行为建议准确性的前提下完善学习过程。为了进一步提高 BCIPM 的有效性，我们采用了预先训练初始嵌入的策略。这一步对于丰富项目感知偏好是至关重要的，特别是在与目标行为相关的数据稀少的情况下。在四个真实世界数据集上进行的综合实验表明，BCIPM 的性能优于几个领先的最先进的模型，验证了我们提出的方法的鲁棒性和有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior-Contextualized+Item+Preference+Modeling+for+Multi-Behavior+Recommendation)|0|
|[Exploring the Individuality and Collectivity of Intents behind Interactions for Graph Collaborative Filtering](https://doi.org/10.1145/3626772.3657738)|Yi Zhang, Lei Sang, Yiwen Zhang|Anhui University|Intent modeling has attracted widespread attention in recommender systems. Asthe core motivation behind user selection of items, intent is crucial forelucidating recommendation results. The current mainstream modeling method isto abstract the intent into unknowable but learnable shared or non-sharedparameters. Despite considerable progress, we argue that it still confronts thefollowing challenges: firstly, these methods only capture the coarse-grainedaspects of intent, ignoring the fact that user-item interactions will beaffected by collective and individual factors (e.g., a user may choose a moviebecause of its high box office or because of his own unique preferences);secondly, modeling believable intent is severely hampered by implicit feedback,which is incredibly sparse and devoid of true semantics. To address thesechallenges, we propose a novel recommendation framework designated as BilateralIntent-guided Graph Collaborative Filtering (BIGCF). Specifically, we take acloser look at user-item interactions from a causal perspective and put forththe concepts of individual intent-which signifies private preferences-andcollective intent-which denotes overall awareness. To counter the sparsity ofimplicit feedback, the feature distributions of users and items are encoded viaa Gaussian-based graph generation strategy, and we implement the recommendationprocess through bilateral intent-guided graph reconstruction re-sampling.Finally, we propose graph contrastive regularization for both interaction andintent spaces to uniformize users, items, intents, and interactions in aself-supervised and non-augmented paradigm. Experimental results on threereal-world datasets demonstrate the effectiveness of BIGCF compared withexisting solutions.|意图建模在推荐系统中引起了广泛的关注。作为用户选择项目背后的核心动机，意图是否定推荐结果的关键。目前主流的建模方法是将意图抽象为不可知但可学的共享或非共享参数。尽管取得了相当大的进步，我们认为它仍然面临以下挑战: 首先，这些方法只捕获意图的粗粒度方面，忽略了用户项目的交互将受到集体和个人因素的影响(例如，用户可能会选择一部电影，因为它的高票房或因为他自己的独特偏好) ; 其次，建模可信的意图是严重阻碍隐式反馈，这是令人难以置信的稀疏和缺乏真正的语义。为了应对这些挑战，我们提出了一个新的推荐框架，命名为双边/意图引导的图形协同过滤(bIGCF)。具体来说，我们从因果关系的角度来看待用户-项目的交互，提出了个人意图的概念——表示私人偏好——和集体意图——表示整体意识。针对隐式反馈的稀疏性，采用基于高斯的图生成策略对用户和项目的特征分布进行编码，并通过双边意图引导的图重构重采样实现推荐过程。最后，我们提出了交互空间和意图空间的图形对比正则化，在自监督和非增强的范式中统一用户、项目、意图和交互。在三个实际数据集上的实验结果表明了 BIGCF 方法与现有方法相比的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Individuality+and+Collectivity+of+Intents+behind+Interactions+for+Graph+Collaborative+Filtering)|0|
|[Content-based Graph Reconstruction for Cold-start Item Recommendation](https://doi.org/10.1145/3626772.3657801)|Jinri Kim, Eungi Kim, Kwangeun Yeo, Yujin Jeon, Chanwoo Kim, Sewon Lee, Joonseok Lee|Seoul National Univ., Seoul, Republic of Korea|Graph convolutions have been successfully applied to recommendation systems, utilizing high-order collaborative signals present in the user-item interaction graph. This idea, however, has not been applicable to the cold-start items, since cold nodes are isolated in the graph and thus do not take advantage of information exchange from neighboring nodes. Recently, there have been a few attempts to utilize graph convolutions on item-item or user-user attribute graphs to capture high-order collaborative signals for cold-start cases, but these approaches are still limited in that the item-item or user-user graph falls short in capturing the dynamics of user-item interactions, as their edges are constructed based on arbitrary and heuristic attribute similarity. In this paper, we introduce Content-based Graph Reconstruction for Cold-start item recommendation (CGRC), employing a masked graph autoencoder structure and multimodal contents to directly incorporate interaction-based high-order connectivity, applicable even in cold-start scenarios. To address the cold-start items directly on the interaction graph, our approach trains the model to reconstruct plausible user-item interactions from masked edges of randomly chosen cold items, simulating fresh items without connection to users. This strategy enables the model to infer potential edges for unseen cold-start nodes. Extensive experiments on real-world datasets demonstrate the superiority of our model.|图卷积已成功应用于推荐系统，利用高阶协作信号存在于用户项交互图中。然而，这种思想并不适用于冷启动项，因为冷节点在图中是孤立的，因此不利用相邻节点之间的信息交换。近年来，利用项目卷积或用户-用户属性图来获取冷启动情况下的高阶协作信号的方法已经有了一些尝试，但这些方法仍然受到项目卷积或用户-用户图在获取用户-项目交互动态方面的局限，因为它们的边是基于任意和启发式属性相似性构造的。本文介绍了基于内容的图重构技术在冷启动项目推荐中的应用，该技术采用屏蔽图自动编码器结构和多模态内容直接结合基于交互的高阶连通性，适用于冷启动项目推荐。为了直接处理交互图上的冷启动项目，我们的方法训练模型从随机选择的冷启动项目的掩盖边缘重建合理的用户-项目交互，模拟与用户没有联系的新鲜项目。该策略使模型能够推断出未知冷启动节点的潜在边。在实际数据集上的大量实验证明了该模型的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Content-based+Graph+Reconstruction+for+Cold-start+Item+Recommendation)|0|
|[Unbiased Learning-to-Rank Needs Unconfounded Propensity Estimation](https://doi.org/10.1145/3626772.3657772)|Dan Luo, Lixin Zou, Qingyao Ai, Zhiyu Chen, Chenliang Li, Dawei Yin, Brian D. Davison|Baidu Inc., Beijing, China; Tsinghua University, Beijing, China; Lehigh University, Bethlehem, PA, USA; Wuhan University, Wuhan, China; Amazon.com, Inc., Seattle, WA, USA|The logs of the use of a search engine provide sufficient data to train a better ranker. However, it is well known that such implicit feedback reflects biases, and in particular a presentation bias that favors higher-ranked results. Unbiased Learning-to-Rank (ULTR) methods attempt to optimize performance by jointly modeling this bias along with the ranker so that the bias can be removed. Such methods have been shown to provide theoretical soundness, and promise superior performance and low deployment costs. However, existing ULTR methods don't recognize that query-document relevance is a confounder -- it affects both the likelihood of a result being clicked because of relevance and the likelihood of the result being ranked high by the base ranker. Moreover, the performance guarantees of existing ULTR methods assume the use of a weak ranker -- one that does a poor job of ranking documents based on relevance to a query. In practice, of course, commercial search engines use highly tuned rankers, and desire to improve upon them using the implicit judgments in search logs. This results in a significant correlation between position and relevance, which leads existing ULTR methods to overestimate click propensities in highly ranked results, reducing ULTR's effectiveness. This paper is the first to demonstrate the problem of propensity overestimation by ULTR algorithms, based on a causal analysis. We develop a new learning objective based on a backdoor adjustment. In addition, we introduce the Logging-Policy-aware Propensity (LPP) model that can jointly learn LPP and a more accurate ranker. We extensively test our approach on two public benchmark tasks and show that our proposal is effective, practical and significantly outperforms the state of the art.|使用搜索引擎的日志提供了足够的数据来训练一个更好的排名。然而，众所周知，这种隐性反馈反映了偏见，特别是偏向于排名较高的结果的表示偏见。无偏学习排序(ULTR)方法试图通过将这种偏差与排序器联合建模来优化性能，从而消除这种偏差。这样的方法已被证明提供了理论上的可靠性，并承诺优越的性能和低部署成本。然而，现有的 ULTR 方法没有认识到查询文档的相关性是一个混杂因素——它影响因为相关性而被点击的结果的可能性和基础排名结果被排名高的可能性。此外，现有 ULTR 方法的性能保证假设使用了一个弱排名器——一个根据与查询的相关性对文档进行排名的工作做得很糟糕的排名器。当然，在实践中，商业搜索引擎使用高度调整的排名，并希望利用搜索日志中隐含的判断来改进它们。这导致了位置和相关性之间的显著相关性，这导致现有的 ULTR 方法高估了高排名结果中的点击倾向，降低了 ULTR 的有效性。本文首次在分析因果关系的基础上，论证了 ULTR 算法存在的倾向高估问题。我们开发了一个新的学习目标的基础上后门调整。此外，我们还介绍了日志策略感知倾向(LPP)模型，该模型可以联合学习 LPP 和一个更准确的排名。我们在两个公共基准任务上广泛测试了我们的方法，并表明我们的建议是有效的、实用的，而且明显优于最先进的水平。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Learning-to-Rank+Needs+Unconfounded+Propensity+Estimation)|0|
|[Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context](https://doi.org/10.1145/3626772.3657803)|Moyu Zhang, Yongxiang Tang, Jinxin Hu, Yu Zhang|Unaffiliated; Lazada Group|Existing methods often adjust representations adaptively only after aggregating user behavior sequences. This coarse-grained approach to re-weighting the entire user sequence hampers the model's ability to accurately model the user interest migration across different scenarios. To enhance the model's capacity to capture user interests from historical behavior sequences in each scenario, we develop a ranking framework named the Scenario-Adaptive Fine-Grained Personalization Network (SFPNet), which designs a kind of fine-grained method for multi-scenario personalized recommendations. Specifically, SFPNet comprises a series of blocks named as Scenario-Tailoring Block, stacked sequentially. Each block initially deploys a parameter personalization unit to integrate scenario information at a coarse-grained level by redefining fundamental features. Subsequently, we consolidate scenario-adaptively adjusted feature representations to serve as context information. By employing residual connection, we incorporate this context into the representation of each historical behavior, allowing for context-aware fine-grained customization of the behavior representations at the scenario-level, which in turn supports scenario-aware user interest modeling.|现有的方法通常只在聚合用户行为序列之后才自适应地调整表示。这种重新加权整个用户序列的粗粒度方法阻碍了模型在不同场景间精确建模用户兴趣迁移的能力。为了提高模型从每个场景中的历史行为序列中获取用户兴趣的能力，提出了一种基于场景-自适应细粒度个性化网络(Scenario-AdaptiveFine-GrainedPersonalization Network，SFPNet)的排序框架，该框架设计了一种用于多场景个性化推荐的细粒度方法。具体来说，SFPNet 包含一系列名为 Scenario-Tailoring Block 的块，按顺序堆叠。每个块最初部署一个参数个性化单元，通过重新定义基本特性在粗粒度级别集成场景信息。随后，我们整合场景-自适应调整的特征表示作为上下文信息。通过使用剩余连接，我们将这个上下文整合到每个历史行为的表示中，允许在场景级别对行为表示进行上下文感知的细粒度定制，这反过来又支持场景感知的用户兴趣建模。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario-Adaptive+Fine-Grained+Personalization+Network:+Tailoring+User+Behavior+Representation+to+the+Scenario+Context)|0|
|[EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention](https://doi.org/10.1145/3626772.3657805)|Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, JiRong Wen|Renmin University of China; Huawei|To capture user preference, transformer models have been widely applied tomodel sequential user behavior data. The core of transformer architecture liesin the self-attention mechanism, which computes the pairwise attention scoresin a sequence. Due to the permutation-equivariant nature, positional encodingis used to enhance the attention between token representations. In thissetting, the pairwise attention scores can be derived by both semanticdifference and positional difference. However, prior studies often model thetwo kinds of difference measurements in different ways, which potentiallylimits the expressive capacity of sequence modeling. To address this issue,this paper proposes a novel transformer variant with complex vector attention,named EulerFormer, which provides a unified theoretical framework to formulateboth semantic difference and positional difference. The EulerFormer involvestwo key technical improvements. First, it employs a new transformation functionfor efficiently transforming the sequence tokens into polar-form complexvectors using Euler's formula, enabling the unified modeling of both semanticand positional information in a complex rotation form.Secondly, it develops adifferential rotation mechanism, where the semantic rotation angles can becontrolled by an adaptation function, enabling the adaptive integration of thesemantic and positional information according to the semanticcontexts.Furthermore, a phase contrastive learning task is proposed to improvethe anisotropy of contextual representations in EulerFormer. Our theoreticalframework possesses a high degree of completeness and generality. It is morerobust to semantic variations and possesses moresuperior theoretical propertiesin principle. Extensive experiments conducted on four public datasetsdemonstrate the effectiveness and efficiency of our approach.|为了捕获用户偏好，转换器模型已被广泛应用于模型顺序用户行为数据。变压器结构的核心是自注意机制，它计算一个序列的成对注意得分。由于置换等变的特性，位置编码被用来增强标记表示之间的注意力。在这种情况下，成对的注意分数可以通过语义差异和位置差异得出。然而，先前的研究往往以不同的方式对这两种差异测量进行建模，这可能限制了序列建模的表达能力。为了解决这一问题，本文提出了一种新的具有复矢量注意力的变换器变体，称为欧拉变换器，它提供了一个统一的理论框架来描述语义差异和位置差异。欧拉前者包括两个关键的技术改进。首先，利用欧拉公式有效地将序列标记转换为极形复合向量，使得语义和位置信息以复杂的旋转形式统一建模成为可能。其次，提出了一种差分旋转机制，该机制通过自适应函数控制语义旋转角度，实现了语义和位置信息根据语义上下文的自适应集成。此外，本文还提出了一个相位对比学习任务来改善 EulerForm 中上下文表示的各向异性。我们的理论框架具有较高的完整性和普遍性。它对语义变异具有更强的鲁棒性，在原则上具有更优越的理论性。在四个公共数据集上进行的大量实验证明了我们方法的有效性和效率。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EulerFormer:+Sequential+User+Behavior+Modeling+with+Complex+Vector+Attention)|0|
|[Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors](https://doi.org/10.1145/3626772.3657974)|Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, Linjian Mo|Westlake University; Ant Group|With the rise of large language models (LLMs), recent works have leveragedLLMs to improve the performance of click-through rate (CTR) prediction.However, we argue that a critical obstacle remains in deploying LLMs forpractical use: the efficiency of LLMs when processing long textual userbehaviors. As user sequences grow longer, the current efficiency of LLMs isinadequate for training on billions of users and items. To break through theefficiency barrier of LLMs, we propose Behavior Aggregated HierarchicalEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.Specifically, BAHE proposes a novel hierarchical architecture that decouplesthe encoding of user behaviors from inter-behavior interactions. Firstly, toprevent computational redundancy from repeated encoding of identical userbehaviors, BAHE employs the LLM's pre-trained shallow layers to extractembeddings of the most granular, atomic user behaviors from extensive usersequences and stores them in the offline database. Subsequently, the deeper,trainable layers of the LLM facilitate intricate inter-behavior interactions,thereby generating comprehensive user embeddings. This separation allows thelearning of high-level user representations to be independent of low-levelbehavior encoding, significantly reducing computational complexity. Finally,these refined user embeddings, in conjunction with correspondingly processeditem embeddings, are incorporated into the CTR model to compute the CTR scores.Extensive experimental results show that BAHE reduces training time and memoryby five times for CTR models using LLMs, especially with longer user sequences.BAHE has been deployed in a real-world system, allowing for daily updates of 50million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTRprediction.|随着大型语言模型(LLM)的兴起，最近的工作已经利用 LLM 来提高点进率预测(CTR)的性能。然而，我们认为在实际应用中部署 LLM 仍然存在一个关键的障碍: 当处理长文本用户行为时 LLM 的效率。随着用户序列的增长，LLM 目前的效率不足以对数十亿用户和项目进行培训。为了突破 LLM 的效率障碍，我们提出了行为聚合层次编码(BAHE)来提高基于 LLM 的 CTR 建模的效率。具体来说，BAHE 提出了一种新的层次结构，将用户行为的编码与行为间的交互分离开来。首先，为了防止计算冗余重复编码相同的用户行为，BAHE 使用 LLM 预先训练的浅层来从大量的用户序列中提取最细粒度的原子用户行为，并将它们存储在离线数据库中。随后，LLM 的更深层、可训练的层促进了复杂的行为间交互，从而产生了全面的用户嵌入。这种分离使得高级用户表示的学习独立于低级行为编码，显著降低了计算复杂度。最后，这些改进的用户嵌入，结合相应的处理过的项目嵌入，被合并到 CTR 模型中来计算 CTR 分数。大量的实验结果表明，BAHE 使用 LLM 将 CTR 模型的训练时间和记忆降低了5倍，特别是对于更长的用户序列。 BAHE 已经部署在现实世界的系统中，允许在8个 A100图形处理器上每天更新5000万个 CTR 数据，使 LLM 用于工业 CTR 预测变得实用。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Length+Barrier:+LLM-Enhanced+CTR+Prediction+in+Long+Textual+User+Behaviors)|0|
|[Multi-intent-aware Session-based Recommendation](https://doi.org/10.1145/3626772.3657928)|Minjin Choi, Hyeyoung Kim, Hyunsouk Cho, Jongwuk Lee|Ajou University; Sungkyunkwan University; Sungkyunkwan University Artificial intelligence|Session-based recommendation (SBR) aims to predict the following item a userwill interact with during an ongoing session. Most existing SBR models focus ondesigning sophisticated neural-based encoders to learn a sessionrepresentation, capturing the relationship among session items. However, theytend to focus on the last item, neglecting diverse user intents that may existwithin a session. This limitation leads to significant performance drops,especially for longer sessions. To address this issue, we propose a novel SBRmodel, called Multi-intent-aware Session-based Recommendation Model (MiaSRec).It adopts frequency embedding vectors indicating the item frequency in sessionto enhance the information about repeated items. MiaSRec represents varioususer intents by deriving multiple session representations centered on each itemand dynamically selecting the important ones. Extensive experimental resultsshow that MiaSRec outperforms existing state-of-the-art SBR models on sixdatasets, particularly those with longer average session length, achieving upto 6.27https://github.com/jin530/MiaSRec.|基于会话的推荐(SBR)旨在预测用户将在正在进行的会话期间与以下项目进行交互。大多数现有的 SBR 模型侧重于设计复杂的基于神经的编码器来学习会话表示，捕获会话项之间的关系。然而，他们倾向于关注最后一项，忽略了会话中可能存在的不同用户意图。这种限制会导致显著的性能下降，特别是对于较长的会话。为了解决这个问题，我们提出了一种新的 SBR 模型，称为多意图感知的基于会话的推荐模型(MiaSRec)。采用频率嵌入向量表示会话中项目的频率，增强重复项目的信息。MiaSRec 通过派生以每个项为中心的多个会话表示并动态选择重要的会话表示来表示各种用户意图。大量的实验结果表明，MiaSrec 在6个数据集上优于现有的最先进的 SBR 模型，特别是那些平均会话长度更长的模型，达到了6.27的 https://github.com/jin530/MiaSRec。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-intent-aware+Session-based+Recommendation)|0|
|[PLAID SHIRTTT for Large-Scale Streaming Dense Retrieval](https://doi.org/10.1145/3626772.3657964)|Dawn J. Lawrie, Efsun Selin Kayi, Eugene Yang, James Mayfield, Douglas W. Oard|University of Maryland; Johns Hopkins University HLTCOE|PLAID, an efficient implementation of the ColBERT late interaction bi-encoderusing pretrained language models for ranking, consistently achievesstate-of-the-art performance in monolingual, cross-language, and multilingualretrieval. PLAID differs from ColBERT by assigning terms to clusters andrepresenting those terms as cluster centroids plus compressed residual vectors.While PLAID is effective in batch experiments, its performance degrades instreaming settings where documents arrive over time because representations ofnew tokens may be poorly modeled by the earlier tokens used to select clustercentroids. PLAID Streaming Hierarchical Indexing that Runs on Terabytes ofTemporal Text (PLAID SHIRTTT) addresses this concern using multi-phaseincremental indexing based on hierarchical sharding. Experiments on ClueWeb09and the multilingual NeuCLIR collection demonstrate the effectiveness of thisapproach both for the largest collection indexed to date by the ColBERTarchitecture and in the multilingual setting, respectively.|PLAID 是 ColBERT 后期交互双编码器的有效实现，使用预先训练的语言模型进行排名，始终在单语言、跨语言和多语言检索方面取得最佳性能。PLAID 不同于 ColBERT，它将术语分配给聚类，并将这些术语表示为聚类质心加上压缩的残差向量。虽然 PLAID 在批处理实验中是有效的，但它的性能降低了文档随时间到达的流化设置，因为用于选择集群中心的早期令牌可能对新令牌的表示建模不足。基于 T 级时态文本的 PLAID 流式分层索引(PLAID SHIRTTT)使用基于分层分片的多阶段增量索引解决了这个问题。在 ClueWeb09和多语言 NeuCLIR 集合上的实验分别证明了这种方法对 ColBERT 架构迄今为止索引的最大集合和多语言设置的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PLAID+SHIRTTT+for+Large-Scale+Streaming+Dense+Retrieval)|0|
|[Towards Ethical Item Ranking: A Paradigm Shift from User-Centric to Item-Centric Approaches](https://doi.org/10.1145/3626772.3657977)|Guilherme Ramos, Mirko Marras, Ludovico Boratto|Instituto de Telecomunicações, and Instituto Superior Técnico, ULisboa, Lisbon, Portugal; University of Cagliari, Cagliari, Italy|Ranking systems are instrumental in shaping user experiences by determining the relevance and order of presented items. However, current approaches, particularly those revolving around user-centric reputation scoring, raise ethical concerns associated with scoring individuals. To counter such issues, in this paper, we introduce a novel item ranking system approach that strategically transitions its emphasis from scoring users to calculating item rankings relying exclusively on items' ratings information, to achieve the same objective. Experiments on three datasets show that our approach achieves higher effectiveness and efficiency than state-of-the-art baselines. Furthermore, the resulting rankings are more robust to spam and resistant to bribery, contributing to a novel and ethically sound direction for item ranking systems.|排名系统有助于通过确定呈现项目的相关性和顺序来塑造用户体验。然而，目前的方法，特别是那些围绕以用户为中心的声誉评分的方法，提出了与个人评分相关的伦理问题。为了解决这些问题，本文提出了一种新的项目排名系统方法，该方法从对用户进行评分转变为完全依靠项目的评分信息来计算项目排名，以达到同样的目的。在三个数据集上的实验表明，该方法比最先进的基线方法具有更高的效率和效果。此外，由此产生的排名更强大的垃圾邮件和抵抗贿赂，有助于一个新颖和道德上健全的项目排名系统的方向。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Ethical+Item+Ranking:+A+Paradigm+Shift+from+User-Centric+to+Item-Centric+Approaches)|0|
|[A Large-scale Offer Alignment Model for Partitioning Filtering and Matching Product Offers](https://doi.org/10.1145/3626772.3661351)|Wenyu Huang, André Melo, Jeff Z. Pan|The University of Edinburgh, Edinburgh, United Kingdom; Huawei Technologies R&D, Edinburgh, United Kingdom|Offer alignment is a key step in a product knowledge graph construction pipeline. It aims to align retailer offers of the same product for better coverage of product details. With the rapid development of online shopping services, the offer alignment task is applied in ever larger datasets. This work aims to build an offer alignment system that can efficiently be used in large-scale offer data. The key components of this system include: 1) common offer encoders for encoding text offer data into representations; 2) trainable LSH partitioning module to divide similar offers into small blocks; 3) lightweight sophisticated late-interactions for efficient filtering and scoring of offer alignment candidate pairs. We evaluate the system on public WDC offer alignment dataset, as well as DBLP-Scholar and DBLP-ACM.|报价对齐是产品知识图构建流程中的一个关键步骤。它旨在使零售商提供的同一产品，以更好地覆盖产品的细节。随着网上购物服务的迅速发展，报价对齐任务在越来越大的数据集中得到了广泛的应用。本文旨在建立一个能够有效应用于大规模报价数据的报价对齐系统。该系统的关键组成部分包括: 1)用于编码文本的通用报价编码器将数据提供到表示中; 2)可训练的 LSH 分区模块将类似的报价划分为小块; 3)轻量级复杂的后期交互，以便对报价对齐候选对进行有效的过滤和评分。我们在公共 WDC 提供的比对数据集上以及 DBLP-Scholar 和 DBLP-ACM 上对该系统进行了评估。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Large-scale+Offer+Alignment+Model+for+Partitioning+Filtering+and+Matching+Product+Offers)|0|
|[Interest Clock: Time Perception in Real-Time Streaming Recommendation System](https://doi.org/10.1145/3626772.3661369)|Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, Zuotao Liu|ByteDance|User preferences follow a dynamic pattern over a day, e.g., at 8 am, a usermight prefer to read news, while at 8 pm, they might prefer to watch movies.Time modeling aims to enable recommendation systems to perceive time changes tocapture users' dynamic preferences over time, which is an important andchallenging problem in recommendation systems. Especially, streamingrecommendation systems in the industry, with only available samples of thecurrent moment, present greater challenges for time modeling. There is still alack of effective time modeling methods for streaming recommendation systems.In this paper, we propose an effective and universal method Interest Clock toperceive time information in recommendation systems. Interest Clock firstencodes users' time-aware preferences into a clock (hour-level personalizedfeatures) and then uses Gaussian distribution to smooth and aggregate them intothe final interest clock embedding according to the current time for the finalprediction. By arming base models with Interest Clock, we conduct online A/Btests, obtaining +0.509duration respectively. Besides, the extended offline experiments showimprovements as well. Interest Clock has been deployed on Douyin Music App.|用户偏好在一天中遵循一种动态模式，例如，在早上8点，用户可能更喜欢阅读新闻，而在晚上8点，他们可能更喜欢看电影。时间建模的目的是使推荐系统能够感知时间的变化，捕获用户随时间变化的动态偏好，这是推荐系统中的一个重要而又具有挑战性的问题。特别是，业界的流式推荐系统，只有当前时刻的可用样本，对时间建模提出了更大的挑战。针对流媒体推荐系统中时间建模方法的不足，本文提出了一种有效而通用的时间建模方法——兴趣时钟法。兴趣时钟首先将用户的时间感知偏好编码成一个时钟(小时级别的个性化功能) ，然后使用正态分布来平滑和聚合它们，根据当前的时间嵌入到最终的兴趣时钟中，进行最终的预测。通过使用兴趣时钟武装基本模型，我们进行在线 A/Btest，分别获得 + 0.509的持续时间。此外，延长的离线实验也有所改善。兴趣时钟已经在抖音音乐应用程序上部署。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interest+Clock:+Time+Perception+in+Real-Time+Streaming+Recommendation+System)|0|
|[Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback](https://doi.org/10.1145/3626772.3657689)|Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Large+Language+Model+Alignment+for+Information+Retrieval+via+Contrastive+Feedback)|0|
|[Amazon-KG: A Knowledge Graph Enhanced Cross-Domain Recommendation Dataset](https://doi.org/10.1145/3626772.3657880)|Yuhan Wang, Qing Xie, Mengzi Tang, Lin Li, Jingling Yuan, Yongjian Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Amazon-KG:+A+Knowledge+Graph+Enhanced+Cross-Domain+Recommendation+Dataset)|0|
|[Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive Multimodal Knowledge Graph Completion](https://doi.org/10.1145/3626772.3657838)|Yu Zhao, Ying Zhang, Baohang Zhou, Xinying Qian, Kehui Song, Xiangrui Cai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrast+then+Memorize:+Semantic+Neighbor+Retrieval-Enhanced+Inductive+Multimodal+Knowledge+Graph+Completion)|0|
|[The Treatment of Ties in Rank-Biased Overlap](https://doi.org/10.1145/3626772.3657700)|Matteo Corsi, Julián Urbano||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Treatment+of+Ties+in+Rank-Biased+Overlap)|0|
|[What Matters in a Measure? A Perspective from Large-Scale Search Evaluation](https://doi.org/10.1145/3626772.3657845)|Paul Thomas, Gabriella Kazai, Nick Craswell, Seth Spielman||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+Matters+in+a+Measure?+A+Perspective+from+Large-Scale+Search+Evaluation)|0|
|[CaDRec: Contextualized and Debiased Recommender Model](https://doi.org/10.1145/3626772.3657799)|Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Jiyi Li, Dongjin Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaDRec:+Contextualized+and+Debiased+Recommender+Model)|0|
|[Going Beyond Popularity and Positivity Bias: Correcting for Multifactorial Bias in Recommender Systems](https://doi.org/10.1145/3626772.3657749)|Jin Huang, Harrie Oosterhuis, Masoud Mansoury, Herke van Hoof, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Going+Beyond+Popularity+and+Positivity+Bias:+Correcting+for+Multifactorial+Bias+in+Recommender+Systems)|0|
|[Configurable Fairness for New Item Recommendation Considering Entry Time of Items](https://doi.org/10.1145/3626772.3657694)|Huizhong Guo, Dongxia Wang, Zhu Sun, Haonan Zhang, Jinfeng Li, Jie Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Configurable+Fairness+for+New+Item+Recommendation+Considering+Entry+Time+of+Items)|0|
|[Generative Retrieval via Term Set Generation](https://doi.org/10.1145/3626772.3657797)|Peitian Zhang, Zheng Liu, Yujia Zhou, Zhicheng Dou, Fangchao Liu, Zhao Cao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Retrieval+via+Term+Set+Generation)|0|
|[MIRROR: A Multi-View Reciprocal Recommender System for Online Recruitment](https://doi.org/10.1145/3626772.3657776)|Zhi Zheng, Xiao Hu, Shanshan Gao, Hengshu Zhu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIRROR:+A+Multi-View+Reciprocal+Recommender+System+for+Online+Recruitment)|0|
|[Who To Align With: Feedback-Oriented Multi-Modal Alignment in Recommendation Systems](https://doi.org/10.1145/3626772.3657701)|Yang Li, Qi'ao Zhao, Chen Lin, Jinsong Su, Zhilin Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+To+Align+With:+Feedback-Oriented+Multi-Modal+Alignment+in+Recommendation+Systems)|0|
|[EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation](https://doi.org/10.1145/3626772.3657890)|Shaorun Zhang, Zhiyu He, Ziyi Ye, Peijie Sun, Qingyao Ai, Min Zhang, Yiqun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EEG-SVRec:+An+EEG+Dataset+with+User+Multidimensional+Affective+Engagement+Labels+in+Short+Video+Recommendation)|0|
|[Multimodality Invariant Learning for Multimedia-Based New Item Recommendation](https://doi.org/10.1145/3626772.3658596)|Haoyue Bai, Le Wu, Min Hou, Miaomiao Cai, Zhuangzhuang He, Yuyang Zhou, Richang Hong, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodality+Invariant+Learning+for+Multimedia-Based+New+Item+Recommendation)|0|
|[Semi-supervised Prototype Semantic Association Learning for Robust Cross-modal Retrieval](https://doi.org/10.1145/3626772.3657756)|Junsheng Wang, Tiantian Gong, Yan Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-supervised+Prototype+Semantic+Association+Learning+for+Robust+Cross-modal+Retrieval)|0|
|[Hypergraph Convolutional Network for User-Oriented Fairness in Recommender Systems](https://doi.org/10.1145/3626772.3657737)|Zhongxuan Han, Chaochao Chen, Xiaolin Zheng, Li Zhang, Yuyuan Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypergraph+Convolutional+Network+for+User-Oriented+Fairness+in+Recommender+Systems)|0|
|[Hierarchical Semantics Alignment for 3D Human Motion Retrieval](https://doi.org/10.1145/3626772.3657804)|Yang Yang, Haoyu Shi, Huaiwen Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Semantics+Alignment+for+3D+Human+Motion+Retrieval)|0|
|[A Large Scale Test Corpus for Semantic Table Search](https://doi.org/10.1145/3626772.3657877)|Aristotelis Leventidis, Martin Pekár Christensen, Matteo Lissandrini, Laura Di Rocco, Katja Hose, Renée J. Miller||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Large+Scale+Test+Corpus+for+Semantic+Table+Search)|0|
|[JDivPS: A Diversified Product Search Dataset](https://doi.org/10.1145/3626772.3657888)|Zhirui Deng, Zhicheng Dou, Yutao Zhu, Xubo Qin, Pengchao Cheng, Jiangxu Wu, Hao Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JDivPS:+A+Diversified+Product+Search+Dataset)|0|
|[An E-Commerce Dataset Revealing Variations during Sales](https://doi.org/10.1145/3626772.3657870)|Jianfu Zhang, Qingtao Yu, Yizhou Chen, Guoliang Zhou, Yawen Liu, Yawei Sun, Chen Liang, Guangda Huzhang, Yabo Ni, Anxiang Zeng, Han Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+E-Commerce+Dataset+Revealing+Variations+during+Sales)|0|
|[Exploring Multi-Scenario Multi-Modal CTR Prediction with a Large Scale Dataset](https://doi.org/10.1145/3626772.3657865)|Zhaoxin Huan, Ke Ding, Ang Li, Xiaolu Zhang, Xu Min, Yong He, Liang Zhang, Jun Zhou, Linjian Mo, Jinjie Gu, Zhongyi Liu, Wenliang Zhong, Guannan Zhang, Chenliang Li, Fajie Yuan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Multi-Scenario+Multi-Modal+CTR+Prediction+with+a+Large+Scale+Dataset)|0|
|[Dimension Importance Estimation for Dense Information Retrieval](https://doi.org/10.1145/3626772.3657691)|Guglielmo Faggioli, Nicola Ferro, Raffaele Perego, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dimension+Importance+Estimation+for+Dense+Information+Retrieval)|0|
|[Large Language Models for Next Point-of-Interest Recommendation](https://doi.org/10.1145/3626772.3657840)|Peibo Li, Maarten de Rijke, Hao Xue, Shuang Ao, Yang Song, Flora D. Salim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Next+Point-of-Interest+Recommendation)|0|
|[The Impact of Group Membership Bias on the Quality and Fairness of Exposure in Ranking](https://doi.org/10.1145/3626772.3657752)|Ali Vardasbi, Maarten de Rijke, Fernando Diaz, Mostafa Dehghani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Group+Membership+Bias+on+the+Quality+and+Fairness+of+Exposure+in+Ranking)|0|
|[Grand: A Fast and Accurate Graph Retrieval Framework via Knowledge Distillation](https://doi.org/10.1145/3626772.3657773)|Lin Lan, Pinghui Wang, Rui Shi, Tingqing Liu, Juxiang Zeng, Feiyang Sun, Yang Ren, Jing Tao, Xiaohong Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grand:+A+Fast+and+Accurate+Graph+Retrieval+Framework+via+Knowledge+Distillation)|0|
|[Unmasking Privacy: A Reproduction and Evaluation Study of Obfuscation-based Perturbation Techniques for Collaborative Filtering](https://doi.org/10.1145/3626772.3657858)|Alex Martinez, Mihnea Tufis, Ludovico Boratto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unmasking+Privacy:+A+Reproduction+and+Evaluation+Study+of+Obfuscation-based+Perturbation+Techniques+for+Collaborative+Filtering)|0|
|[GPT4Rec: Graph Prompt Tuning for Streaming Recommendation](https://doi.org/10.1145/3626772.3657720)|Peiyan Zhang, Yuchen Yan, Xi Zhang, Liying Kang, Chaozhuo Li, Feiran Huang, Senzhang Wang, Sunghun Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPT4Rec:+Graph+Prompt+Tuning+for+Streaming+Recommendation)|0|
|[I3: Intent-Introspective Retrieval Conditioned on Instructions](https://doi.org/10.1145/3626772.3657745)|Kaihang Pan, Juncheng Li, Wenjie Wang, Hao Fei, Hongye Song, Wei Ji, Jun Lin, Xiaozhong Liu, TatSeng Chua, Siliang Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=I3:+Intent-Introspective+Retrieval+Conditioned+on+Instructions)|0|
|[Disentangling ID and Modality Effects for Session-based Recommendation](https://doi.org/10.1145/3626772.3657748)|Xiaokun Zhang, Bo Xu, Zhaochun Ren, Xiaochen Wang, Hongfei Lin, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangling+ID+and+Modality+Effects+for+Session-based+Recommendation)|0|
|[Large Language Models are Learnable Planners for Long-Term Recommendation](https://doi.org/10.1145/3626772.3657683)|Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+are+Learnable+Planners+for+Long-Term+Recommendation)|0|
|[Identifiability of Cross-Domain Recommendation via Causal Subspace Disentanglement](https://doi.org/10.1145/3626772.3657758)|Jing Du, Zesheng Ye, Bin Guo, Zhiwen Yu, Lina Yao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifiability+of+Cross-Domain+Recommendation+via+Causal+Subspace+Disentanglement)|0|
|[DeCoCDR: Deployable Cloud-Device Collaboration for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3657786)|Yu Li, Yi Zhang, Zimu Zhou, Qiang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeCoCDR:+Deployable+Cloud-Device+Collaboration+for+Cross-Domain+Recommendation)|0|
|[Mutual Information-based Preference Disentangling and Transferring for Non-overlapped Multi-target Cross-domain Recommendations](https://doi.org/10.1145/3626772.3657780)|Zhi Li, Daichi Amagata, Yihong Zhang, Takahiro Hara, Shuichiro Haruta, Kei Yonekawa, Mori Kurokawa||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Information-based+Preference+Disentangling+and+Transferring+for+Non-overlapped+Multi-target+Cross-domain+Recommendations)|0|
|[LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset](https://doi.org/10.1145/3626772.3657887)|Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, Yiqun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LeCaRDv2:+A+Large-Scale+Chinese+Legal+Case+Retrieval+Dataset)|0|
|[Behavior Pattern Mining-based Multi-Behavior Recommendation](https://doi.org/10.1145/3626772.3657973)|Haojie Li, Zhiyong Cheng, Xu Yu, Jinhuan Liu, Guanfeng Liu, Junwei Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior+Pattern+Mining-based+Multi-Behavior+Recommendation)|0|
|[Dense Retrieval with Continuous Explicit Feedback for Systematic Review Screening Prioritisation](https://doi.org/10.1145/3626772.3657921)|Xinyu Mao, Shengyao Zhuang, Bevan Koopman, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Retrieval+with+Continuous+Explicit+Feedback+for+Systematic+Review+Screening+Prioritisation)|0|
|[Cross-reconstructed Augmentation for Dual-target Cross-domain Recommendation](https://doi.org/10.1145/3626772.3657902)|Qingyang Mao, Qi Liu, Zhi Li, Likang Wu, Bing Lv, Zheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-reconstructed+Augmentation+for+Dual-target+Cross-domain+Recommendation)|0|
|[Distillation for Multilingual Information Retrieval](https://doi.org/10.1145/3626772.3657955)|Eugene Yang, Dawn J. Lawrie, James Mayfield||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distillation+for+Multilingual+Information+Retrieval)|0|
|[Estimating the Hessian Matrix of Ranking Objectives for Stochastic Learning to Rank with Gradient Boosted Trees](https://doi.org/10.1145/3626772.3657918)|Jingwei Kang, Maarten de Rijke, Harrie Oosterhuis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Estimating+the+Hessian+Matrix+of+Ranking+Objectives+for+Stochastic+Learning+to+Rank+with+Gradient+Boosted+Trees)|0|
|[Information Diffusion Prediction via Cascade-Retrieved In-context Learning](https://doi.org/10.1145/3626772.3657909)|Ting Zhong, Jienan Zhang, Zhangtao Cheng, Fan Zhou, Xueqin Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Diffusion+Prediction+via+Cascade-Retrieved+In-context+Learning)|0|
|[Masked Graph Transformer for Large-Scale Recommendation](https://doi.org/10.1145/3626772.3657971)|Huiyuan Chen, Zhe Xu, ChinChia Michael Yeh, Vivian Lai, Yan Zheng, Minghua Xu, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masked+Graph+Transformer+for+Large-Scale+Recommendation)|0|
|[Modeling Domains as Distributions with Uncertainty for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3657930)|Xianghui Zhu, Mengqun Jin, Hengyu Zhang, Chang Meng, Daoxin Zhang, Xiu Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Domains+as+Distributions+with+Uncertainty+for+Cross-Domain+Recommendation)|0|
|[SCM4SR: Structural Causal Model-based Data Augmentation for Robust Session-based Recommendation](https://doi.org/10.1145/3626772.3657940)|Muskan Gupta, Priyanka Gupta, Jyoti Narwariya, Lovekesh Vig, Gautam Shroff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCM4SR:+Structural+Causal+Model-based+Data+Augmentation+for+Robust+Session-based+Recommendation)|0|
|[USimAgent: Large Language Models for Simulating Search Users](https://doi.org/10.1145/3626772.3657963)|Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, Jiaxin Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=USimAgent:+Large+Language+Models+for+Simulating+Search+Users)|0|
|[ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3661348)|Chaoqun Hou, Yuanhang Zhou, Yi Cao, Tong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ECAT:+A+Entire+space+Continual+and+Adaptive+Transfer+Learning+Framework+for+Cross-Domain+Recommendation)|0|
|[Minimizing Live Experiments in Recommender Systems: User Simulation to Evaluate Preference Elicitation Policies](https://doi.org/10.1145/3626772.3661358)|ChihWei Hsu, Martin Mladenov, Ofer Meshi, James Pine, Hubert Pham, Shane Li, Xujian Liang, Anton Polishko, Li Yang, Ben Scheetz, Craig Boutilier||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Minimizing+Live+Experiments+in+Recommender+Systems:+User+Simulation+to+Evaluate+Preference+Elicitation+Policies)|0|
|[A Semantic Search Engine for Helping Patients Find Doctors and Locations in a Large Healthcare Organization](https://doi.org/10.1145/3626772.3661349)|Mayank Kejriwal, Hamid Haidarian, MinHsueh Chiu, Andy Xiang, Deep Shrestha, Faizan Javed||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Semantic+Search+Engine+for+Helping+Patients+Find+Doctors+and+Locations+in+a+Large+Healthcare+Organization)|0|
|[Clinical Trial Retrieval via Multi-grained Similarity Learning](https://doi.org/10.1145/3626772.3661366)|Junyu Luo, Cheng Qian, Lucas Glass, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clinical+Trial+Retrieval+via+Multi-grained+Similarity+Learning)|0|
|[Search under Uncertainty: Cognitive Biases and Heuristics: A Tutorial on Testing, Mitigating and Accounting for Cognitive Biases in Search Experiments](https://doi.org/10.1145/3626772.3661382)|Jiqun Liu, Leif Azzopardi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search+under+Uncertainty:+Cognitive+Biases+and+Heuristics:+A+Tutorial+on+Testing,+Mitigating+and+Accounting+for+Cognitive+Biases+in+Search+Experiments)|0|
|[TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision](https://doi.org/10.1145/3626772.3657788)|Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, Weinan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TRAD:+Enhancing+LLM+Agents+with+Step-Wise+Thought+Retrieval+and+Aligned+Decision)|0|
|[Representation Learning and Information Retrieval](https://doi.org/10.1145/3626772.3657995)|Yiming Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+and+Information+Retrieval)|0|
|["In-Context Learning" or: How I learned to stop worrying and love "Applied Information Retrieval"](https://doi.org/10.1145/3626772.3657842)|Andrew Parry, Debasis Ganguly, Manish Chandra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="In-Context+Learning"+or:+How+I+learned+to+stop+worrying+and+love+"Applied+Information+Retrieval")|0|
|[LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval](https://doi.org/10.1145/3626772.3657740)|Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, Changsheng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LDRE:+LLM-based+Divergent+Reasoning+and+Ensemble+for+Zero-Shot+Composed+Image+Retrieval)|0|
|[EditKG: Editing Knowledge Graph for Recommendation](https://doi.org/10.1145/3626772.3657723)|Gu Tang, Xiaoying Gan, Jinghe Wang, Bin Lu, Lyuwen Wu, Luoyi Fu, Chenghu Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EditKG:+Editing+Knowledge+Graph+for+Recommendation)|0|
|[GUITAR: Gradient Pruning toward Fast Neural Ranking](https://doi.org/10.1145/3626772.3657728)|Weijie Zhao, Shulong Tan, Ping Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GUITAR:+Gradient+Pruning+toward+Fast+Neural+Ranking)|0|
|[Revisiting Document Expansion and Filtering for Effective First-Stage Retrieval](https://doi.org/10.1145/3626772.3657850)|Watheq Mansour, Shengyao Zhuang, Guido Zuccon, Joel Mackenzie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Document+Expansion+and+Filtering+for+Effective+First-Stage+Retrieval)|0|
|[Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image Retrieval](https://doi.org/10.1145/3626772.3657727)|Haokun Wen, Xuemeng Song, Xiaolin Chen, Yinwei Wei, Liqiang Nie, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+but+Effective+Raw-Data+Level+Multimodal+Fusion+for+Composed+Image+Retrieval)|0|
|[Browsing and Searching Metadata of TREC](https://doi.org/10.1145/3626772.3657873)|Timo Breuer, Ellen M. Voorhees, Ian Soboroff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Browsing+and+Searching+Metadata+of+TREC)|0|
|[ACORDAR 2.0: A Test Collection for Ad Hoc Dataset Retrieval with Densely Pooled Datasets and Question-Style Queries](https://doi.org/10.1145/3626772.3657866)|Qiaosheng Chen, Weiqing Luo, Zixian Huang, Tengteng Lin, Xiaxia Wang, Ahmet Soylu, Basil Ell, Baifan Zhou, Evgeny Kharlamov, Gong Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACORDAR+2.0:+A+Test+Collection+for+Ad+Hoc+Dataset+Retrieval+with+Densely+Pooled+Datasets+and+Question-Style+Queries)|0|
|[Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling](https://doi.org/10.1145/3626772.3657767)|Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcement+Learning-based+Recommender+Systems+with+Large+Language+Models+for+State+Reward+and+Action+Modeling)|0|
|[OpenP5: An Open-Source Platform for Developing, Training, and Evaluating LLM-based Recommender Systems](https://doi.org/10.1145/3626772.3657883)|Shuyuan Xu, Wenyue Hua, Yongfeng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenP5:+An+Open-Source+Platform+for+Developing,+Training,+and+Evaluating+LLM-based+Recommender+Systems)|0|
|[Fair Recommendations with Limited Sensitive Attributes: A Distributionally Robust Optimization Approach](https://doi.org/10.1145/3626772.3657822)|Tianhao Shi, Yang Zhang, Jizhi Zhang, Fuli Feng, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Recommendations+with+Limited+Sensitive+Attributes:+A+Distributionally+Robust+Optimization+Approach)|0|
|[Large Language Models and Future of Information Retrieval: Opportunities and Challenges](https://doi.org/10.1145/3626772.3657848)|ChengXiang Zhai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+and+Future+of+Information+Retrieval:+Opportunities+and+Challenges)|0|
|[Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding](https://doi.org/10.1145/3626772.3657746)|Hansi Zeng, Chen Luo, Hamed Zamani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Planning+Ahead+in+Generative+Retrieval:+Guiding+Autoregressive+Generation+through+Simultaneous+Decoding)|0|
|[Course Recommender Systems Need to Consider the Job Market](https://doi.org/10.1145/3626772.3657847)|Jibril Frej, Anna Dai, Syrielle Montariol, Antoine Bosselut, Tanja Käser||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Course+Recommender+Systems+Need+to+Consider+the+Job+Market)|0|
|[Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients](https://doi.org/10.1145/3626772.3657785)|Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leave+No+Patient+Behind:+Enhancing+Medication+Recommendation+for+Rare+Disease+Patients)|0|
|[MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation](https://doi.org/10.1145/3626772.3657867)|Andreea Iana, Goran Glavas, Heiko Paulheim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIND+Your+Language:+A+Multilingual+Dataset+for+Cross-lingual+News+Recommendation)|0|
|[Steering Large Language Models for Cross-lingual Information Retrieval](https://doi.org/10.1145/3626772.3657819)|Ping Guo, Yubing Ren, Yue Hu, Yanan Cao, Yunpeng Li, Heyan Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Steering+Large+Language+Models+for+Cross-lingual+Information+Retrieval)|0|
|[DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion](https://doi.org/10.1145/3626772.3657779)|Aishwarya Maheswaran, Kaushal Kumar Maurya, Manish Gupta, Maunendra Sankar Desarkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DAC:+Quantized+Optimal+Transport+Reward-based+Reinforcement+Learning+Approach+to+Detoxify+Query+Auto-Completion)|0|
|[IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues](https://doi.org/10.1145/3626772.3657760)|Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IM-RAG:+Multi-Round+Retrieval-Augmented+Generation+Through+Learning+Inner+Monologues)|0|
|[Towards Human-centered Proactive Conversational Agents](https://doi.org/10.1145/3626772.3657843)|Yang Deng, Lizi Liao, Zhonghua Zheng, Grace Hui Yang, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Human-centered+Proactive+Conversational+Agents)|0|
|[TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants](https://doi.org/10.1145/3626772.3657860)|Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffrey Dalton, Leif Azzopardi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TREC+iKAT+2023:+A+Test+Collection+for+Evaluating+Conversational+and+Interactive+Knowledge+Assistants)|0|
|[UGNCL: Uncertainty-Guided Noisy Correspondence Learning for Efficient Cross-Modal Matching](https://doi.org/10.1145/3626772.3657806)|Quanxing Zha, Xin Liu, Yiuming Cheung, Xing Xu, Nannan Wang, Jianjia Cao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UGNCL:+Uncertainty-Guided+Noisy+Correspondence+Learning+for+Efficient+Cross-Modal+Matching)|0|
|[DHMAE: A Disentangled Hypergraph Masked Autoencoder for Group Recommendation](https://doi.org/10.1145/3626772.3657699)|Yingqi Zhao, Haiwei Zhang, Qijie Bai, Changli Nie, Xiaojie Yuan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DHMAE:+A+Disentangled+Hypergraph+Masked+Autoencoder+for+Group+Recommendation)|0|
|[Are We Really Achieving Better Beyond-Accuracy Performance in Next Basket Recommendation?](https://doi.org/10.1145/3626772.3657835)|Ming Li, Yuanna Liu, Sami Jullien, Mozhdeh Ariannezhad, Andrew Yates, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+We+Really+Achieving+Better+Beyond-Accuracy+Performance+in+Next+Basket+Recommendation?)|0|
|[AutoDCS: Automated Decision Chain Selection in Deep Recommender Systems](https://doi.org/10.1145/3626772.3657818)|Dugang Liu, Shenxian Xian, Yuhao Wu, Chaohua Yang, Xing Tang, Xiuqiang He, Zhong Ming||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoDCS:+Automated+Decision+Chain+Selection+in+Deep+Recommender+Systems)|0|
|[EasyRL4Rec: An Easy-to-use Library for Reinforcement Learning Based Recommender Systems](https://doi.org/10.1145/3626772.3657868)|Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, Min Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EasyRL4Rec:+An+Easy-to-use+Library+for+Reinforcement+Learning+Based+Recommender+Systems)|0|
|[Explainability for Transparent Conversational Information-Seeking](https://doi.org/10.1145/3626772.3657768)|Weronika Lajewska, Damiano Spina, Johanne Trippas, Krisztian Balog||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainability+for+Transparent+Conversational+Information-Seeking)|0|
|[Evaluating Search System Explainability with Psychometrics and Crowdsourcing](https://doi.org/10.1145/3626772.3657796)|Catherine Chen, Carsten Eickhoff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Search+System+Explainability+with+Psychometrics+and+Crowdsourcing)|0|
|[Enhancing Dataset Search with Compact Data Snippets](https://doi.org/10.1145/3626772.3657837)|Qiaosheng Chen, Jiageng Chen, Xiao Zhou, Gong Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Dataset+Search+with+Compact+Data+Snippets)|0|
|[When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications](https://doi.org/10.1145/3626772.3657722)|Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, Yefeng Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+MOE+Meets+LLMs:+Parameter+Efficient+Fine-tuning+for+Multi-task+Medical+Applications)|0|
|[OEHR: An Orthopedic Electronic Health Record Dataset](https://doi.org/10.1145/3626772.3657885)|Yibo Xie, Kaifan Wang, Jiawei Zheng, Feiyan Liu, Xiaoli Wang, Guofeng Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OEHR:+An+Orthopedic+Electronic+Health+Record+Dataset)|0|
|[SIGformer: Sign-aware Graph Transformer for Recommendation](https://doi.org/10.1145/3626772.3657747)|Sirui Chen, Jiawei Chen, Sheng Zhou, Bohao Wang, Shen Han, Chanfei Su, Yuqing Yuan, Can Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SIGformer:+Sign-aware+Graph+Transformer+for+Recommendation)|0|
|[Scaling Laws For Dense Retrieval](https://doi.org/10.1145/3626772.3657743)|Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Laws+For+Dense+Retrieval)|0|
|[Diffusion Models for Generative Outfit Recommendation](https://doi.org/10.1145/3626772.3657719)|Yiyan Xu, Wenjie Wang, Fuli Feng, Yunshan Ma, Jizhi Zhang, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Models+for+Generative+Outfit+Recommendation)|0|
|[Collaborative Filtering Based on Diffusion Models: Unveiling the Potential of High-Order Connectivity](https://doi.org/10.1145/3626772.3657742)|Yu Hou, JinDuk Park, WonYong Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Filtering+Based+on+Diffusion+Models:+Unveiling+the+Potential+of+High-Order+Connectivity)|0|
|[Graph Signal Diffusion Model for Collaborative Filtering](https://doi.org/10.1145/3626772.3657759)|Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Signal+Diffusion+Model+for+Collaborative+Filtering)|0|
|[Multi-granular Adversarial Attacks against Black-box Neural Ranking Models](https://doi.org/10.1145/3626772.3657704)|YuAn Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-granular+Adversarial+Attacks+against+Black-box+Neural+Ranking+Models)|0|
|[Optimal Transport Enhanced Cross-City Site Recommendation](https://doi.org/10.1145/3626772.3657757)|Xinhang Li, Xiangyu Zhao, Zihao Wang, Yang Duan, Yong Zhang, Chunxiao Xing||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimal+Transport+Enhanced+Cross-City+Site+Recommendation)|0|
|[Disentangled Contrastive Hypergraph Learning for Next POI Recommendation](https://doi.org/10.1145/3626772.3657726)|Yantong Lai, Yijun Su, Lingwei Wei, Tianqi He, Haitao Wang, Gaode Chen, Daren Zha, Qiang Liu, Xingxing Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Contrastive+Hypergraph+Learning+for+Next+POI+Recommendation)|0|
|[CLLP: Contrastive Learning Framework Based on Latent Preferences for Next POI Recommendation](https://doi.org/10.1145/3626772.3657730)|Hongli Zhou, Zhihao Jia, Haiyang Zhu, Zhizheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLLP:+Contrastive+Learning+Framework+Based+on+Latent+Preferences+for+Next+POI+Recommendation)|0|
|[OpenSiteRec: An Open Dataset for Site Recommendation](https://doi.org/10.1145/3626772.3657875)|Xinhang Li, Xiangyu Zhao, Yejing Wang, Yu Liu, Chong Chen, Cheng Long, Yong Zhang, Chunxiao Xing||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenSiteRec:+An+Open+Dataset+for+Site+Recommendation)|0|
|[Fairness-Aware Exposure Allocation via Adaptive Reranking](https://doi.org/10.1145/3626772.3657794)|Thomas Jänich, Graham McDonald, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-Aware+Exposure+Allocation+via+Adaptive+Reranking)|0|
|[A Taxation Perspective for Fair Re-ranking](https://doi.org/10.1145/3626772.3657766)|Chen Xu, Xiaopeng Ye, Wenjie Wang, Liang Pang, Jun Xu, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Taxation+Perspective+for+Fair+Re-ranking)|0|
|[A Dual-Embedding Based DQN for Worker Recruitment in Spatial Crowdsourcing with Social Network](https://doi.org/10.1145/3626772.3657718)|Yucen Gao, Wei Liu, Jianxiong Guo, Xiaofeng Gao, Guihai Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dual-Embedding+Based+DQN+for+Worker+Recruitment+in+Spatial+Crowdsourcing+with+Social+Network)|0|
|[Efficient Community Search Based on Relaxed k-Truss Index](https://doi.org/10.1145/3626772.3657708)|Xiaoqin Xie, Shuangyuan Liu, Jiaqi Zhang, Shuai Han, Wei Wang, Wu Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Community+Search+Based+on+Relaxed+k-Truss+Index)|0|
|[Untargeted Adversarial Attack on Knowledge Graph Embeddings](https://doi.org/10.1145/3626772.3657702)|Tianzhe Zhao, Jiaoyan Chen, Yanchi Ru, Qika Lin, Yuxia Geng, Jun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Untargeted+Adversarial+Attack+on+Knowledge+Graph+Embeddings)|0|
|[Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval](https://doi.org/10.1145/3626772.3657792)|Guangyuan Ma, Xing Wu, Zijia Lin, Songlin Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Drop+your+Decoder:+Pre-training+with+Bag-of-Word+Prediction+for+Dense+Passage+Retrieval)|0|
|[M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval](https://doi.org/10.1145/3626772.3657833)|Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang, Qingpei Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M2-RAAP:+A+Multi-Modal+Recipe+for+Advancing+Adaptation-based+Pre-training+towards+Effective+and+Efficient+Zero-shot+Video-text+Retrieval)|0|
|[CaLa: Complementary Association Learning for Augmenting Comoposed Image Retrieval](https://doi.org/10.1145/3626772.3657823)|Xintong Jiang, Yaxiong Wang, Mengjian Li, Yujiao Wu, Bingwen Hu, Xueming Qian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaLa:+Complementary+Association+Learning+for+Augmenting+Comoposed+Image+Retrieval)|0|
|[CFIR:  Fast and Effective Long-Text To Image Retrieval for Large Corpora](https://doi.org/10.1145/3626772.3657741)|Zijun Long, Xuri Ge, Richard McCreadie, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CFIR:++Fast+and+Effective+Long-Text+To+Image+Retrieval+for+Large+Corpora)|0|
|[CaseLink: Inductive Graph Learning for Legal Case Retrieval](https://doi.org/10.1145/3626772.3657693)|Yanran Tang, Ruihong Qiu, Hongzhi Yin, Xue Li, Zi Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaseLink:+Inductive+Graph+Learning+for+Legal+Case+Retrieval)|0|
|[Explicitly Integrating Judgment Prediction with Legal Document Retrieval: A Law-Guided Generative Approach](https://doi.org/10.1145/3626772.3657717)|Weicong Qin, Zelin Cao, Weijie Yu, Zihua Si, Sirui Chen, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicitly+Integrating+Judgment+Prediction+with+Legal+Document+Retrieval:+A+Law-Guided+Generative+Approach)|0|
|[A Persona-Infused Cross-Task Graph Network for Multimodal Emotion Recognition with Emotion Shift Detection in Conversations](https://doi.org/10.1145/3626772.3657944)|Geng Tu, Feng Xiong, Bin Liang, Ruifeng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Persona-Infused+Cross-Task+Graph+Network+for+Multimodal+Emotion+Recognition+with+Emotion+Shift+Detection+in+Conversations)|0|
|[Analyzing and Mitigating Repetitions in Trip Recommendation](https://doi.org/10.1145/3626772.3657970)|Wenzheng Shu, Kangqi Xu, Wenxin Tai, Ting Zhong, Yong Wang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+and+Mitigating+Repetitions+in+Trip+Recommendation)|0|
|[Cluster-based Partial Dense Retrieval Fused with Sparse Text Retrieval](https://doi.org/10.1145/3626772.3657972)|Yingrui Yang, Parker Carlson, Shanxiu He, Yifan Qiao, Tao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cluster-based+Partial+Dense+Retrieval+Fused+with+Sparse+Text+Retrieval)|0|
|[Contextualization with SPLADE for High Recall Retrieval](https://doi.org/10.1145/3626772.3657919)|Eugene Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextualization+with+SPLADE+for+High+Recall+Retrieval)|0|
|[Convex Feature Embedding for Face and Voice Association](https://doi.org/10.1145/3626772.3657975)|Jiwoo Kang, Taewan Kim, YoungHo Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Convex+Feature+Embedding+for+Face+and+Voice+Association)|0|
|[Enhancing Criminal Case Matching through Diverse Legal Factors](https://doi.org/10.1145/3626772.3657960)|Jie Zhao, Ziyu Guan, Wei Zhao, Yue Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Criminal+Case+Matching+through+Diverse+Legal+Factors)|0|
|[Faster Learned Sparse Retrieval with Block-Max Pruning](https://doi.org/10.1145/3626772.3657906)|Antonio Mallia, Torsten Suel, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Faster+Learned+Sparse+Retrieval+with+Block-Max+Pruning)|0|
|[Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://doi.org/10.1145/3626772.3657951)|Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-Tuning+LLaMA+for+Multi-Stage+Text+Retrieval)|0|
|[Graph Diffusive Self-Supervised Learning for Social Recommendation](https://doi.org/10.1145/3626772.3657962)|Jiuqiang Li, Hongjun Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Diffusive+Self-Supervised+Learning+for+Social+Recommendation)|0|
|[Improving In-Context Learning via Sequentially Selection and Preference Alignment for Few-Shot Aspect-Based Sentiment Analysis](https://doi.org/10.1145/3626772.3657932)|Qianlong Wang, Keyang Ding, Xuan Luo, Ruifeng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+In-Context+Learning+via+Sequentially+Selection+and+Preference+Alignment+for+Few-Shot+Aspect-Based+Sentiment+Analysis)|0|
|[Language Fairness in Multilingual Information Retrieval](https://doi.org/10.1145/3626772.3657943)|Eugene Yang, Thomas Jänich, James Mayfield, Dawn J. Lawrie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Language+Fairness+in+Multilingual+Information+Retrieval)|0|
|[Large Language Models Based Stemming for Information Retrieval: Promises, Pitfalls and Failures](https://doi.org/10.1145/3626772.3657949)|Shuai Wang, Shengyao Zhuang, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+Based+Stemming+for+Information+Retrieval:+Promises,+Pitfalls+and+Failures)|0|
|[MACA: Memory-aided Coarse-to-fine Alignment for Text-based Person Search](https://doi.org/10.1145/3626772.3657915)|Liangxu Su, Rong Quan, Zhiyuan Qi, Jie Qin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MACA:+Memory-aided+Coarse-to-fine+Alignment+for+Text-based+Person+Search)|0|
|[Negative as Positive: Enhancing Out-of-distribution Generalization for Graph Contrastive Learning](https://doi.org/10.1145/3626772.3657927)|Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Negative+as+Positive:+Enhancing+Out-of-distribution+Generalization+for+Graph+Contrastive+Learning)|0|
|[On Backbones and Training Regimes for Dense Retrieval in African Languages](https://doi.org/10.1145/3626772.3657952)|Akintunde Oladipo, Mofetoluwa Adeyemi, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Backbones+and+Training+Regimes+for+Dense+Retrieval+in+African+Languages)|0|
|[Predicting Micro-video Popularity via Multi-modal Retrieval Augmentation](https://doi.org/10.1145/3626772.3657929)|Ting Zhong, Jian Lang, Yifan Zhang, Zhangtao Cheng, Kunpeng Zhang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Micro-video+Popularity+via+Multi-modal+Retrieval+Augmentation)|0|
|[Searching for Physical Documents in Archival Repositories](https://doi.org/10.1145/3626772.3657896)|Tokinori Suzuki, Douglas W. Oard, Emi Ishita, Yoichi Tomiura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Searching+for+Physical+Documents+in+Archival+Repositories)|0|
|[Self-Explainable Next POI Recommendation](https://doi.org/10.1145/3626772.3657967)|Kai Yang, Yi Yang, Qiang Gao, Ting Zhong, Yong Wang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Explainable+Next+POI+Recommendation)|0|
|[Synthetic Test Collections for Retrieval Evaluation](https://doi.org/10.1145/3626772.3657942)|Hossein A. Rahmani, Nick Craswell, Emine Yilmaz, Bhaskar Mitra, Daniel Campos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthetic+Test+Collections+for+Retrieval+Evaluation)|0|
|[SPLATE: Sparse Late Interaction Retrieval](https://doi.org/10.1145/3626772.3657968)|Thibault Formal, Stéphane Clinchant, Hervé Déjean, Carlos Lassance||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPLATE:+Sparse+Late+Interaction+Retrieval)|0|
|[The Surprising Effectiveness of Rankers trained on Expanded Queries](https://doi.org/10.1145/3626772.3657938)|Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Surprising+Effectiveness+of+Rankers+trained+on+Expanded+Queries)|0|
|[Turbo-CF: Matrix Decomposition-Free Graph Filtering for Fast Recommendation](https://doi.org/10.1145/3626772.3657916)|JinDuk Park, YongMin Shin, WonYong Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Turbo-CF:+Matrix+Decomposition-Free+Graph+Filtering+for+Fast+Recommendation)|0|
|[Unifying Graph Retrieval and Prompt Tuning for Graph-Grounded Text Classification](https://doi.org/10.1145/3626772.3657934)|Le Dai, Yu Yin, Enhong Chen, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Graph+Retrieval+and+Prompt+Tuning+for+Graph-Grounded+Text+Classification)|0|
|[Weighted KL-Divergence for Document Ranking Model Refinement](https://doi.org/10.1145/3626772.3657946)|Yingrui Yang, Yifan Qiao, Shanxiu He, Tao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weighted+KL-Divergence+for+Document+Ranking+Model+Refinement)|0|
|[Using Large Language Models for Math Information Retrieval](https://doi.org/10.1145/3626772.3657907)|Behrooz Mansouri, Reihaneh Maarefdoust||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Large+Language+Models+for+Math+Information+Retrieval)|0|
|[A Question-Answering Assistant over Personal Knowledge Graph](https://doi.org/10.1145/3626772.3657665)|Lingyuan Liu, Huifang Du, Xiaolian Zhang, Mengying Guo, Haofen Wang, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Question-Answering+Assistant+over+Personal+Knowledge+Graph)|0|
|[ConvLogRecaller: Real-Time Conversational Lifelog Recaller](https://doi.org/10.1145/3626772.3657659)|YuanChi Lee, AnZi Yen, HenHsen Huang, HsinHsi Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ConvLogRecaller:+Real-Time+Conversational+Lifelog+Recaller)|0|
|[CLIP-Branches:  Interactive Fine-Tuning for Text-Image Retrieval](https://doi.org/10.1145/3626772.3657678)|Christian Lülf, Denis Mayr Lima Martins, Marcos Antonio Vaz Salles, Yongluan Zhou, Fabian Gieseke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLIP-Branches:++Interactive+Fine-Tuning+for+Text-Image+Retrieval)|0|
|[Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation](https://doi.org/10.1145/3626772.3657673)|Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Img2Loc:+Revisiting+Image+Geolocalization+using+Multi-modality+Foundation+Models+and+Image-based+Retrieval-Augmented+Generation)|0|
|[JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs](https://doi.org/10.1145/3626772.3657677)|Wanying Ding, Manoj Cherukumalli, Santosh Chikoti, Vinay K. Chaudhri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JPEC:+A+Novel+Graph+Neural+Network+for+Competitor+Retrieval+in+Financial+Knowledge+Graphs)|0|
|[MACRec: A Multi-Agent Collaboration Framework for Recommendation](https://doi.org/10.1145/3626772.3657669)|Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MACRec:+A+Multi-Agent+Collaboration+Framework+for+Recommendation)|0|
|[ModelGalaxy: A Versatile Model Retrieval Platform](https://doi.org/10.1145/3626772.3657676)|Wenling Zhang, Yixiao Li, Zhaotian Li, Hailong Sun, Xiang Gao, Xudong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ModelGalaxy:+A+Versatile+Model+Retrieval+Platform)|0|
|[RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation](https://doi.org/10.1145/3626772.3657660)|Viju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, Roman Teucher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RAG-Ex:+A+Generic+Framework+for+Explaining+Retrieval+Augmented+Generation)|0|
|[ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement](https://doi.org/10.1145/3626772.3657680)|Saurabh Bhausaheb Zinjad, Amrita Bhattacharjee, Amey Bhilegaonkar, Huan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ResumeFlow:+An+LLM-facilitated+Pipeline+for+Personalized+Resume+Generation+and+Refinement)|0|
|[ScholarNodes: Applying Content-based Filtering to Recommend Interdisciplinary Communities within Scholarly Social Networks](https://doi.org/10.1145/3626772.3657668)|Md Asaduzzaman Noor, Jason A. Clark, John W. Sheppard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ScholarNodes:+Applying+Content-based+Filtering+to+Recommend+Interdisciplinary+Communities+within+Scholarly+Social+Networks)|0|
|[Synthetic Query Generation using Large Language Models for Virtual Assistants](https://doi.org/10.1145/3626772.3661355)|Sonal Sannigrahi, Thiago FragaSilva, Youssef Oualil, Christophe Van Gysel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthetic+Query+Generation+using+Large+Language+Models+for+Virtual+Assistants)|0|
|[A Study on Unsupervised Question and Answer Generation for Legal Information Retrieval and Precedents Understanding](https://doi.org/10.1145/3626772.3661354)|Johny Moreira, Altigran S. da Silva, Edleno Silva de Moura, Leandro Bezerra Marinho||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+on+Unsupervised+Question+and+Answer+Generation+for+Legal+Information+Retrieval+and+Precedents+Understanding)|0|
|[Reflections on the Coding Ability of LLMs for Analyzing Market Research Surveys](https://doi.org/10.1145/3626772.3661362)|Shi Zong, Santosh Kolagati, Amit Chaudhary, Josh Seltzer, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reflections+on+the+Coding+Ability+of+LLMs+for+Analyzing+Market+Research+Surveys)|0|
|[Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering](https://doi.org/10.1145/3626772.3661370)|Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, Zheng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Generation+with+Knowledge+Graphs+for+Customer+Service+Question+Answering)|0|
|[Striking the Right Chord: A Comprehensive Approach to Amazon Music Search Spell Correction](https://doi.org/10.1145/3626772.3661344)|Siddharth Sharma, Shiyun Yang, Ajinkya Walimbe, Tarun Sharma, Joaquin Delgado||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Striking+the+Right+Chord:+A+Comprehensive+Approach+to+Amazon+Music+Search+Spell+Correction)|0|
|[SLH-BIA: Short-Long Hawkes Process for Buy It Again Recommendations at Scale](https://doi.org/10.1145/3626772.3661374)|Rankyung Park, Amit Pande, David Relyea, Pushkar Chennu, Prathyusha Kanmanth Reddy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SLH-BIA:+Short-Long+Hawkes+Process+for+Buy+It+Again+Recommendations+at+Scale)|0|
|[Are Embeddings Enough? SIRIP Panel on the Future of Embeddings in Industry IR Systems](https://doi.org/10.1145/3626772.3661360)|Jon Degenhardt, Tracy Holloway King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Embeddings+Enough?+SIRIP+Panel+on+the+Future+of+Embeddings+in+Industry+IR+Systems)|0|
|[Large Language Model Powered Agents for Information Retrieval](https://doi.org/10.1145/3626772.3661375)|An Zhang, Yang Deng, Yankai Lin, Xu Chen, JiRong Wen, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+Powered+Agents+for+Information+Retrieval)|0|
|[High Recall Retrieval Via Technology-Assisted Review](https://doi.org/10.1145/3626772.3661376)|Lenora Gray, David D. Lewis, Jeremy Pickens, Eugene Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High+Recall+Retrieval+Via+Technology-Assisted+Review)|0|
|[Large Language Models for Recommendation: Past, Present, and Future](https://doi.org/10.1145/3626772.3661383)|Keqin Bao, Jizhi Zhang, Xinyu Lin, Yang Zhang, Wenjie Wang, Fuli Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Recommendation:+Past,+Present,+and+Future)|0|
|[Recent Advances in Generative Information Retrieval](https://doi.org/10.1145/3626772.3661379)|Yubao Tang, Ruqing Zhang, Zhaochun Ren, Jiafeng Guo, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Generative+Information+Retrieval)|0|
|[Robust Information Retrieval](https://doi.org/10.1145/3626772.3661380)|YuAn Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Information+Retrieval)|0|
|[IR-RAG @ SIGIR24: Information Retrieval's Role in RAG Systems](https://doi.org/10.1145/3626772.3657984)|Fabio Petroni, Federico Siciliano, Fabrizio Silvestri, Giovanni Trappolini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IR-RAG+@+SIGIR24:+Information+Retrieval's+Role+in+RAG+Systems)|0|
|[A Predictive Framework for Query Reformulation](https://doi.org/10.1145/3626772.3657653)|Reyhaneh Goli||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Predictive+Framework+for+Query+Reformulation)|0|
|[Multimodal Representation and Retrieval [MRR 2024]](https://doi.org/10.1145/3626772.3657987)|Xinliang Zhu, Arnab Dhua, Douglas Gray, I. Zeki Yalniz, Tan Yu, Mohamed Elhoseiny, Bryan Plummer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Representation+and+Retrieval+[MRR+2024])|0|
|[Axiomatic Guidance for Efficient and Controlled Neural Search](https://doi.org/10.1145/3626772.3657651)|Andrew Parry||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Axiomatic+Guidance+for+Efficient+and+Controlled+Neural+Search)|0|
|[Personalized Large Language Models through Parameter Efficient Fine-Tuning Techniques](https://doi.org/10.1145/3626772.3657657)|Marco Braga||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Large+Language+Models+through+Parameter+Efficient+Fine-Tuning+Techniques)|0|
|[Towards a Framework for Legal Case Retrieval](https://doi.org/10.1145/3626772.3657650)|Tebo LeburuDingalo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Framework+for+Legal+Case+Retrieval)|0|
|[Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs](https://doi.org/10.1145/3626772.3657712)|Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+the+Evaluation+of+Dialogue+Systems:+Effects+of+User+Feedback+on+Crowdworkers+and+LLMs)|0|
|[General-Purpose User Modeling with Behavioral Logs: A Snapchat Case Study](https://doi.org/10.1145/3626772.3657908)|Qixiang Fang, Zhihan Zhou, Francesco Barbieri, Yozen Liu, Leonardo Neves, Dong Nguyen, Daniel L. Oberski, Maarten W. Bos, Ron Dotsch||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=General-Purpose+User+Modeling+with+Behavioral+Logs:+A+Snapchat+Case+Study)|0|
|[Neural Passage Quality Estimation for Static Pruning](https://doi.org/10.1145/3626772.3657765)|Xuejun Chang, Debabrata Mishra, Craig Macdonald, Sean MacAvaney||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Passage+Quality+Estimation+for+Static+Pruning)|0|
|[COMI: COrrect and MItigate Shortcut Learning Behavior in Deep Neural Networks](https://doi.org/10.1145/3626772.3657729)|Lili Zhao, Qi Liu, Linan Yue, Wei Chen, Liyi Chen, Ruijun Sun, Chao Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COMI:+COrrect+and+MItigate+Shortcut+Learning+Behavior+in+Deep+Neural+Networks)|0|
|[LLM-enhanced Cascaded Multi-level Learning on Temporal Heterogeneous Graphs](https://doi.org/10.1145/3626772.3657731)|Fengyi Wang, Guanghui Zhu, Chunfeng Yuan, Yihua Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-enhanced+Cascaded+Multi-level+Learning+on+Temporal+Heterogeneous+Graphs)|0|
|[Self-Improving Teacher Cultivates Better Student: Distillation Calibration for Multimodal Large Language Models](https://doi.org/10.1145/3626772.3657692)|Xinwei Li, Li Lin, Shuai Wang, Chen Qian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Improving+Teacher+Cultivates+Better+Student:+Distillation+Calibration+for+Multimodal+Large+Language+Models)|0|
|[Deep Automated Mechanism Design for Integrating Ad Auction and Allocation in Feed](https://doi.org/10.1145/3626772.3657774)|Xuejian Li, Ze Wang, Bingqi Zhu, Fei He, Yongkang Wang, Xingxing Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Automated+Mechanism+Design+for+Integrating+Ad+Auction+and+Allocation+in+Feed)|0|
|[TGOnline: Enhancing Temporal Graph Learning with Adaptive Online Meta-Learning](https://doi.org/10.1145/3626772.3657791)|Ruijie Wang, Jingyuan Huang, Yutong Zhang, Jinyang Li, Yufeng Wang, Wanyu Zhao, Shengzhong Liu, Charith Mendis, Tarek F. Abdelzaher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TGOnline:+Enhancing+Temporal+Graph+Learning+with+Adaptive+Online+Meta-Learning)|0|
|[Intent Distribution based Bipartite Graph Representation Learning](https://doi.org/10.1145/3626772.3657739)|Haojie Li, Wei Wei, Guanfeng Liu, Jinhuan Liu, Feng Jiang, Junwei Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intent+Distribution+based+Bipartite+Graph+Representation+Learning)|0|
|[MTMS: Multi-teacher Multi-stage Knowledge Distillation for Reasoning-Based Machine Reading Comprehension](https://doi.org/10.1145/3626772.3657824)|Zhuo Zhao, Zhiwen Xie, Guangyou Zhou, Jimmy Xiangji Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MTMS:+Multi-teacher+Multi-stage+Knowledge+Distillation+for+Reasoning-Based+Machine+Reading+Comprehension)|0|
|[Exploring the Trade-Off within Visual Information for MultiModal Sentence Summarization](https://doi.org/10.1145/3626772.3657753)|Minghuan Yuan, Shiyao Cui, Xinghua Zhang, Shicheng Wang, Hongbo Xu, Tingwen Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Trade-Off+within+Visual+Information+for+MultiModal+Sentence+Summarization)|0|
|[ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages](https://doi.org/10.1145/3626772.3657891)|Bhawna Piryani, Jamshid Mozafari, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ChroniclingAmericaQA:+A+Large-scale+Question+Answering+Dataset+based+on+Historical+American+Newspaper+Pages)|0|
|[BRB-KMeans: Enhancing Binary Data Clustering for Binary Product Quantization](https://doi.org/10.1145/3626772.3657898)|Suwon Lee, SangMin Choi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BRB-KMeans:+Enhancing+Binary+Data+Clustering+for+Binary+Product+Quantization)|0|
|[Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data Manipulation](https://doi.org/10.1145/3626772.3657976)|Yoori Oh, Yoseob Han, Kyogu Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distance+Sampling-based+Paraphraser+Leveraging+ChatGPT+for+Text+Data+Manipulation)|0|
|[Fake News Detection via Multi-scale Semantic Alignment and Cross-modal Attention](https://doi.org/10.1145/3626772.3657905)|Jiandong Wang, Hongguang Zhang, Chun Liu, Xiongjun Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fake+News+Detection+via+Multi-scale+Semantic+Alignment+and+Cross-modal+Attention)|0|
|[Label Hierarchical Structure-Aware Multi-Label Few-Shot Intent Detection via Prompt Tuning](https://doi.org/10.1145/3626772.3657947)|Xiaotong Zhang, Xinyi Li, Han Liu, Xinyue Liu, Xianchao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Hierarchical+Structure-Aware+Multi-Label+Few-Shot+Intent+Detection+via+Prompt+Tuning)|0|
|[MKV: Mapping Key Semantics into Vectors for Rumor Detection](https://doi.org/10.1145/3626772.3657937)|Yang Li, Liguang Liu, Jiacai Guo, LapKei Lee, Fu Lee Wang, Zhenguo Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MKV:+Mapping+Key+Semantics+into+Vectors+for+Rumor+Detection)|0|
|[PAG-LLM: Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors](https://doi.org/10.1145/3626772.3657959)|Vikas Yadav, Zheng Tang, Vijay Srinivasan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAG-LLM:+Paraphrase+and+Aggregate+with+Large+Language+Models+for+Minimizing+Intent+Classification+Errors)|0|
|[Self-Referential Review: Exploring the Impact of Self-Reference Effect in Review](https://doi.org/10.1145/3626772.3657969)|Kyusik Kim, Hyungwoo Song, Bongwon Suh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Referential+Review:+Exploring+the+Impact+of+Self-Reference+Effect+in+Review)|0|
|[Unbiased Validation of Technology-Assisted Review for eDiscovery](https://doi.org/10.1145/3626772.3657903)|Gordon V. Cormack, Maura R. Grossman, Andrew Harbison, Tom O'Halloran, Bronagh McManus||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Validation+of+Technology-Assisted+Review+for+eDiscovery)|0|
|[Homogeneous-listing-augmented Self-supervised Multimodal Product Title Refinement](https://doi.org/10.1145/3626772.3661347)|Jiaqi Deng, Kaize Shi, Huan Huo, Dingxian Wang, Guandong Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Homogeneous-listing-augmented+Self-supervised+Multimodal+Product+Title+Refinement)|0|
|[GATS: Generative Audience Targeting System for Online Advertising](https://doi.org/10.1145/3626772.3661372)|Cong Jiang, Zhongde Chen, Bo Zhang, Yankun Ren, Xin Dong, Lei Cheng, Xinxing Yang, Longfei Li, Jun Zhou, Linjian Mo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GATS:+Generative+Audience+Targeting+System+for+Online+Advertising)|0|
|[ScienceDirect Topic Pages: A Knowledge Base of Scientific Concepts Across Various Science Domains](https://doi.org/10.1145/3626772.3661353)|Artemis Çapari, Hosein Azarbonyad, Georgios Tsatsaronis, Zubair Afzal, Judson Dunham||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ScienceDirect+Topic+Pages:+A+Knowledge+Base+of+Scientific+Concepts+Across+Various+Science+Domains)|0|
|[GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration](https://doi.org/10.1145/3626772.3657655)|Ben Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GOLF:+Goal-Oriented+Long-term+liFe+tasks+supported+by+human-AI+collaboration)|0|
|[CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks](https://doi.org/10.1145/3626772.3657778)|Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CorpusLM:+Towards+a+Unified+Language+Model+on+Corpus+for+Knowledge-Intensive+Tasks)|0|
|[Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph](https://doi.org/10.1145/3626772.3657706)|Zhiyu Fang, ShuaiLong Lei, Xiaobin Zhu, Chun Yang, ShiXue Zhang, XuCheng Yin, Jingyan Qin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transformer-based+Reasoning+for+Learning+Evolutionary+Chain+of+Events+on+Temporal+Knowledge+Graph)|0|
|[NativE: Multi-modal Knowledge Graph Completion in the Wild](https://doi.org/10.1145/3626772.3657800)|Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, Huajun Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NativE:+Multi-modal+Knowledge+Graph+Completion+in+the+Wild)|0|
|[MetaHKG: Meta Hyperbolic Learning for Few-shot Temporal Reasoning](https://doi.org/10.1145/3626772.3657711)|Ruijie Wang, Yutong Zhang, Jinyang Li, Shengzhong Liu, Dachun Sun, Tianchen Wang, Tianshi Wang, Yizhuo Chen, Denizhan Kara, Tarek F. Abdelzaher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaHKG:+Meta+Hyperbolic+Learning+for+Few-shot+Temporal+Reasoning)|0|
|[YAGO 4.5: A Large and Clean Knowledge Base with a Rich Taxonomy](https://doi.org/10.1145/3626772.3657876)|Fabian M. Suchanek, Mehwish Alam, Thomas Bonald, Lihu Chen, PierreHenri Paris, Jules Soria||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=YAGO+4.5:+A+Large+and+Clean+Knowledge+Base+with+a+Rich+Taxonomy)|0|
|[Uncontextualized significance considered dangerous](https://doi.org/10.1145/3626772.3657827)|Nicola Ferro, Mark Sanderson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncontextualized+significance+considered+dangerous)|0|
|[CIRAL: A Test Collection for CLIR Evaluations in African Languages](https://doi.org/10.1145/3626772.3657884)|Mofetoluwa Adeyemi, Akintunde Oladipo, Xinyu Zhang, David AlfonsoHermelo, Mehdi Rezagholizadeh, Boxing Chen, AbdulHakeem Omotayo, Idris Abdulmumin, Naome A. Etori, Toyib Babatunde Musa, Samuel Fanijo, Oluwabusayo Olufunke Awoyomi, Saheed Abdullahi Salahudeen, Labaran Adamu Mohammed, Daud Olamide Abolade, Falalu Ibrahim Lawan, Maryam Sabo Abubakar, Ruqayya Nasir Iro, Amina Abubakar Imam, Shafie Abdi Mohamed, Hanad Mohamud Mohamed, Tunde Oluwaseyi Ajayi, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CIRAL:+A+Test+Collection+for+CLIR+Evaluations+in+African+Languages)|0|
|[IDGenRec: LLM-RecSys Alignment with Textual ID Learning](https://doi.org/10.1145/3626772.3657821)|Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDGenRec:+LLM-RecSys+Alignment+with+Textual+ID+Learning)|0|
|[Enhanced Packed Marker with Entity Information for Aspect Sentiment Triplet Extraction](https://doi.org/10.1145/3626772.3657734)|You Li, Xupeng Zeng, Yixiao Zeng, Yuming Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhanced+Packed+Marker+with+Entity+Information+for+Aspect+Sentiment+Triplet+Extraction)|0|
|[Exogenous and Endogenous Data Augmentation for Low-Resource Complex Named Entity Recognition](https://doi.org/10.1145/3626772.3657754)|Xinghua Zhang, Gaode Chen, Shiyao Cui, Jiawei Sheng, Tingwen Liu, Hongbo Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exogenous+and+Endogenous+Data+Augmentation+for+Low-Resource+Complex+Named+Entity+Recognition)|0|
|[ACE-2005-PT: Corpus for Event Extraction in Portuguese](https://doi.org/10.1145/3626772.3657872)|Luís Filipe Cunha, Purificação Silvano, Ricardo Campos, Alípio Jorge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACE-2005-PT:+Corpus+for+Event+Extraction+in+Portuguese)|0|
|[Universal Adversarial Perturbations for Vision-Language Pre-trained Models](https://doi.org/10.1145/3626772.3657781)|PengFei Zhang, Zi Huang, Guangdong Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Universal+Adversarial+Perturbations+for+Vision-Language+Pre-trained+Models)|0|
|[Adaptive In-Context Learning with Large Language Models for Bundle Generation](https://doi.org/10.1145/3626772.3657808)|Zhu Sun, Kaidong Feng, Jie Yang, Xinghua Qu, Hui Fang, YewSoon Ong, Wenyuan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+In-Context+Learning+with+Large+Language+Models+for+Bundle+Generation)|0|
|[Beyond Accuracy: Investigating Error Types in GPT-4 Responses to USMLE Questions](https://doi.org/10.1145/3626772.3657882)|Soumyadeep Roy, Aparup Khatua, Fatemeh Ghoochani, Uwe Hadler, Wolfgang Nejdl, Niloy Ganguly||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Accuracy:+Investigating+Error+Types+in+GPT-4+Responses+to+USMLE+Questions)|0|
|[SuicidEmoji: Derived Emoji Dataset and Tasks for Suicide-Related Social Content](https://doi.org/10.1145/3626772.3657852)|Tianlin Zhang, Kailai Yang, Shaoxiong Ji, Boyang Liu, Qianqian Xie, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SuicidEmoji:+Derived+Emoji+Dataset+and+Tasks+for+Suicide-Related+Social+Content)|0|
|[LADy 💃: A Benchmark Toolkit for Latent Aspect Detection Enriched with Backtranslation Augmentation](https://doi.org/10.1145/3626772.3657894)|Farinam Hemmatizadeh, Christine Wong, Alice Yu, Hossein Fani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LADy+💃:+A+Benchmark+Toolkit+for+Latent+Aspect+Detection+Enriched+with+Backtranslation+Augmentation)|0|
|[A Reproducibility Study of PLAID](https://doi.org/10.1145/3626772.3657856)|Sean MacAvaney, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Reproducibility+Study+of+PLAID)|0|
|[Bootstrap Deep Metric for Seed Expansion in Attributed Networks](https://doi.org/10.1145/3626772.3657687)|Chunquan Liang, Yifan Wang, Qiankun Chen, Xinyuan Feng, Luyue Wang, Mei Li, Hongming Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bootstrap+Deep+Metric+for+Seed+Expansion+in+Attributed+Networks)|0|
|[Improving the Accuracy of Locally Differentially Private Community Detection by Order-consistent Data Perturbation](https://doi.org/10.1145/3626772.3657836)|Taolin Guo, Shunshun Peng, Zhejian Zhang, Mengmeng Yang, KwokYan Lam||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Accuracy+of+Locally+Differentially+Private+Community+Detection+by+Order-consistent+Data+Perturbation)|0|
|[CIQA: A Coding Inspired Question Answering Model](https://doi.org/10.1145/3626772.3657830)|Mousa Arraf, Kira Radinsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CIQA:+A+Coding+Inspired+Question+Answering+Model)|0|
|[Let Me Show You Step by Step: An Interpretable Graph Routing Network for Knowledge-based Visual Question Answering](https://doi.org/10.1145/3626772.3657790)|Duokang Wang, Linmei Hu, Rui Hao, Yingxia Shao, Xin Lv, Liqiang Nie, Juanzi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Let+Me+Show+You+Step+by+Step:+An+Interpretable+Graph+Routing+Network+for+Knowledge-based+Visual+Question+Answering)|0|
|[Flexible and Adaptable Summarization via Expertise Separation](https://doi.org/10.1145/3626772.3657789)|Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qingqing Zhu, Rui Yan, Xin Gao, Xiangliang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Flexible+and+Adaptable+Summarization+via+Expertise+Separation)|0|
|[ArabicaQA: A Comprehensive Dataset for Arabic Question Answering](https://doi.org/10.1145/3626772.3657889)|Abdelrahman Abdallah, Mahmoud SalahEldin Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ArabicaQA:+A+Comprehensive+Dataset+for+Arabic+Question+Answering)|0|
|[TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions](https://doi.org/10.1145/3626772.3657855)|Jamshid Mozafari, Anubhav Jangra, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TriviaHG:+A+Dataset+for+Automatic+Hint+Generation+from+Factoid+Questions)|0|
|[Capability-aware Prompt Reformulation Learning for Text-to-Image Generation](https://doi.org/10.1145/3626772.3657787)|Jingtao Zhan, Qingyao Ai, Yiqun Liu, Jia Chen, Shaoping Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capability-aware+Prompt+Reformulation+Learning+for+Text-to-Image+Generation)|0|
|[Short Video Ordering via Position Decoding and Successor Prediction](https://doi.org/10.1145/3626772.3657795)|Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Ziyao Chen, Qing Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Short+Video+Ordering+via+Position+Decoding+and+Successor+Prediction)|0|
|[Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models](https://doi.org/10.1145/3626772.3657698)|Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event+Grounded+Criminal+Court+View+Generation+with+Cooperative+(Large)+Language+Models)|0|
|[Legal Statute Identification: A Case Study using State-of-the-Art Datasets and Methods](https://doi.org/10.1145/3626772.3657879)|Shounak Paul, Rajas Bhatt, Pawan Goyal, Saptarshi Ghosh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Legal+Statute+Identification:+A+Case+Study+using+State-of-the-Art+Datasets+and+Methods)|0|
|[CivilSum: A Dataset for Abstractive Summarization of Indian Court Decisions](https://doi.org/10.1145/3626772.3657859)|Manuj Malik, Zheng Zhao, Marcio Fonseca, Shrisha Rao, Shay B. Cohen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CivilSum:+A+Dataset+for+Abstractive+Summarization+of+Indian+Court+Decisions)|0|
|[Analyzing Fusion Methods Using the Condorcet Rule](https://doi.org/10.1145/3626772.3657912)|Liron Tyomkin, Oren Kurland||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Fusion+Methods+Using+the+Condorcet+Rule)|0|
|[Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange](https://doi.org/10.1145/3626772.3657945)|Ankit Satpute, Noah Gießing, André GreinerPetter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+LLMs+Master+Math?+Investigating+Large+Language+Models+on+Math+Stack+Exchange)|0|
|[Combining Large Language Models and Crowdsourcing for Hybrid Human-AI Misinformation Detection](https://doi.org/10.1145/3626772.3657965)|Xia Zeng, David La Barbera, Kevin Roitero, Arkaitz Zubiaga, Stefano Mizzaro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combining+Large+Language+Models+and+Crowdsourcing+for+Hybrid+Human-AI+Misinformation+Detection)|0|
|[Counterfactual Augmentation for Robust Authorship Representation Learning](https://doi.org/10.1145/3626772.3657956)|Hieu Man, Thien Huu Nguyen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Augmentation+for+Robust+Authorship+Representation+Learning)|0|
|[Enhancing Task Performance in Continual Instruction Fine-tuning Through Format Uniformity](https://doi.org/10.1145/3626772.3657920)|Xiaoyu Tan, Leijun Cheng, Xihe Qiu, Shaojie Shi, Yuan Cheng, Wei Chu, Yinghui Xu, Yuan Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Task+Performance+in+Continual+Instruction+Fine-tuning+Through+Format+Uniformity)|0|
|[From Text to Context: An Entailment Approach for News Stakeholder Classification](https://doi.org/10.1145/3626772.3657901)|Alapan Kuila, Sudeshna Sarkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Text+to+Context:+An+Entailment+Approach+for+News+Stakeholder+Classification)|0|
|[Graph Reasoning Enhanced Language Models for Text-to-SQL](https://doi.org/10.1145/3626772.3657961)|Zheng Gong, Ying Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Reasoning+Enhanced+Language+Models+for+Text-to-SQL)|0|
|[IdmGAE: Importance-Inspired Dynamic Masking for Graph Autoencoders](https://doi.org/10.1145/3626772.3657913)|Ge Chen, Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu, Cuicui Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IdmGAE:+Importance-Inspired+Dynamic+Masking+for+Graph+Autoencoders)|0|
|[Inferring Climate Change Stances from Multimodal Tweets](https://doi.org/10.1145/3626772.3657950)|Nan Bai, Ricardo da Silva Torres, Anna Fensel, Tamara Metze, Art Dewulf||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inferring+Climate+Change+Stances+from+Multimodal+Tweets)|0|
|[Instruction-Guided Bullet Point Summarization of Long Financial Earnings Call Transcripts](https://doi.org/10.1145/3626772.3657948)|Subhendu Khatuya, Koushiki Sinha, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Instruction-Guided+Bullet+Point+Summarization+of+Long+Financial+Earnings+Call+Transcripts)|0|
|[Modeling Scholarly Collaboration and Temporal Dynamics in Citation Networks for Impact Prediction](https://doi.org/10.1145/3626772.3657926)|Pengwei Yan, Yangyang Kang, Zhuoren Jiang, Kaisong Song, Tianqianjin Lin, Changlong Sun, Xiaozhong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Scholarly+Collaboration+and+Temporal+Dynamics+in+Citation+Networks+for+Impact+Prediction)|0|
|[Multi-view Mixed Attention for Contrastive Learning on Hypergraphs](https://doi.org/10.1145/3626772.3657897)|Jongsoo Lee, DongKyu Chae||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-view+Mixed+Attention+for+Contrastive+Learning+on+Hypergraphs)|0|
|[Old IR Methods Meet RAG](https://doi.org/10.1145/3626772.3657935)|Oz Huly, Idan Pogrebinsky, David Carmel, Oren Kurland, Yoelle Maarek||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Old+IR+Methods+Meet+RAG)|0|
|[Prediction of the Realisation of an Information Need: An EEG Study](https://doi.org/10.1145/3626772.3657981)|Niall McGuire, Yashar Moshfeghi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prediction+of+the+Realisation+of+an+Information+Need:+An+EEG+Study)|0|
|[R-ODE: Ricci Curvature Tells When You Will be Informed](https://doi.org/10.1145/3626772.3657954)|Li Sun, Jingbin Hu, Mengjie Li, Hao Peng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R-ODE:+Ricci+Curvature+Tells+When+You+Will+be+Informed)|0|
|[PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking](https://doi.org/10.1145/3626772.3657904)|Yuzhang Xie, Jiaying Lu, Joyce Ho, Fadi B. Nahab, Xiao Hu, Carl Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PromptLink:+Leveraging+Large+Language+Models+for+Cross-Source+Biomedical+Concept+Linking)|0|
|[ReCODE: Modeling Repeat Consumption with Neural ODE](https://doi.org/10.1145/3626772.3657936)|Sunhao Dai, Changle Qu, Sirui Chen, Xiao Zhang, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCODE:+Modeling+Repeat+Consumption+with+Neural+ODE)|0|
|[RLStop: A Reinforcement Learning Stopping Method for TAR](https://doi.org/10.1145/3626772.3657911)|Reem Bin Hezam, Mark Stevenson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RLStop:+A+Reinforcement+Learning+Stopping+Method+for+TAR)|0|
|[Timeline Summarization in the Era of LLMs](https://doi.org/10.1145/3626772.3657899)|Daivik Sojitra, Raghav Jain, Sriparna Saha, Adam Jatowt, Manish Gupta||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Timeline+Summarization+in+the+Era+of+LLMs)|0|
|[TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning](https://doi.org/10.1145/3626772.3657978)|Jing Zhu, Xiang Song, Vassilis N. Ioannidis, Danai Koutra, Christos Faloutsos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TouchUp-G:+Improving+Feature+Representation+through+Graph-Centric+Finetuning)|0|
|[An Integrated Data Processing Framework for Pretraining Foundation Models](https://doi.org/10.1145/3626772.3657671)|Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Integrated+Data+Processing+Framework+for+Pretraining+Foundation+Models)|0|
|[Detecting and Explaining Emotions in Video Advertisements](https://doi.org/10.1145/3626772.3657664)|Joachim Vanneste, Manisha Verma, Debasis Ganguly||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+and+Explaining+Emotions+in+Video+Advertisements)|0|
|[FactCheck Editor: Multilingual Text Editor with End-to-End fact-checking](https://doi.org/10.1145/3626772.3657663)|Vinay Setty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FactCheck+Editor:+Multilingual+Text+Editor+with+End-to-End+fact-checking)|0|
|[Shadowfax: Harnessing Textual Knowledge Base Population](https://doi.org/10.1145/3626772.3657666)|Maxime Prieur, Cédric du Mouza, Guillaume Gadek, Bruno Grilhères||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shadowfax:+Harnessing+Textual+Knowledge+Base+Population)|0|
|[SynDy: Synthetic Dynamic Dataset Generation Framework for Misinformation Tasks](https://doi.org/10.1145/3626772.3657667)|Michael Shliselberg, Ashkan Kazemi, Scott A. Hale, Shiri DoriHacohen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SynDy:+Synthetic+Dynamic+Dataset+Generation+Framework+for+Misinformation+Tasks)|0|
|[TextData: Save What You Know and Find What You Don't](https://doi.org/10.1145/3626772.3657681)|Kevin Ros, Kedar Takwane, Ashwin Patil, Rakshana Jayaprakash, ChengXiang Zhai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TextData:+Save+What+You+Know+and+Find+What+You+Don't)|0|
|[Towards Robust QA Evaluation via Open LLMs](https://doi.org/10.1145/3626772.3657675)|Ehsan Kamalloo, Shivani Upadhyay, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+QA+Evaluation+via+Open+LLMs)|0|
|[Truth-O-Meter: Handling Multiple Inconsistent Sources Repairing LLM Hallucinations](https://doi.org/10.1145/3626772.3657679)|Boris Galitsky, Anton Chernyavskiy, Dmitry I. Ilvovsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Truth-O-Meter:+Handling+Multiple+Inconsistent+Sources+Repairing+LLM+Hallucinations)|0|
|["Ask Me Anything": How Comcast Uses LLMs to Assist Agents in Real Time](https://doi.org/10.1145/3626772.3661345)|Scott Rome, Tianwen Chen, Raphael Tang, Luwei Zhou, Ferhan Ture||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="Ask+Me+Anything":+How+Comcast+Uses+LLMs+to+Assist+Agents+in+Real+Time)|0|
|[unKR: A Python Library for Uncertain Knowledge Graph Reasoning by Representation Learning](https://doi.org/10.1145/3626772.3657661)|Jingting Wang, Tianxing Wu, Shilin Chen, Yunchang Liu, Shutong Zhu, Wei Li, Jingyi Xu, Guilin Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=unKR:+A+Python+Library+for+Uncertain+Knowledge+Graph+Reasoning+by+Representation+Learning)|0|
|[A Field Guide to Automatic Evaluation of LLM-Generated Summaries](https://doi.org/10.1145/3626772.3661346)|Tempest A. van Schaik, Brittany Pugh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Field+Guide+to+Automatic+Evaluation+of+LLM-Generated+Summaries)|0|
|[Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models](https://doi.org/10.1145/3626772.3661361)|Vinay Setty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Surprising+Efficacy+of+Fine-Tuned+Transformers+for+Fact-Checking+over+Larger+Language+Models)|0|
|[Enhancing Baidu Multimodal Advertisement with Chinese Text-to-Image Generation via Bilingual Alignment and Caption Synthesis](https://doi.org/10.1145/3626772.3661350)|Kang Zhao, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wen Tao, Cong Han, Shuanglong Li, Lin Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Baidu+Multimodal+Advertisement+with+Chinese+Text-to-Image+Generation+via+Bilingual+Alignment+and+Caption+Synthesis)|0|
|[Misinformation Mitigation Praxis: Lessons Learned and Future Directions from Co·Insights](https://doi.org/10.1145/3626772.3661352)|Scott A. Hale, Kiran Garimella, Shiri DoriHacohen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Misinformation+Mitigation+Praxis:+Lessons+Learned+and+Future+Directions+from+Co·Insights)|0|
|[Graph-Based Audience Expansion Model for Marketing Campaigns](https://doi.org/10.1145/3626772.3661363)|Md. Mostafizur Rahman, Daisuke Kikuta, Yu Hirate, Toyotaro Suzumura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Based+Audience+Expansion+Model+for+Marketing+Campaigns)|0|
|[Empowering Large Language Models: Tool Learning for Real-World Interaction](https://doi.org/10.1145/3626772.3661381)|Hongru Wang, Yujia Qin, Yankai Lin, Jeff Z. Pan, KamFai Wong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+Large+Language+Models:+Tool+Learning+for+Real-World+Interaction)|0|
|[Large Language Models for Tabular Data: Progresses and Future Directions](https://doi.org/10.1145/3626772.3661384)|Haoyu Dong, Zhiruo Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Tabular+Data:+Progresses+and+Future+Directions)|0|
|[Preventing and Detecting Misinformation Generated by Large Language Models](https://doi.org/10.1145/3626772.3661377)|Aiwei Liu, Qiang Sheng, Xuming Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preventing+and+Detecting+Misinformation+Generated+by+Large+Language+Models)|0|
|[LLM4Eval: Large Language Model for Evaluation in IR](https://doi.org/10.1145/3626772.3657992)|Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM4Eval:+Large+Language+Model+for+Evaluation+in+IR)|0|
|[Machine Generated Explanations and Their Evaluation](https://doi.org/10.1145/3626772.3657652)|Edward Richards||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Generated+Explanations+and+Their+Evaluation)|0|
|[Leveraging LLMs for Detecting and Modeling the Propagation of Misinformation in Social Networks](https://doi.org/10.1145/3626772.3657654)|Payel Santra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+LLMs+for+Detecting+and+Modeling+the+Propagation+of+Misinformation+in+Social+Networks)|0|
|[Mosaicing Prevention in Declassification](https://doi.org/10.1145/3626772.3657656)|Nathaniel Rollings||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mosaicing+Prevention+in+Declassification)|0|
