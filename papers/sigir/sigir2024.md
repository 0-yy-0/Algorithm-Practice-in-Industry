# SIGIR2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation](https://doi.org/10.1145/3626772.3657783)|Alireza Salemi, Surya Kallumadi, Hamed Zamani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimization+Methods+for+Personalizing+Large+Language+Models+through+Retrieval+Augmentation)|4|
|[On Generative Agents in Recommendation](https://doi.org/10.1145/3626772.3657844)|An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Generative+Agents+in+Recommendation)|4|
|[C-Pack: Packed Resources For General Chinese Embeddings](https://doi.org/10.1145/3626772.3657878)|Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, JianYun Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C-Pack:+Packed+Resources+For+General+Chinese+Embeddings)|3|
|[Large Language Models can Accurately Predict Searcher Preferences](https://doi.org/10.1145/3626772.3657707)|Paul Thomas, Seth Spielman, Nick Craswell, Bhaskar Mitra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+can+Accurately+Predict+Searcher+Preferences)|3|
|[Data-efficient Fine-tuning for LLM-based Recommendation](https://doi.org/10.1145/3626772.3657807)|Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Data-efficient+Fine-tuning+for+LLM-based+Recommendation)|2|
|[The Power of Noise: Redefining Retrieval for RAG Systems](https://doi.org/10.1145/3626772.3657834)|Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio Silvestri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Power+of+Noise:+Redefining+Retrieval+for+RAG+Systems)|2|
|[LLaRA: Large Language-Recommendation Assistant](https://doi.org/10.1145/3626772.3657690)|Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLaRA:+Large+Language-Recommendation+Assistant)|2|
|[Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning](https://doi.org/10.1145/3626772.3657828)|Yuyue Zhao, Jiancan Wu, Xiang Wang, Wei Tang, Dingxian Wang, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Let+Me+Do+It+For+You:+Towards+LLM+Empowered+Recommendation+via+Tool+Learning)|2|
|[Evaluating Retrieval Quality in Retrieval-Augmented Generation](https://doi.org/10.1145/3626772.3657957)|Alireza Salemi, Hamed Zamani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Retrieval+Quality+in+Retrieval-Augmented+Generation)|2|
|[Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization](https://doi.org/10.1145/3626772.3657923)|Hamed Zamani, Michael Bendersky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Stochastic+RAG:+End-to-End+Retrieval-Augmented+Generation+through+Expected+Utility+Maximization)|2|
|[What do Users Really Ask Large Language Models? An Initial Log Analysis of Google Bard Interactions in the Wild](https://doi.org/10.1145/3626772.3657914)|Johanne R. Trippas, Sara Fahad Dawood Al Lawati, Joel Mackenzie, Luke Gallagher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+do+Users+Really+Ask+Large+Language+Models?+An+Initial+Log+Analysis+of+Google+Bard+Interactions+in+the+Wild)|2|
|[GraphGPT: Graph Instruction Tuning for Large Language Models](https://doi.org/10.1145/3626772.3657775)|Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GraphGPT:+Graph+Instruction+Tuning+for+Large+Language+Models)|2|
|[UniSAR: Modeling User Transition Behaviors between Search and Recommendation](https://doi.org/10.1145/3626772.3657811)|Teng Shi, Zihua Si, Jun Xu, Xiao Zhang, Xiaoxue Zang, Kai Zheng, Dewei Leng, Yanan Niu, Yang Song|Renmin University of China Gaoling School of Artificial Intelligence; Kuaishou Technology Co., Ltd.|Nowadays, many platforms provide users with both search and recommendation services as important tools for accessing information. The phenomenon has led to a correlation between user search and recommendation behaviors, providing an opportunity to model user interests in a fine-grained way. Existing approaches either model user search and recommendation behaviors separately or overlook the different transitions between user search and recommendation behaviors. In this paper, we propose a framework named UniSAR that effectively models the different types of fine-grained behavior transitions for providing users a Unified Search And Recommendation service. Specifically, UniSAR models the user transition behaviors between search and recommendation through three steps: extraction, alignment, and fusion, which are respectively implemented by transformers equipped with pre-defined masks, contrastive learning that aligns the extracted fine-grained user transitions, and cross-attentions that fuse different transitions. To provide users with a unified service, the learned representations are fed into the downstream search and recommendation models. Joint learning on both search and recommendation data is employed to utilize the knowledge and enhance each other. Experimental results on two public datasets demonstrated the effectiveness of UniSAR in terms of enhancing both search and recommendation simultaneously. The experimental analysis further validates that UniSAR enhances the results by successfully modeling the user transition behaviors between search and recommendation.|目前，许多平台为用户提供搜索和推荐服务，作为获取信息的重要工具。这种现象导致了用户搜索和推荐行为之间的相关性，为用户兴趣的细粒度建模提供了机会。现有的方法或者分别模拟用户搜索和推荐行为，或者忽略用户搜索和推荐行为之间的不同转换。在本文中，我们提出一个名为 UniSAR 的框架，有效地模拟不同类型的细粒度行为转换，为用户提供一个统一的搜索和推荐服务。具体而言，UniSAR 通过三个步骤对搜索和推荐之间的用户转换行为进行建模: 提取、对齐和融合，这些步骤分别由配备预定义掩码的变压器实现，对比学习将提取的细粒度用户转换对齐，交叉注意融合不同的转换。为了向用户提供统一的服务，学习表示被反馈到下游搜索和推荐模型中。对搜索数据和推荐数据进行联合学习，以利用知识并相互增强。在两个公共数据集上的实验结果证明了 UniSAR 在同时提高搜索和推荐能力方面的有效性。实验分析进一步验证了 UniSAR 通过成功地模拟用户在搜索和推荐之间的转换行为，提高了结果的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UniSAR:+Modeling+User+Transition+Behaviors+between+Search+and+Recommendation)|1|
|[Poisoning Decentralized Collaborative Recommender System and Its Countermeasures](https://doi.org/10.1145/3626772.3657814)|Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, Hongzhi Yin|The University of Queensland; University of Electronic Science and Technology of China; Southern University of Science and Technology; The University of Queensland School of Electrical Engineering and Computer Science|To make room for privacy and efficiency, the deployment of many recommender systems is experiencing a shift from central servers to personal devices, where the federated recommender systems (FedRecs) and decentralized collaborative recommender systems (DecRecs) are arguably the two most representative paradigms. While both leverage knowledge (e.g., gradients) sharing to facilitate learning local models, FedRecs rely on a central server to coordinate the optimization process, yet in DecRecs, the knowledge sharing directly happens between clients. Knowledge sharing also opens a backdoor for model poisoning attacks, where adversaries disguise themselves as benign clients and disseminate polluted knowledge to achieve malicious goals like promoting an item's exposure rate. Although research on such poisoning attacks provides valuable insights into finding security loopholes and corresponding countermeasures, existing attacks mostly focus on FedRecs, and are either inapplicable or ineffective for DecRecs. Compared with FedRecs where the tampered information can be universally distributed to all clients once uploaded to the cloud, each adversary in DecRecs can only communicate with neighbor clients of a small size, confining its impact to a limited range. To fill the gap, we present a novel attack method named Poisoning with Adaptive Malicious Neighbors (PAMN). With item promotion in top-K recommendation as the attack objective, PAMN effectively boosts target items' ranks with several adversaries that emulate benign clients and transfers adaptively crafted gradients conditioned on each adversary's neighbors. Moreover, with the vulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on user-level gradient clipping with sparsified updating is proposed. Extensive experiments demonstrate the effectiveness of the poisoning attack and the robustness of our defensive mechanism.|为了给隐私和效率腾出空间，许多推荐系统的部署正在经历从中央服务器到个人设备的转变，其中联邦推荐系统(FedRecs)和分散式协作推荐系统(DecRecs)可以说是两个最具代表性的范例。虽然两者都利用知识共享(例如，梯度)来促进本地模型的学习，FedRecs 依赖于一个中央服务器来协调优化过程，但在 DecRecs 中，知识共享直接发生在客户之间。知识共享还为模型中毒攻击打开了一个后门，在这种攻击中，对手把自己伪装成良性的客户，传播受污染的知识，以达到恶意目的，比如提高项目的曝光率。尽管对这类中毒攻击的研究为发现安全漏洞和相应的对策提供了有价值的见解，但现有的攻击主要集中在 FedRecs 上，对 DecRecs 要么不适用，要么无效。与 FedRecs 相比，DecRecs 中的每个对手只能与小规模的邻居客户机通信，将其影响限制在有限的范围内。为了填补这一空白，我们提出了一种新的攻击方法，称为自适应恶意邻居中毒(PAMN)。通过在 top-K 推荐中的物品推广作为攻击目标，PAMN 可以有效地提高目标物品的等级，其中有几个对手可以模仿良性客户，并根据每个对手的邻居传输自适应的精心制作的渐变。此外，针对 DecRecs 的漏洞，提出了一种基于用户级梯度裁剪和稀疏更新的专用防御机制。大量的实验证明了中毒攻击的有效性和我们防御机制的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Poisoning+Decentralized+Collaborative+Recommender+System+and+Its+Countermeasures)|1|
|[Resources for Combining Teaching and Research in Information Retrieval Coursework](https://doi.org/10.1145/3626772.3657886)|Maik Fröbe, Harrisen Scells, Theresa Elstner, Christopher Akiki, Lukas Gienapp, Jan Heinrich Reimer, Sean MacAvaney, Benno Stein, Matthias Hagen, Martin Potthast||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resources+for+Combining+Teaching+and+Research+in+Information+Retrieval+Coursework)|1|
|[Leveraging LLMs for Unsupervised Dense Retriever Ranking](https://doi.org/10.1145/3626772.3657798)|Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+LLMs+for+Unsupervised+Dense+Retriever+Ranking)|1|
|[Large Language Models for Intent-Driven Session Recommendations](https://doi.org/10.1145/3626772.3657688)|Zhu Sun, Hongyang Liu, Xinghua Qu, Kaidong Feng, Yan Wang, Yew Soon Ong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Intent-Driven+Session+Recommendations)|1|
|[Scalable Community Search over Large-scale Graphs based on Graph Transformer](https://doi.org/10.1145/3626772.3657771)|Yuxiang Wang, Xiaoxuan Gou, Xiaoliang Xu, Yuxia Geng, Xiangyu Ke, Tianxing Wu, Zhiyuan Yu, Runhuai Chen, Xiangying Wu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scalable+Community+Search+over+Large-scale+Graphs+based+on+Graph+Transformer)|1|
|[LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction](https://doi.org/10.1145/3626772.3661357)|Chenhao Fang, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Kaushiki Nag, Evren Körpeoglu, Sushant Kumar, Kannan Achan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-Ensemble:+Optimal+Large+Language+Model+Ensemble+Method+for+E-commerce+Product+Attribute+Value+Extraction)|1|
|[Question Suggestion for Conversational Shopping Assistants Using Product Metadata](https://doi.org/10.1145/3626772.3661371)|Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Question+Suggestion+for+Conversational+Shopping+Assistants+Using+Product+Metadata)|1|
|[A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models](https://doi.org/10.1145/3626772.3657813)|Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Setwise+Approach+for+Effective+and+Highly+Efficient+Zero-shot+Ranking+with+Large+Language+Models)|1|
|[Ranked List Truncation for Large Language Model-based Re-Ranking](https://doi.org/10.1145/3626772.3657864)|Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranked+List+Truncation+for+Large+Language+Model-based+Re-Ranking)|1|
|[Fine-grained Textual Inversion Network for Zero-Shot Composed Image Retrieval](https://doi.org/10.1145/3626772.3657831)|Haoqiang Lin, Haokun Wen, Xuemeng Song, Meng Liu, Yupeng Hu, Liqiang Nie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-grained+Textual+Inversion+Network+for+Zero-Shot+Composed+Image+Retrieval)|1|
|[Denoising Diffusion Recommender Model](https://doi.org/10.1145/3626772.3657825)|Jujia Zhao, Wenjie Wang, Yiyan Xu, Teng Sun, Fuli Feng, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Denoising+Diffusion+Recommender+Model)|1|
|[Systematic Evaluation of Neural Retrieval Models on the Touché 2020 Argument Retrieval Subset of BEIR](https://doi.org/10.1145/3626772.3657861)|Nandan Thakur, Luiz Bonifacio, Maik Fröbe, Alexander Bondarenko, Ehsan Kamalloo, Martin Potthast, Matthias Hagen, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Systematic+Evaluation+of+Neural+Retrieval+Models+on+the+Touché+2020+Argument+Retrieval+Subset+of+BEIR)|1|
|[Generative Retrieval as Multi-Vector Dense Retrieval](https://doi.org/10.1145/3626772.3657697)|Shiguang Wu, Wenda Wei, Mengqi Zhang, Zhumin Chen, Jun Ma, Zhaochun Ren, Maarten de Rijke, Pengjie Ren||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Retrieval+as+Multi-Vector+Dense+Retrieval)|1|
|[A Workbench for Autograding Retrieve/Generate Systems](https://doi.org/10.1145/3626772.3657871)|Laura Dietz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Workbench+for+Autograding+Retrieve/Generate+Systems)|1|
|[Evaluating Generative Ad Hoc Information Retrieval](https://doi.org/10.1145/3626772.3657849)|Lukas Gienapp, Harrisen Scells, Niklas Deckers, Janek Bevendorff, Shuai Wang, Johannes Kiesel, Shahbaz Syed, Maik Fröbe, Guido Zuccon, Benno Stein, Matthias Hagen, Martin Potthast||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Generative+Ad+Hoc+Information+Retrieval)|1|
|[Embark on DenseQuest: A System for Selecting the Best Dense Retriever for a Custom Collection](https://doi.org/10.1145/3626772.3657674)|Ekaterina Khramtsova, Teerapong Leelanupab, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embark+on+DenseQuest:+A+System+for+Selecting+the+Best+Dense+Retriever+for+a+Custom+Collection)|1|
|[QuanTemp: A real-world open-domain benchmark for fact-checking numerical claims](https://doi.org/10.1145/3626772.3657874)|Venktesh V, Abhijit Anand, Avishek Anand, Vinay Setty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=QuanTemp:+A+real-world+open-domain+benchmark+for+fact-checking+numerical+claims)|1|
|[Instruction-based Hypergraph Pretraining](https://doi.org/10.1145/3626772.3657715)|Mingdai Yang, Zhiwei Liu, Liangwei Yang, Xiaolong Liu, Chen Wang, Hao Peng, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Instruction-based+Hypergraph+Pretraining)|1|
|[Characterizing Information Seeking Processes with Multiple Physiological Signals](https://doi.org/10.1145/3626772.3657793)|Kaixin Ji, Danula Hettiachchi, Flora D. Salim, Falk Scholer, Damiano Spina||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Characterizing+Information+Seeking+Processes+with+Multiple+Physiological+Signals)|1|
|[Resources for Brewing BEIR: Reproducible Reference Models and Statistical Analyses](https://doi.org/10.1145/3626772.3657862)|Ehsan Kamalloo, Nandan Thakur, Carlos Lassance, Xueguang Ma, JhengHong Yang, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Resources+for+Brewing+BEIR:+Reproducible+Reference+Models+and+Statistical+Analyses)|1|
|[On the Evaluation of Machine-Generated Reports](https://doi.org/10.1145/3626772.3657846)|James Mayfield, Eugene Yang, Dawn J. Lawrie, Sean MacAvaney, Paul McNamee, Douglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Selin Kayi, Kate Sanders, Marc Mason, Noah Hibbler||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Evaluation+of+Machine-Generated+Reports)|1|
|[Are Large Language Models Good at Utility Judgments?](https://doi.org/10.1145/3626772.3657784)|Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Large+Language+Models+Good+at+Utility+Judgments?)|1|
|[CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to Web Relevance Ranking](https://doi.org/10.1145/3626772.3657851)|Josef Vonásek, Milan Straka, Rostislav Krc, Lenka Lasonová, Ekaterina Egorova, Jana Straková, Jakub Náplava|Seznam.cz; Institute of Formal and Applied Linguistics, Charles University|We present CWRCzech, Click Web Ranking dataset for Czech, a 100M query-document Czech click dataset for relevance ranking with user behavior data collected from search engine logs of Seznam.cz. To the best of our knowledge, CWRCzech is the largest click dataset with raw text published so far. It provides document positions in the search results as well as information about user behavior: 27.6M clicked documents and 10.8M dwell times. In addition, we also publish a manually annotated Czech test for the relevance task, containing nearly 50k query-document pairs, each annotated by at least 2 annotators. Finally, we analyze how the user behavior data improve relevance ranking and show that models trained on data automatically harnessed at sufficient scale can surpass the performance of models trained on human annotated data. CWRCzech is published under an academic non-commercial license and is available to the research community at https://github.com/seznam/CWRCzech.|我们为捷克提供了一个100M 的查询文档捷克点击数据集，用于从 Seznam.cz 的搜索引擎日志中收集的用户行为数据进行相关性排名。据我们所知，CWR 捷克是迄今为止发布原始文本的最大的点击数据集。它提供了搜索结果中的文档位置以及关于用户行为的信息: 27.6 M 的单击文档和10.8 M 的停留时间。此外，我们还为相关任务发布了一个手动注释的捷克测试，包含近50k 个查询-文档对，每个查询-文档对至少由2个注释者进行注释。最后，我们分析了用户行为数据如何提高相关性排名，并表明在足够大的规模上自动利用数据训练的模型可以超过在人类注释数据上训练的模型的性能。捷克语研究中心以学术非商业许可证的形式发表论文，研究团体可以在 https://github.com/seznam/CWRCzech 获得该论文。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CWRCzech:+100M+Query-Document+Czech+Click+Dataset+and+Its+Application+to+Web+Relevance+Ranking)|0|
|[A Unified Search and Recommendation Framework Based on Multi-Scenario Learning for Ranking in E-commerce](https://doi.org/10.1145/3626772.3661356)|Jinhan Liu, Qiyu Chen, Junjie Xu, Junjie Li, Baoli Li, Sulong Xu|JD or JD.com; JD|Search and recommendation (S R) are the two most important scenarios in e-commerce. The majority of users typically interact with products in S R scenarios, indicating the need and potential for joint modeling. Traditional multi-scenario models use shared parameters to learn the similarity of multiple tasks, and task-specific parameters to learn the divergence of individual tasks. This coarse-grained modeling approach does not effectively capture the differences between S R scenarios. Furthermore, this approach does not sufficiently exploit the information across the global label space. These issues can result in the suboptimal performance of multi-scenario models in handling both S R scenarios. To address these issues, we propose an effective and universal framework for Unified Search and Recommendation (USR), designed with S R Views User Interest Extractor Layer (IE) and S R Views Feature Generator Layer (FG) to separately generate user interests and scenario-agnostic feature representations for S R. Next, we introduce a Global Label Space Multi-Task Layer (GLMT) that uses global labels as supervised signals of auxiliary tasks and jointly models the main task and auxiliary tasks using conditional probability. Extensive experimental evaluations on real-world industrial datasets show that USR can be applied to various multi-scenario models and significantly improve their performance. Online A/B testing also indicates substantial performance gains across multiple metrics. Currently, USR has been successfully deployed in the 7Fresh App.|搜索和推荐(S R)是电子商务中最重要的两种情况。大多数用户通常在 S R 场景中与产品交互，这表明了联合建模的需要和潜力。传统的多场景模型使用共享参数来学习多个任务的相似性，使用特定任务的参数来学习单个任务的差异性。这种粗粒度建模方法不能有效地捕获 S R 场景之间的差异。此外，这种方法不能充分利用全局标签空间中的信息。这些问题可能导致多场景模型在处理两个 S R 场景时的次优性能。为了解决这些问题，我们提出了一个有效和通用的统一搜索和推荐(USR)框架，该框架使用 S R 视图用户兴趣提取层(IE)和 S R 视图特征生成层(FG)分别生成用户兴趣和场景无关的特征表示。接下来，我们引入了一个全局标签空间多任务层(GLMT) ，它使用全局标签作为辅助任务的监督信号，并使用条件概率联合建模主要任务和辅助任务。对现实世界工业数据集的大量实验评估表明，USR 可以应用于各种多场景模型，并显著提高其性能。在线 A/B 测试还表明跨多个指标的性能显著提高。目前，USR 已经成功地部署在7Fresh 应用程序中。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Unified+Search+and+Recommendation+Framework+Based+on+Multi-Scenario+Learning+for+Ranking+in+E-commerce)|0|
|[Improving Embedding-Based Retrieval in Friend Recommendation with ANN Query Expansion](https://doi.org/10.1145/3626772.3661367)|Pau PerngHwa Kung, Zihao Fan, Tong Zhao, Yozen Liu, Zhixin Lai, Jiahui Shi, Yan Wu, Jun Yu, Neil Shah, Ganesh Venkataraman|Snap|Embedding-based retrieval in graph-based recommendation has shown great improvements over traditional graph walk retrieval methods, and has been adopted in large-scale industry applications such as friend recommendations [16]. However, it is not without its challenges: retraining graph embeddings frequently due to changing data is slow and costly, and producing high recall of approximate nearest neighbor search (ANN) on such embeddings is challenging due to the power law distribution of the indexed users. In this work, we address theses issues by introducing a simple query expansion method in ANN, called FriendSeedSelection, where for each node query, we construct a set of 1-hop embeddings and run ANN search. We highlight our approach does not require any model-level tuning, and is inferred from the data at test-time. This design choice effectively enables our recommendation system to adapt to the changing graph distribution without frequent heavy model retraining. We also discuss how we design our system to efficiently construct such queries online to support 10k+ QPS. For friend recommendation, our method shows improvements of recall, and 11% relative friend reciprocated communication metric gains, now serving over 800 million monthly active users at Snapchat.|在基于图的推荐中嵌入式检索已经显示出对传统的图步检索方法的巨大改进，并且已经被大规模的工业应用如好友推荐所采用[16]。然而，这并非没有挑战: 由于数据的变化而频繁地重新训练图嵌入是缓慢和昂贵的，并且由于索引用户的幂律分布，在这种嵌入上产生高召回的近似最近邻搜索(ANN)是具有挑战性的。在这项工作中，我们通过在人工神经网络中引入一个简单的查询扩展方法，称为 FriendSeedSelection，对于每个节点查询，我们构造一组1跳嵌入并运行人工神经网络搜索来解决这些问题。我们强调我们的方法不需要任何模型级别的调优，并且是从测试时的数据中推断出来的。这种设计选择有效地使我们的推荐系统能够适应变化的图形分布，而不需要频繁的重模型再训练。我们还讨论了如何设计我们的系统，以有效地构建这样的查询在线支持10k + QPS。对于朋友推荐，我们的方法显示了回忆的改进，11% 的亲属朋友回馈了通信指标的收益，现在为 Snapchat 超过8亿的月活跃用户提供服务。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+Embedding-Based+Retrieval+in+Friend+Recommendation+with+ANN+Query+Expansion)|0|
|[Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search](https://doi.org/10.1145/3626772.3657815)|Hideaki Joko, Shubham Chatterjee, Andrew Ramsay, Arjen P. de Vries, Jeff Dalton, Faegheh Hasibi|University of Glasgow; Radboud University; University of Edinburgh|The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.|会话代理的未来将为用户提供个性化的信息响应。然而，在开发模型方面的一个重大挑战是缺乏跨越多个会议并反映真实世界用户偏好的大规模对话数据集。以前的方法依赖于难以伸缩的绿色向导设置中的专家，特别是对于个性化任务。我们的方法 LAPS 通过使用大型语言模型(LLM)来指导单个人类工作者生成个性化对话来解决这个问题。这种方法已被证明可以加快创作过程，提高质量。LAPS 可以收集大规模、人工编写、多会话和多域会话，包括提取用户首选项。与现有的数据集相比，LAPS 产生的对话和专家创建的对话一样自然和多样化，这与完全合成的方法形成了对比。采集的数据集适合于训练偏好提取和个性化响应生成。我们的研究结果表明，明确使用提取的偏好生成的响应更好地匹配用户的实际偏好，突出了使用提取的偏好的价值超过简单的对话历史。总的来说，LAPS 引入了一种新的方法，利用 LLM 创建真实的个性化会话数据，比以前的方法更有效率和效果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Doing+Personal+LAPS:+LLM-Augmented+Dialogue+Construction+for+Personalized+Multi-Session+Conversational+Search)|0|
|[Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3626772.3657733)|Alireza Salemi, Hamed Zamani|University of Massachusetts Amherst|This paper introduces uRAG–a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.|本文介绍了一个统一检索引擎的框架 uRAG，它可以为多个下游检索增强生成(RAG)系统提供服务。每个 RAG 系统都为一个独特的目的使用检索结果，例如开放域问题回答、事实验证、实体链接和关系提取。我们引入了一个通用的培训指导方针，它标准化了搜索引擎与下游 RAG 系统之间的通信，这些 RAG 系统参与优化检索模型。这为我们建立一个大规模实验生态系统奠定了基础，该系统包括18个参与培训的 RAG 系统和18个使用 uRAG 作为搜索引擎新用户的未知 RAG 系统。利用这个实验生态系统，我们回答了许多基础研究问题，这些问题提高了我们对开发机器搜索引擎的承诺和挑战的理解。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Search+Engine+for+Machines:+Unified+Ranking+for+Multiple+Retrieval-Augmented+Large+Language+Models)|0|
|[Sequential Recommendation with Collaborative Explanation via Mutual Information Maximization](https://doi.org/10.1145/3626772.3657770)|Yi Yu, Kazunari Sugiyama, Adam Jatowt|Kyoto University; University of Innsbruck; Osaka Seikei University|Current research on explaining sequential recommendations lacks reliable benchmarks and quantitative metrics, making it difficult to compare explanation performance between different models. In this work, we propose a new explanation type, namely, collaborative explanation, into sequential recommendation, allowing a unified approach for modeling user actions and assessing the performance of both recommendation and explanation. We accomplish this by framing the problem as a joint sequential prediction task, which takes a sequence of user's past item-explanation pairs and predicts the next item along with its associated explanation. We propose a pipeline that comprises data preparation and a model adaptation framework called Sequential recommendation with Collaborative Explanation (SCE). This framework can be flexibly applied to any sequential recommendation model for this problem. Furthermore, to address the issue of inconsistency between item and explanation representations when learning both sub-tasks, we propose Sequential recommendation with Collaborative Explanation via Mutual Information Maximization (SCEMIM). Our extensive experiments demonstrate that: (i) SCE framework is effective in enabling sequential models to make recommendations and provide accurate explanations. (ii) Importantly, SCEMIM enhances the consistency between recommendations and explanations, leading to further improvements in the performance of both sub-tasks.|目前关于解释顺序推荐的研究缺乏可靠的基准和量化指标，因此难以比较不同模型之间的解释性能。在这项工作中，我们提出了一个新的解释类型，即协作解释，到顺序推荐，允许一个统一的方法来建模用户的行为和评估两者的性能的推荐和解释。我们通过将问题框架为一个联合的顺序预测任务来完成这个任务，该任务采用用户过去的项目解释对的序列，并预测下一个项目及其相关的解释。我们提出了一个流水线，包括数据准备和模型适应框架称为顺序推荐与协作解释(SCE)。该框架可以灵活地应用于该问题的任何顺序推荐模型。此外，为了解决两个子任务学习过程中项目表征与解释表征不一致的问题，本文提出了基于互信息最大化的协同解释的序贯推荐方法。我们的大量实验表明: (i) SCE 框架能够有效地使序贯模型提出建议并提供准确的解释。(ii)重要的是，SCEMIM 加强了建议和解释之间的一致性，从而进一步改善了这两个子任务的表现。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Recommendation+with+Collaborative+Explanation+via+Mutual+Information+Maximization)|0|
|[A Learning-to-Rank Formulation of Clustering-Based Approximate Nearest Neighbor Search](https://doi.org/10.1145/3626772.3657931)|Thomas Vecchiato, Claudio Lucchese, Franco Maria Nardini, Sebastian Bruch|Ca' Foscari University of Venice; Pinecone; ISTI-CNR|A critical piece of the modern information retrieval puzzle is approximate nearest neighbor search. Its objective is to return a set of k data points that are closest to a query point, with its accuracy measured by the proportion of exact nearest neighbors captured in the returned set. One popular approach to this question is clustering: The indexing algorithm partitions data points into non-overlapping subsets and represents each partition by a point such as its centroid. The query processing algorithm first identifies the nearest clusters – a process known as routing – then performs a nearest neighbor search over those clusters only. In this work, we make a simple observation: The routing function solves a ranking problem. Its quality can therefore be assessed with a ranking metric, making the function amenable to learning-to-rank. Interestingly, ground-truth is often freely available: Given a query distribution in a top-k configuration, the ground-truth is the set of clusters that contain the exact top-k vectors. We develop this insight and apply it to Maximum Inner Product Search (MIPS). As we demonstrate empirically on various datasets, learning a simple linear function consistently improves the accuracy of clustering-based MIPS.|现代信息检索难题的一个关键部分是近似最近邻搜索。它的目标是返回一组最接近查询点的 k 个数据点，其精度由返回集中捕获的精确最近邻点的比例来衡量。解决这个问题的一种流行方法是聚类: 索引算法将数据分割成不重叠的子集，并用一个点(如其质心)表示每个分区。查询处理算法首先识别最近的集群——一个称为路由的过程——然后仅对这些集群执行最近邻搜索。在这项工作中，我们做了一个简单的观察: 路由函数解决了一个排序问题。因此，它的质量可以评估与排名度量，使功能适合学习到排名。有趣的是，地面真相通常是免费提供的: 给定 top-k 配置中的查询分布，地面真相是包含精确 top-k 向量的集合。我们开发了这种洞察力，并将其应用于最大内部产品搜索(MIPS)。正如我们在各种数据集上的经验证明，学习一个简单的线性函数可以持续地提高基于聚类的 MIPS 的准确性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Learning-to-Rank+Formulation+of+Clustering-Based+Approximate+Nearest+Neighbor+Search)|0|
|[A Surprisingly Simple yet Effective Multi-Query Rewriting Method for Conversational Passage Retrieval](https://doi.org/10.1145/3626772.3657933)|Ivica Kostric, Krisztian Balog|University of Stavanger; University of Stavanger & Google Research|Conversational passage retrieval is challenging as it often requires the resolution of references to previous utterances and needs to deal with the complexities of natural language, such as coreference and ellipsis. To address these challenges, pre-trained sequence-to-sequence neural query rewriters are commonly used to generate a single de-contextualized query based on conversation history. Previous research shows that combining multiple query rewrites for the same user utterance has a positive effect on retrieval performance. We propose the use of a neural query rewriter to generate multiple queries and show how to integrate those queries in the passage retrieval pipeline efficiently. The main strength of our approach lies in its simplicity: it leverages how the beam search algorithm works and can produce multiple query rewrites at no additional cost. Our contributions further include devising ways to utilize multi-query rewrites in both sparse and dense first-pass retrieval. We demonstrate that applying our approach on top of a standard passage retrieval pipeline delivers state-of-the-art performance without sacrificing efficiency.|会话短文检索是一个具有挑战性的问题，因为它往往需要解决对以前话语的引用，并需要处理自然语言的复杂性，如共引和省略。为了应对这些挑战，预先训练的序列到序列神经查询重写器通常用于生成基于会话历史的单个去上下文化查询。以往的研究表明，对同一用户语句进行多次查询重写对检索性能有积极的影响。我们提出使用神经查询重写器来生成多个查询，并说明如何有效地将这些查询集成到文章检索流水线中。我们方法的主要优点在于它的简单性: 它利用了束搜索算法的工作方式，并且可以在不增加成本的情况下产生多个查询重写。我们的贡献还包括设计在稀疏和密集首通检索中利用多查询重写的方法。我们演示了将我们的方法应用于标准通道检索流水线之上，可以在不牺牲效率的情况下提供最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Surprisingly+Simple+yet+Effective+Multi-Query+Rewriting+Method+for+Conversational+Passage+Retrieval)|0|
|[Memory-Efficient Deep Recommender Systems using Approximate Rotary Compositional Embedding](https://doi.org/10.1145/3626772.3657953)|Dongning Ma, Xun Jiao|Villanova University Electrical and Computer Engineering; Villanova University ECE|Embedding tables in deep recommender systems (DRS) process categorical data, which can be memory-intensive due to the high feature cardinality. In this paper, we propose Approximate Rotary Compositional Embedding (ARCE), which intentionally trades off performance to aggressively reduce the size of the embedding tables. Specifically, ARCE uses compositional embedding to split large embedding tables into smaller compositions and replaces index look-ups with vector rotations. To regain the performance loss of this trade-off, ARCE features an input approximation where one index is mapped into multiple indices, creating a larger space for a potential increased learning capability. Experimental results show that using ARCE can reduce the memory overhead of embedding tables in DRS by more than 1000x with less than 3% performance loss, highlighting the potential of using ARCE for less memory intensive DRS designs. We open-source ARCE at https://github.com/VU-DETAIL/arce.|深度推荐系统(DRS)中嵌入表处理分类数据，由于特征基数高，可能会占用大量内存。在本文中，我们提出了近似旋转组合嵌入(ARCE) ，它有意地牺牲性能以积极地减少嵌入表的大小。具体来说，ARCE 使用组合嵌入将大型嵌入表拆分为较小的组合，并用向量旋转替换索引查找。为了重新获得这种折衷的性能损失，ARCE 采用了一种输入近似，其中一个索引映射到多个索引中，为潜在的增强的学习能力创造了更大的空间。实验结果表明，使用 ARCE 可以将 DRS 中嵌入表的内存开销减少1000倍以上，性能损失小于3% ，突出了使用 ARCE 进行内存密集型 DRS 设计的潜力。我们开源的 ARCE  https://github.com/vu-detail/ARCE。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Memory-Efficient+Deep+Recommender+Systems+using+Approximate+Rotary+Compositional+Embedding)|0|
|[Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking](https://doi.org/10.1145/3626772.3657670)|Sara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, Anton Korikov, Scott Sanner|University of Waterloo; University of Toronto|Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., "I'm watching my weight"). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., "classy joint for a date"). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.|会话推荐系统必须理解用户偏好和意图的丰富多样的自然语言(NL)表达，通常以间接的方式进行沟通(例如，“我在减肥”)。这种复杂的语句使得检索相关项目变得具有挑战性，特别是如果仅仅使用不完整或过时的元数据。幸运的是，许多域名都有丰富的项目评论，涵盖标准的元数据类别，并提供可能符合用户兴趣的复杂意见(例如，“优雅的约会联合”)。然而，直到最近才有了大型语言模型(LLM) ，让我们能够在用户生成的评论中解开用户偏好话语和复杂语言之间的常识性联系。此外，LLM 为半结构化对话状态跟踪、复杂意图和偏好理解以及生成建议、解释和问题答案提供了新的范例。因此，我们引入了一种新技术 RA-Rec，这是一种用于 ConvRec 的恢复增强的 LLM 驱动的对话状态跟踪系统，通过一个视频、开源 GitHub 仓库和交互式 Google Colab 笔记本进行了展示。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Conversational+Recommendation+with+Prompt-based+Semi-Structured+Natural+Language+State+Tracking)|0|
|[LLMGR: Large Language Model-based Generative Retrieval in Alipay Search](https://doi.org/10.1145/3626772.3661364)|Chen Wei, Yixin Ji, Zeyuan Chen, Jia Xu, Zhongyi Liu|Ant Group Search Recommendation Technology Department; Soochow University School of Computer Science & Technology; Ant Group|The search system aims to help users quickly find items according to queries they enter, which includes the retrieval and ranking modules. Traditional retrieval is a multi-stage process, including indexing and sorting, which cannot be optimized end-to-end. With the real data about mini-apps in the Alipay search, we find that many complex queries fail to display the relevant mini-apps, seriously threatening users' search experience. To address the challenges, we propose a Large Language Model-based Generative Retrieval (LLMGR) approach for retrieving mini-app candidates. The information of the mini-apps is encoded into the large model, and the title of the mini-app is directly generated. Through the online A/B test in Alipay search, LLMGR as a supplementary source has statistically significant improvements in the Click-Through Rate (CTR) of the search system compared to traditional methods. In this paper, we have deployed a novel retrieval method for the Alipay search system and demonstrated that generative retrieval methods based on LLM can improve the performance of search system, particularly for complex queries, which have an average increase of 0.2% in CTR.|该搜索系统旨在帮助用户根据输入的查询快速查找项目，其中包括检索和排序模块。传统的检索是一个多阶段的过程，包括索引和排序，不能实现端到端的优化。通过对支付宝搜索中迷你应用的真实数据进行分析，我们发现许多复杂的查询都无法显示相关的迷你应用，严重威胁了用户的搜索体验。为了应对这些挑战，我们提出了一种基于大语言模型的生成检索(LLMGR)方法来检索迷你应用程序候选者。迷你应用程序的信息被编码到大模型中，并直接生成迷你应用程序的标题。通过支付宝搜索中的在线 A/B 测试，作为补充来源的 LLMGR 在统计学上显著改善了搜索系统的点进率(ctrr) ，而不是传统方法。本文针对支付宝搜索系统提出了一种新的检索方法，并证明了基于 LLM 的生成式检索方法可以提高搜索系统的性能，尤其是对于平均点击率提高0.2% 的复杂查询。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLMGR:+Large+Language+Model-based+Generative+Retrieval+in+Alipay+Search)|0|
|[Optimizing E-commerce Search: Toward a Generalizable and Rank-Consistent Pre-Ranking Model](https://doi.org/10.1145/3626772.3661343)|Enqiang Xu, Yiming Qiu, Junyang Bai, Ping Zhang, Dadong Miao, Songlin Wang, Guoyu Tang, Lin Liu, Mingming Li|JD.com|In large e-commerce platforms, search systems are typically composed of a series of modules, including recall, pre-ranking, and ranking phases. The pre-ranking phase, serving as a lightweight module, is crucial for filtering out the bulk of products in advance for the downstream ranking module. Industrial efforts on optimizing the pre-ranking model have predominantly focused on enhancing ranking consistency, model structure, and generalization towards long-tail items. Beyond these optimizations, meeting the system performance requirements presents a significant challenge. Contrasting with existing industry works, we propose a novel method: a Generalizable and RAnk-ConsistEnt Pre-Ranking Model (GRACE), which achieves: 1) Ranking consistency by introducing multiple binary classification tasks that predict whether a product is within the top-k results as estimated by the ranking model, which facilitates the addition of learning objectives on common point-wise ranking models; 2) Generalizability through contrastive learning of representation for all products by pre-training on a subset of ranking product embeddings; 3) Ease of implementation in feature construction and online deployment. Our extensive experiments demonstrate significant improvements in both offline metrics and online A/B test: a 0.75 increase in CVR.|在大型电子商务平台中，搜索系统通常由一系列模块组成，包括召回、预排序和排序阶段。预排序阶段作为一个轻量级模块，对于提前过滤掉下游排序模块的大部分产品至关重要。优化预排序模型的工业努力主要集中在增强排序一致性、模型结构和对长尾项目的推广。除了这些优化之外，满足系统性能需求也是一个重大的挑战。与现有的行业工作相比，我们提出了一种新的方法: 一个一般化和排名一致的预排名模型(GRACE) ，它实现了: 1)排名一致性通过引入多个二进制分类任务，预测一个产品是否在由排名模型估计的前 k 结果之内，这有助于增加学习目标的共同点明智的排名模型; 2)通过对比学习的表示对所有产品的一个排名产品嵌入子集的预训练的一般化; 3)易于实施的功能构建和在线部署。我们的大量实验表明，在离线指标和在线 A/B 测试方面都有显著改善: CVR 增加了0.75。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+E-commerce+Search:+Toward+a+Generalizable+and+Rank-Consistent+Pre-Ranking+Model)|0|
|[A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search](https://doi.org/10.1145/3626772.3661359)|Huimu Wang, Mingming Li, Dadong Miao, Songlin Wang, Guoyu Tang, Lin Liu, Sulong Xu, Jinghe Hu|JD or JD.com; JD.com|Re-ranking is a process of rearranging ranking list to more effectively meet user demands by accounting for the interrelationships between items. Existing methods predominantly enhance the precision of search results, often at the expense of diversity, leading to outcomes that may not fulfill the varied needs of users. Conversely, methods designed to promote diversity might compromise the precision of the results, failing to satisfy the users' requirements for accuracy. To alleviate the above problems, this paper proposes a Preference-oriented Diversity Model Based on Mutual-information (PODM-MI), which consider both accuracy and diversity in the re-ranking process. Specifically, PODM-MI adopts Multidimensional Gaussian distributions based on variational inference to capture users' diversity preferences with uncertainty. Then we maximize the mutual information between the diversity preferences of the users and the candidate items using the maximum variational inference lower bound to enhance their correlations. Subsequently, we derive a utility matrix based on the correlations, enabling the adaptive ranking of items in line with user preferences and establishing a balance between the aforementioned objectives. Experimental results on real-world online e-commerce systems demonstrate the significant improvements of PODM-MI, and we have successfully deployed PODM-MI on an e-commerce search platform.|重新排序是通过考虑项目之间的相互关系来重新安排排序列表以更有效地满足用户需求的过程。现有的方法主要是提高搜索结果的精确度，往往以牺牲多样性为代价，导致结果可能无法满足用户的不同需求。相反，旨在促进多样性的方法可能会损害结果的精确性，不能满足用户对精确性的要求。针对上述问题，本文提出了一种基于互信息的偏好导向多样性模型(PODM-MI) ，该模型在重排序过程中同时考虑了准确性和多样性。具体来说，PODM-MI 采用基于变分推理的多维高斯分布来捕获具有不确定性的用户多样性偏好。然后利用最大变分推理下界，最大化用户多样性偏好与候选项之间的相互信息，以增强它们之间的相关性。随后，我们推导出一个基于相关性的效用矩阵，使项目的自适应排序符合用户偏好，并建立上述目标之间的平衡。在实际的在线电子商务系统上的实验结果表明，PODM-MI 算法得到了显著的改进，并成功地在电子商务搜索平台上部署了 PODM-MI 算法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Preference-oriented+Diversity+Model+Based+on+Mutual-information+in+Re-ranking+for+E-commerce+Search)|0|
|[Query Performance Prediction for Conversational Search and Beyond](https://doi.org/10.1145/3626772.3657658)|Chuan Meng|University of Amsterdam|Query performance prediction (QPP) is a key task in information retrieval (IR) [1]. The QPP task is to estimate the retrieval quality of a search system for a query without human relevance judgments. In summary, I aim to solve 4 limitations identified in previous QPP studies: I have published 3 papers that address 3 of these limitations, while the remaining one is the focus of my future work. While extensively explored for traditional ad-hoc search, QPP for conversational search (CS) [4] has been little studied. I have identified limitation 1 in previous QPP studies: There is a lack of a comprehensive investigation into how well existing QPP methods designed for ad-hoc search perform in the context of CS. To fill this research gap, I have conducted a comprehensive reproducibility study [5], where I examined various QPP methods that were designed for ad-hoc search in the CS setting. I have made the code and data publicly available on https://github.com/ChuanMeng/QPP4CS. Moreover, I have identified limitation 2 in previous studies on QPP for CS: There is a lack of research in investigating and leveraging the CS-specific features that do not exist in ad-hoc search to improve QPP quality for CS. I have authored a paper to fill this research gap [3]. Specifically, my empirical analysis indicates a correlation between query rewriting quality in CS and the actual retrieval quality. Based on this finding, I have proposed a <u>p</u>er<u>pl</u>exity-based pre-retrieval QPP framework (PPL-QPP) for CS, which integrates query rewriting quality into existing QPP methods. Experimental results show that PPL-QPP improves QPP quality. Beyond the scope of QPP for CS, I have identified drawbacks in general QPP methods. Existing QPP methods typically return a single scalar value that indicates the retrieval quality, which results in two issues: (i) relying on a single value to represent different IR metrics leads to a "one size fits all" issue, and (ii) a single value constraints the interpretability of QPP. Thus, I have identified limitation 3: there is a shortage of QPP methods that are capable of effectively predicting various IR evaluation metrics while maintaining interpretability. To address the limitation, I have proposed a QPP framework using automatically <u>gen</u>erated <u>re</u>evance judgments (QPP-GenRE); it decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query [6]. QPP-GenRE enables the prediction of any IR metric using generated relevance judgments as pseudo-labels, and enables the interpretation of predicted IR metrics based on generated judgments. I have fine-tuned an open-source large language model (LLM) for judging relevance. Experimental results show that QPP-GenRE achieves state-of-the-art QPP quality; my fine-tuned LLM demonstrates a high relevance judgment agreement with human assessors. I have made the code and data publicly available on https://github.com/ChuanMeng/QPP-GenRE. As part of my future work, I plan to solve limitation 4: No study has explored the application of QPP in retrieval-augmented generation (RAG) to predict when not to rely on low-quality retrieved items that have the potential to hurt RAG's text generation.|查询性能预测(QPP)是信息检索(IR)[1]中的一项关键任务。QPP 任务是在没有人类相关性判断的情况下，对查询搜索系统的检索质量进行评估。总之，我的目标是解决在以前的 QPP 研究中发现的4个局限性: 我已经发表了3篇论文，解决了其中的3个局限性，而其余的一个是我未来工作的重点。虽然对传统的自组织搜索进行了广泛的研究，但是对会话搜索的 QPP 研究却很少。我已经在以前的 QPP 研究中确定了局限性1: 缺乏一个全面的调查，以了解现有的 QPP 方法设计的特别搜索在 CS 的情况下表现如何。为了填补这个研究空白，我进行了一个全面的重复性研究[5] ，其中我检查了各种 QPP 方法，这些方法是为在 CS 设置中的特别搜索而设计的。我已经把代码和数据公布在 https://github.com/chuanmeng/qpp4cs 上了。此外，我已经在以前的 CS QPP 研究中确定了局限性2: 缺乏研究调查和利用 CS 特定的特征，这些特征在特别搜索中不存在，以提高 CS 的 QPP 质量。我已经写了一篇论文来填补这个研究空白[3]。具体来说，本文的实证分析表明了 CS 中查询重写质量与实际检索质量之间的相关性。基于这一发现，我提出了一个基于实例的检索前 QPP 框架(PPL-QPP) ，该框架将查询重写质量与现有的 QPP 方法相结合。实验结果表明，PPL-QPP 提高了 QPP 的质量。除了 CS 的 QPP 范围，我已经确定了一般 QPP 方法的缺点。现有的 QPP 方法通常返回指示检索质量的单个标量值，这导致两个问题: (i)依赖于单个值来表示不同的 IR 指标导致“一种尺寸适合所有”问题，以及(ii)单个值限制了 QPP 的可解释性。因此，我已经确定了局限性3: 缺乏能够有效预测各种 IR 评估指标同时保持可解释性的 QPP 方法。为了解决这个局限性，我提出了一个 QPP 框架，它使用了自动的 < u > gen </u > ated < u > re </u > 事件判断(QPP-GenRE) ; 它将 QPP 分解为独立的子任务，判断排序列表中的每个项目与给定查询的相关性[6]。QPP-GenRE 能够使用生成的相关性判断作为伪标签来预测任何 IR 度量，并且能够基于生成的判断来解释预测的 IR 度量。我已经微调了一个用于判断相关性的开源大型语言模型(LLM)。实验结果表明，QPP-GenRE 实现了最先进的 QPP 质量，我的微调 LLM 与人类评估者的相关性判断一致性很高。我已经把代码和数据公布在 https://github.com/chuanmeng/qpp-genre 上了。作为我未来工作的一部分，我计划解决局限性4: 还没有研究探索 QPP 在检索增强生成(RAG)中的应用，以预测何时不依赖于有可能损害 RAG 文本生成的低质量检索项。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Query+Performance+Prediction+for+Conversational+Search+and+Beyond)|0|
|[Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset](https://doi.org/10.1145/3626772.3657892)|Philipp Hager, Romain Deffayet, JeanMichel Renders, Onno Zoeter, Maarten de Rijke|Naver Labs Europe; University of Amsterdam; Booking.com|Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in simulation, ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu's search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments reveal that ULTR robustly improves click prediction. However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this benchmark.|无偏学习排名(ULTR)是一个从用户点击中学习的成熟的框架，用户点击往往受到排名收集数据的影响。ULTR 技术虽然在理论上得到了验证，并在仿真中得到了广泛的测试，但缺乏经验验证，特别是在现代搜索引擎上。从百度搜索引擎收集的2023年 WSDM 杯的数据集提供了一个难得的机会来评估突出的 ULTR 技术在现实世界中的表现。尽管在2023年 WSDM 杯和随后的 NTCIR ULTRE-2任务期间提交了多份申请，但目前尚不清楚观察到的改善是否源于应用 ULTR 或其他学习技术。我们重新审视并扩展现有的实验。我们发现，无偏见的学习排序技术并不能带来明显的性能改善，尤其是与排序丢失和查询文档特性的选择所带来的明显差异相比。我们的实验表明，ULTR 强有力地改善点击预测。然而，在点击预测方面取得的这些进展并不能转化为专家相关性注释排名表现的提高，这意味着结论在很大程度上取决于如何在这一基准中衡量成功。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Learning+to+Rank+Meets+Reality:+Lessons+from+Baidu's+Large-Scale+Search+Dataset)|0|
|[CMCLRec: Cross-modal Contrastive Learning for User Cold-start Sequential Recommendation](https://doi.org/10.1145/3626772.3657839)|Xiaolong Xu, Hongsheng Dong, Lianyong Qi, Xuyun Zhang, Haolong Xiang, Xiaoyu Xia, Yanwei Xu, Wanchun Dou|Nanjing University; RMIT University; China University of Petroleum; Macquarle Unnversity; College of Intelligence and Computing, Tianjin University; Macquarie University; Nanjing University of Information Science and Technology|Sequential recommendation models generate embeddings for items through the analysis of historical user-item interactions and utilize the acquired embeddings to predict user preferences. Despite being effective in revealing personalized preferences for users, these models heavily rely on user-item interactions. However, due to the lack of interaction information, new users face challenges when utilizing sequential recommendation models for predictions, which is recognized as the cold-start problem. Recent studies, while addressing this problem within specific structures, often neglect the compatibility with existing sequential recommendation models, making seamless integration into existing models unfeasible.To address this challenge, we propose CMCLRec, a Cross-Modal Contrastive Learning framework for user cold-start RECommendation. This approach aims to solve the user cold-start problem by customizing inputs for cold-start users that align with the requirements of sequential recommendation models in a cross-modal manner. Specifically, CMCLRec adopts cross-modal contrastive learning to construct a mapping from user features to user-item interactions based on warm user data. It then generates a simulated behavior sequence for each cold-start user in turn for recommendation purposes. In this way, CMCLRec is theoretically compatible with any extant sequential recommendation model. Comprehensive experiments conducted on real-world datasets substantiate that, compared with state-of-the-art baseline models, CMCLRec markedly enhances the performance of conventional sequential recommendation models, particularly for cold-start users.|序贯推荐模型通过分析历史上的用户-项目交互，生成项目的嵌入，并利用获得的嵌入来预测用户偏好。尽管这些模型能够有效地向用户展示个性化偏好，但它们严重依赖于用户项目交互。然而，由于缺乏交互信息，新用户在使用顺序推荐模型进行预测时面临着挑战，这被认为是冷启动问题。最近的研究虽然在特定的结构内解决了这个问题，但往往忽视了与现有顺序推荐模型的兼容性，使得与现有模型的无缝集成变得不可行。为了应对这一挑战，我们提出了 CMCLRec，一个用于用户冷启动推荐的跨模态对比学习框架。这种方法旨在解决用户冷启动问题，为冷启动用户定制输入，以跨模式的方式符合顺序推荐模型的要求。具体来说，CMCLRec 采用跨模态对比学习方法，构建了基于暖用户数据的用户特征到用户项交互的映射关系。然后，它为每个冷启动用户依次生成一个模拟的行为序列，用于推荐目的。这样，CMCLRec 在理论上与任何现存的顺序推荐模型兼容。在真实世界数据集上进行的综合实验证实，与最先进的基线模型相比，CMCLRec 显著提高了传统顺序推荐模型的性能，特别是对于冷启动用户。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CMCLRec:+Cross-modal+Contrastive+Learning+for+User+Cold-start+Sequential+Recommendation)|0|
|[Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention](https://doi.org/10.1145/3626772.3657829)|Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai, Xiangyu Zhao, Kesen Zhao, Lantao Hu, Peng Jiang, Kun Gai|Unaffiliated; Kuaishou Technology; City University of Hong Kong School of Data Science; City University of Hong Kong; Kuaishou Technology Strategy Algorithm Department|In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication|在推荐系统(RS)应用领域，强化学习(rL)最近已经成为一种强大的工具，这主要是由于它在优化长期回报方面的熟练程度。尽管如此，由于自学、非政策培训和函数逼近之间错综复杂的相互作用，它在学习过程中存在不稳定性。此外，在多奖励推荐场景中，设计一个适当的奖励设置来协调各种任务的内部动态是相当复杂的。为了应对这些挑战，我们引入了 DT4IER，这是一种基于决策转换器的高级推荐模型，不仅旨在提高推荐的有效性，而且还旨在实现直接用户参与和长期保留之间的和谐平衡。DT4IER 采用了一种创新的多奖励设计，能够巧妙地平衡短期和长期奖励与用户特定属性之间的关系，这有助于增强奖励序列的上下文丰富性，确保推荐过程更加知情和个性化。为了增强其预测能力，DT4IER 采用了高维编码器，巧妙地设计识别和利用不同任务之间错综复杂的相互关系。此外，我们在嵌入预测的动作中整合了一种对比学习方法，这种策略显著地提高了模型的整体性能。在三个实际数据集上的实验证明了 DT4IER 对最先进的顺序推荐系统(SRS)和多任务学习(MTL)模型在特定任务的预测准确性和有效性方面的有效性。可以联机访问源代码，以便于复制|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Recommendation+for+Optimizing+Both+Immediate+Feedback+and+Long-term+Retention)|0|
|[Invisible Relevance Bias: Text-Image Retrieval Models Prefer AI-Generated Images](https://doi.org/10.1145/3626772.3657750)|Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, Xueqi Cheng|Institute of Computing Technology, Chinese Academy of Sciences; Gaoling School of Artificial Intelligence, Renmin University of China|With the advancement of generation models, AI-generated content (AIGC) is becoming more realistic, flooding the Internet. A recent study suggests that this phenomenon causes source bias in text retrieval for web search. Specifically, neural retrieval models tend to rank generated texts higher than human-written texts. In this paper, we extend the study of this bias to cross-modal retrieval. Firstly, we successfully construct a suitable benchmark to explore the existence of the bias. Subsequent extensive experiments on this benchmark reveal that AI-generated images introduce an invisible relevance bias to text-image retrieval models. Specifically, our experiments show that text-image retrieval models tend to rank the AI-generated images higher than the real images, even though the AI-generated images do not exhibit more visually relevant features to the query than real images. This invisible relevance bias is prevalent across retrieval models with varying training data and architectures. Furthermore, our subsequent exploration reveals that the inclusion of AI-generated images in the training data of the retrieval models exacerbates the invisible relevance bias. The above phenomenon triggers a vicious cycle, which makes the invisible relevance bias become more and more serious. To elucidate the potential causes of invisible relevance and address the aforementioned issues, we introduce an effective training method aimed at alleviating the invisible relevance bias. Subsequently, we apply our proposed debiasing method to retroactively identify the causes of invisible relevance, revealing that the AI-generated images induce the image encoder to embed additional information into their representation. This information exhibits a certain consistency across generated images with different semantics and can make the retriever estimate a higher relevance score.|随着生成模型的进步，人工智能生成的内容(AIGC)正变得越来越现实，充斥着互联网。最近的一项研究表明，这种现象造成源偏见的文本检索的网络搜索。具体来说，神经检索模型对生成文本的排名往往高于人写文本。在本文中，我们将这种偏差的研究扩展到跨模态检索。首先，我们成功地构建了一个合适的基准来研究这种偏差的存在。随后在这个基准上进行的大量实验表明，人工智能生成的图像给文本图像检索模型带来了不可见的相关性偏差。具体来说，我们的实验表明，文本图像检索模型对人工智能生成的图像的排序往往高于真实图像，即使人工智能生成的图像并没有表现出更多的视觉相关特征的查询比真实图像。这种看不见的相关性偏差在具有不同训练数据和结构的检索模型中普遍存在。此外，我们随后的研究表明，在检索模型的训练数据中包含人工智能生成的图像加剧了不可见的相关性偏差。上述现象引发了一个恶性循环，使得无形的关联偏差越来越严重。为了阐明隐性相关产生的潜在原因并解决上述问题，我们引入了一种有效的训练方法来缓解隐性相关偏差。随后，我们应用我们提出的去偏方法来追溯识别不可见相关性的原因，揭示了人工智能生成的图像诱导图像编码器嵌入额外的信息到他们的表示。这些信息在生成的具有不同语义的图像之间表现出一定的一致性，并且可以使检索器估计出更高的相关性得分。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Invisible+Relevance+Bias:+Text-Image+Retrieval+Models+Prefer+AI-Generated+Images)|0|
|[Fair Sequential Recommendation without User Demographics](https://doi.org/10.1145/3626772.3657703)|Huimin Zeng, Zhankui He, Zhenrui Yue, Julian J. McAuley, Dong Wang|University of Illinois at Urbana-Champaign; University of California, San Diego; University of Illinois Urbana-Champaign|Much existing literature on fair recommendation (i.e., group fairness) leverages users' demographic attributes (e.g., gender) to develop fair recommendation methods. However, in real-world scenarios, due to privacy concerns and convenience considerations, users may not be willing to share their demographic information with the system, which limits the application of many existing methods. Moreover, sequential recommendation (SR) models achieve state-of-the-art performance compared to traditional collaborative filtering (CF) recommenders, and can represent users solely using user-item interactions (user-free). This leaves a wrong impression that SR models are free from group unfairness by design. In this work, we explore a critical question: how can we build a fair sequential recommendation system without even knowing user demographics? To address this problem, we propose Agnostic FairSeqRec (A-FSR): a model-agnostic and demographic-agnostic debiasing framework for sequential recommendation without requiring users' demographic attributes. Firstly, A-FSR reduces the correlation between the potential stereotypical patterns in the input sequences and final recommendations via Dirichlet neighbor smoothing. Secondly, A-FSR estimates an under-represented group of sequences via a gradient-based heuristic, and implicitly moves training focus towards the under-represented group by minimizing a distributionally robust optimization (DRO) based objective. Results on real-world datasets show that A-FSR achieves significant improvements on group fairness in sequential recommendation, while outperforming other state-of-the-art baselines.|关于公平推荐(即群体公平)的许多现有文献利用用户的人口统计特征(如性别)来发展公平推荐方法。然而，在现实世界的情况下，由于隐私问题和方便的考虑，用户可能不愿意与系统共享他们的人口统计信息，这限制了许多现有方法的应用。此外，序贯推荐(SR)模型与传统的协同过滤推荐(CF)模型相比，可以实现最先进的性能，并且可以完全使用用户项交互(用户自由)来代表用户。这就给人留下了一个错误的印象，认为 SR 模型在设计上不存在群体不公平。在这项工作中，我们探讨了一个关键问题: 我们如何建立一个公平的顺序推荐系统，甚至不知道用户的人口统计？为了解决这个问题，我们提出了不可知的 FairSeqRec (A-FSR) : 一个不需要用户人口统计属性的模型不可知和人口统计不可知的连续推荐消偏框架。首先，A-FSR 通过 Dirichlet 邻域平滑降低了输入序列中潜在的常规模式与最终推荐值之间的相关性。其次，A-FSR 通过基于梯度的启发式算法估计一组未被充分表示的序列，并通过最小化基于分布鲁棒优化(DRO)的目标隐式地将训练焦点移向未被充分表示的序列。实际数据集的结果表明，A-FSR 在顺序推荐方面取得了显著的改善，同时优于其他最先进的基线。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Sequential+Recommendation+without+User+Demographics)|0|
|[Negative Sampling Techniques for Dense Passage Retrieval in a Multilingual Setting](https://doi.org/10.1145/3626772.3657854)|Thilina Chaturanga Rajapakse, Andrew Yates, Maarten de Rijke|University of Amsterdam|The bi-encoder transformer architecture has become popular in open-domain retrieval, surpassing traditional sparse retrieval methods. Using hard negatives during training can improve the effectiveness of dense retrievers, and various techniques have been proposed to generate these hard negatives. We investigate the effectiveness of multiple negative sampling methods based on lexical methods (BM25), clustering, and periodically updated dense indices. We examine techniques that were introduced for finding hard negatives in a monolingual setting and reproduce them in a multilingual setting. We discover a gap amongst these techniques that we fill by proposing a novel clustered training method. Specifically, we focus on monolingual retrieval using multilingual dense retrievers across a broad set of diverse languages. We find that negative sampling based on BM25 negatives is surprisingly effective in an in-distribution setting, but this finding does not generalize to out-of-distribution and zero-shot settings, where the newly proposed method achieves the best results. We conclude with recommendations on which negative sampling methods may be the most effective given different multilingual retrieval scenarios.|双编码器变压器结构已经成为开放域检索中的热点，超越了传统的稀疏检索方法。在训练过程中使用硬负片可以提高密集型检索器的效率，人们提出了各种技术来产生这些硬负片。我们研究了基于词汇方法(BM25)、聚类和周期性更新密集指数的多重负抽样方法的有效性。我们研究了在单语环境下寻找硬负面的技术，并在多语环境下重现这些技术。我们通过提出一种新的聚类训练方法来填补这些技术之间的空白。具体来说，我们的重点是使用多语言密集检索器跨多种语言的单语言检索。我们发现基于 BM25负值的负采样在分布内环境中有惊人的效果，但是这一发现并没有推广到分布外环境和零拍环境中，在这两种环境中，新提出的方法取得了最好的效果。最后，我们给出了在不同的多语言检索场景下，哪种负抽样方法可能是最有效的建议。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Negative+Sampling+Techniques+for+Dense+Passage+Retrieval+in+a+Multilingual+Setting)|0|
|[M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework](https://doi.org/10.1145/3626772.3657686)|Zijian Zhang, Shuchang Liu, Jiaao Yu, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Ziru Liu, Qidong Liu, Hongwei Zhao, Lantao Hu, Peng Jiang, Kun Gai||Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling. Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods. To this end, we introduce M3oE, an adaptive multi-domain multi-task mixture-of-experts recommendation framework. M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives. We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner. Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks. The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization. To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively. Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance. The implementation code is available to ensure reproducibility.|多领域推荐和多任务推荐在利用来自不同领域和目标的公共信息进行全面的用户建模方面展示了它们的有效性。尽管如此，实际的推荐通常同时面对多个领域和任务，而这些领域和任务不能被当前的方法很好地处理。为此，我们介绍了一个自适应的多领域多任务混合专家推荐框架 M3oE。M3oE 集成了多领域信息，映射了跨领域和任务的知识，并优化了多个目标。我们利用三个专家混合模块分别学习通用、领域方面和任务方面的用户偏好，以解决多个领域和任务之间的复杂依赖关系。此外，我们设计了一个两级融合机制，用于精确控制不同领域和任务的特征提取和融合。通过应用 AutoML 技术，进一步提高了框架的适应性，实现了动态结构优化。据作者所知，我们的 M3oE 首次尝试自适应地解决多领域多任务推荐问题。针对不同基线的两个基准数据集的大量实验证明了 M3oE 的优越性能。实现代码可用于确保可重复性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M3oE:+Multi-Domain+Multi-Task+Mixture-of+Experts+Recommendation+Framework)|0|
|[NFARec: A Negative Feedback-Aware Recommender Model](https://doi.org/10.1145/3626772.3657809)|Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Dongjin Yu|Graduate Faculty of Interdisciplinary Research, University of Yamanashi; Faculty of Engineering, Integrated Graduate School of Medicine, Engineering, and Agricultural Sciences; School of Computer Science and Technology, Hangzhou Dianzi University|Graph neural network (GNN)-based models have been extensively studied for recommendations, as they can extract high-order collaborative signals accurately which is required for high-quality recommender systems. However, they neglect the valuable information gained through negative feedback in two aspects: (1) different users might hold opposite feedback on the same item, which hampers optimal information propagation in GNNs, and (2) even when an item vastly deviates from users' preferences, they might still choose it and provide a negative rating. In this paper, we propose a negative feedback-aware recommender model (NFARec) that maximizes the leverage of negative feedback. To transfer information to multi-hop neighbors along an optimal path effectively, NFARec adopts a feedback-aware correlation that guides hypergraph convolutions (HGCs) to learn users' structural representations. Moreover, NFARec incorporates an auxiliary task - predicting the feedback sentiment polarity (i.e., positive or negative) of the next interaction - based on the Transformer Hawkes Process. The task is beneficial for understanding users by learning the sentiment expressed in their previous sequential feedback patterns and predicting future interactions. Extensive experiments demonstrate that NFARec outperforms competitive baselines. Our source code and data are released at https://github.com/WangXFng/NFARec.|基于图形神经网络(GNN)的推荐系统模型能够准确地提取高阶协同信号，是高质量推荐系统所必需的。然而，他们忽视了通过负面反馈获得的有价值的信息在两个方面: (1)不同的用户可能对同一个项目持有相反的反馈，这阻碍了最佳信息在 GNN 中的传播，和(2)即使一个项目大大偏离用户的喜好，他们仍然可能选择它，并提供一个负面评价。在本文中，我们提出了一个负反馈感知的推荐模型(NFARec) ，最大限度地利用负反馈。NFARec 采用反馈感知关联算法，引导超图卷积(HGC)学习用户的结构表示，有效地将信息沿着最优路径传递给多跳邻居。此外，NFARec 还包含了一个辅助任务——预测下一次交互的反馈情绪极性(即正极或负极)——基于变压器霍克斯过程。这项任务有利于了解用户的情绪表达在他们以前的顺序反馈模式和预测未来的交互。大量的实验表明，NFARec 的表现优于竞争基线。我们的源代码和数据在 https://github.com/wangxfng/nfarec 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NFARec:+A+Negative+Feedback-Aware+Recommender+Model)|0|
|[Modeling User Fatigue for Sequential Recommendation](https://doi.org/10.1145/3626772.3657802)|Nian Li, Xin Ban, Cheng Ling, Chen Gao, Lantao Hu, Peng Jiang, Kun Gai, Yong Li, Qingmin Liao|Tsinghua University; Shenzhen International Graduate School, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Independent; Kuaishou Inc.|Recommender systems filter out information that meets user interests. However, users may be tired of the recommendations that are too similar to the content they have been exposed to in a short historical period, which is the so-called user fatigue. Despite the significance for a better user experience, user fatigue is seldom explored by existing recommenders. In fact, there are three main challenges to be addressed for modeling user fatigue, including what features support it, how it influences user interests, and how its explicit signals are obtained. In this paper, we propose to model user Fatigue in interest learning for sequential Recommendations (FRec). To address the first challenge, based on a multi-interest framework, we connect the target item with historical items and construct an interest-aware similarity matrix as features to support fatigue modeling. Regarding the second challenge, built upon feature cross, we propose a fatigue-enhanced multi-interest fusion to capture long-term interest. In addition, we develop a fatigue-gated recurrent unit for short-term interest learning, with temporal fatigue representations as important inputs for constructing update and reset gates. For the last challenge, we propose a novel sequence augmentation to obtain explicit fatigue signals for contrastive learning. We conduct extensive experiments on real-world datasets, including two public datasets and one large-scale industrial dataset. Experimental results show that FRec can improve AUC and GAUC up to 0.026 and 0.019 compared with state-of-the-art models, respectively. Moreover, large-scale online experiments demonstrate the effectiveness of FRec for fatigue reduction. Our codes are released at https://github.com/tsinghua-fib-lab/SIGIR24-FRec.|推荐系统过滤出符合用户兴趣的信息。然而，用户可能会厌倦那些与他们在很短的历史时期内接触到的内容过于相似的推荐，这就是所谓的用户疲劳。尽管这对于更好的用户体验意义重大，但是现有的推荐者很少探讨用户疲劳问题。实际上，建立用户疲劳模型需要解决三个主要问题，包括哪些特性支持用户疲劳，它如何影响用户兴趣，以及如何获得用户疲劳的显性信号。在本文中，我们提出了模型用户疲劳的兴趣学习顺序推荐(FRec)。为了解决第一个问题，我们基于一个多兴趣框架，将目标项目与历史项目连接起来，构造一个感兴趣的相似矩阵作为特征来支持疲劳建模。针对第二个挑战，建立在特征交叉的基础上，我们提出了一种疲劳增强的多兴趣融合来捕获长期兴趣。此外，我们开发了一个用于短期兴趣学习的疲劳门控循环单元，以时间疲劳表示作为构造更新门和复位门的重要输入。针对最后一个挑战，我们提出了一种新的序列增强方法，用于获得用于对比学习的显式疲劳信号。我们对真实世界的数据集进行了广泛的实验，包括两个公共数据集和一个大规模的工业数据集。实验结果表明，与现有模型相比，FRec 可以提高 AUC 和 GAUC，分别达到0.026和0.019。此外，大规模的在线实验证明了 FRec 对疲劳减振的有效性。我们的密码在 https://github.com/tsinghua-fib-lab/sigir24-frec 公布。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+User+Fatigue+for+Sequential+Recommendation)|0|
|[DDPO: Direct Dual Propensity Optimization for Post-Click Conversion Rate Estimation](https://doi.org/10.1145/3626772.3657817)|Hongzu Su, Lichao Meng, Lei Zhu, Ke Lu, Jingjing Li|University of Electronic Science and Technology of China; Tongji University|In online advertising, the sample selection bias problem is a major cause of inaccurate conversion rate estimates. Current mainstream solutions only perform causality-based optimization in the click space since the conversion labels in the non-click space are absent. However, optimization for unclicked samples is equally essential because the non-click space contains more samples and user characteristics than the click space. To exploit the unclicked samples, we propose a Direct Dual Propensity Optimization (DDPO) framework to optimize the model directly in impression space with both clicked and unclicked samples. In this framework, we specifically design a click propensity network and a conversion propensity network. The click propensity network is dedicated to ensuring that optimization in the click space is unbiased. The conversion propensity network is designed to generate pseudo-conversion labels for unclicked samples, thus overcoming the challenge of absent labels in non-click space. With these two propensity networks, we are able to perform causality-based optimization in both click space and non-click space. In addition, to strengthen the causal relationship, we design two causal transfer modules for the conversion rate prediction model with the attention mechanism. The proposed framework is evaluated on five real-world public datasets and one private Tencent advertising dataset. Experimental results verify that our method is able to improve the prediction performance significantly. For instance, our method outperforms the previous state-of-the-art method by 7.0% in terms of the Area Under the Curve on the Ali-CCP dataset.|在网络广告中，样本选择偏差问题是导致转化率估计不准确的主要原因。当前的主流解决方案只在点击空间中执行基于因果关系的优化，因为非点击空间中没有转换标签。然而，对未点击样本的优化同样重要，因为非点击空间比点击空间包含更多的样本和用户特征。为了利用未点击样本，我们提出了一个直接双倾向优化(DDPO)框架，直接在印象空间中对点击样本和未点击样本进行优化。在这个框架中，我们具体设计了一个点击倾向网络和一个转换倾向网络。点击倾向网络致力于确保点击空间的优化是无偏的。转换倾向网络的设计目的是为未点击样本生成伪转换标签，从而克服非点击空间中标签缺失的困难。有了这两个倾向网络，我们就能够在点击空间和非点击空间进行基于因果关系的优化。此外，为了加强因果关系，我们设计了两个具有注意机制的因果传递模块用于转化率预测模型。建议的框架是根据五个真实世界的公共数据集和一个私人腾讯广告数据集进行评估的。实验结果表明，该方法能够显著提高预测性能。例如，在 Ali-CCP 数据集的曲线下面积方面，我们的方法比以前最先进的方法高出7.0% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DDPO:+Direct+Dual+Propensity+Optimization+for+Post-Click+Conversion+Rate+Estimation)|0|
|[A Generic Behavior-Aware Data Augmentation Framework for Sequential Recommendation](https://doi.org/10.1145/3626772.3657682)|Jing Xiao, Weike Pan, Zhong Ming|Shenzhen University|Multi-behavior sequential recommendation (MBSR), which models multi-behavior sequentiality and heterogeneity to better learn users' multifaceted intentions has achieved remarkable success. Though effective, the performance of these approaches may be limited due to the sparsity inherent in a real-world data. Existing data augmentation methods in recommender systems focus solely on a single type of behavior, overlooking the variations in expressing user preferences via different types of behaviors. During the augmentation of samples, it is easy to introduce excessive disturbance or noise, which may mislead the next-item recommendation. To address this limitation, we propose a novel generic framework called multi-behavior data augmentation for sequential recommendation (MBASR). Specifically, we design three behavior-aware data augmentation operations to construct rich training samples. Each augmentation operation takes into account the correlations between behaviors and aligns with the users' behavior patterns. In addition, we introduce a position-based sampling strategy that can effectively reduce the perturbation brought by the augmentation operations to the original data. Note that our model is data-oriented and can thus be embedded in different downstream MBSR models, so the overall framework is generic. Extensive experiments on three real-world datasets demonstrate the effectiveness of our MBASR and its applicability to a wide variety of mainstream MBSR models. Our source code is available at https://github.com/XiaoJing-C/MBASR.|多行为顺序推荐(MBRR)模型对多行为顺序性和异构性进行建模，以更好地了解用户的多方面意图，已取得了显著的成功。尽管这些方法有效，但由于真实世界数据中固有的稀疏性，它们的性能可能会受到限制。推荐系统中现有的数据增强方法只关注单一类型的行为，忽略了通过不同类型的行为表达用户偏好的差异。在样本的增大过程中，容易引入过多的干扰或噪声，从而误导下一项的推荐。为了解决这个问题，我们提出了一种新的通用框架，称为序贯推荐的多行为数据增强(MBASR)。具体来说，我们设计了三个行为感知的数据增强操作来构造丰富的训练样本。每个增强操作都考虑到行为之间的相关性，并与用户的行为模式保持一致。此外，我们还引入了一种基于位置的采样策略，可以有效地减少增广操作对原始数据的干扰。注意，我们的模型是面向数据的，因此可以嵌入到不同的下游 MBSR 模型中，所以总体框架是通用的。在三个实际数据集上的大量实验证明了我们的 MBASR 的有效性及其对各种主流 MBSR 模型的适用性。我们的源代码可以在 https://github.com/xiaojing-c/mbasr 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Generic+Behavior-Aware+Data+Augmentation+Framework+for+Sequential+Recommendation)|0|
|[FineRec: Exploring Fine-grained Sequential Recommendation](https://doi.org/10.1145/3626772.3657761)|Xiaokun Zhang, Bo Xu, Youlin Wu, Yuan Zhong, Hongfei Lin, Fenglong Ma|Pennsylvania State University; Dalian University of Technology|Sequential recommendation is dedicated to offering items of interest for users based on their history behaviors. The attribute-opinion pairs, expressed by users in their reviews for items, provide the potentials to capture user preferences and item characteristics at a fine-grained level. To this end, we propose a novel framework FineRec that explores the attribute-opinion pairs of reviews to finely handle sequential recommendation. Specifically, we utilize a large language model to extract attribute-opinion pairs from reviews. For each attribute, a unique attribute-specific user-opinion-item graph is created, where corresponding opinions serve as the edges linking heterogeneous user and item nodes. Afterwards, we devise a diversity-aware convolution operation to aggregate information within the graphs, enabling attribute-specific user and item representation learning. Ultimately, we present an interaction-driven fusion mechanism to integrate attribute-specific user/item representations across all attributes for generating recommendations. Extensive experiments conducted on several real-world datasets demonstrate the superiority of our FineRec over existing state-ofthe-art methods. Further analysis also verifies the effectiveness of our fine-grained manner in handling the task.|序列推荐致力于根据用户的历史行为为他们提供感兴趣的项目。由用户在项目评论中表达的属性-意见对提供了在细粒度水平上捕获用户偏好和项目特征的潜力。为此，我们提出了一个新的框架 FineRec，探索评论的属性-意见对，以精细处理顺序推荐。具体来说，我们利用一个大型的语言模型来从评论中提取属性-意见对。对于每个属性，创建一个惟一的特定于属性的用户意见项图，其中相应的意见作为连接异构用户和项目节点的边。然后，我们设计一个多样性感知的卷积运算来聚集图中的信息，使特定属性的用户和项目表示学习。最后，我们提出了一种交互驱动的融合机制，用于跨所有属性集成特定于属性的用户/项表示，以生成建议。在几个真实世界数据集上进行的大量实验证明了我们的 FineRec 相对于现有最先进的方法的优越性。进一步的分析还验证了我们处理任务的细粒度方式的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FineRec:+Exploring+Fine-grained+Sequential+Recommendation)|0|
|[ReFer: Retrieval-Enhanced Vertical Federated Recommendation for Full Set User Benefit](https://doi.org/10.1145/3626772.3657763)|Wenjie Li, Zhongren Wang, Jinpeng Wang, Shutao Xia, Jile Zhu, Mingjian Chen, Jiangke Fan, Jia Cheng, Jun Lei|Meituan; Tsinghua University|As an emerging privacy-preserving approach to leveraging cross-platform user interactions, vertical federated learning (VFL) has been increasingly applied in recommender systems. However, vanilla VFL is only applicable to overlapped users, ignoring potential universal interest patterns hidden among non-overlapped users and suffers from limited user group benefits, which hinders its application in real-world recommenders. In this paper, we extend the traditional vertical federated recommendation problem (VFR) to a more realistic Fully-Vertical federated recommendation setting (Fully-VFR) which aims to utilize all available data and serve full user groups. To tackle challenges in implementing Fully-VFR, we propose a Retrieval-enhanced Vertical Federated recommender (ReFer), a groundbreaking initiative that explores retrieval-enhanced machine learning approaches in VFL. Specifically, we establish a general "retrieval-and-utilization" algorithm to enhance the quality of representations across all parties. We design a flexible federated retrieval augmentation (RA) mechanism for VFL: (i) Cross-RA to complement field missing and (ii) Local-RA to promote mutual understanding between user groups. We conduct extensive experiments on both public and industry datasets. Results on both sequential and non-sequential CTR prediction tasks demonstrate that our method achieves significant performance improvements over baselines and is beneficial for all user groups.|作为一种新兴的利用跨平台用户交互的隐私保护方法，垂直联邦学习(VFL)在推荐系统中得到了越来越多的应用。然而，普通的 VFL 只适用于重叠用户，忽略了隐藏在非重叠用户之间的潜在通用兴趣模式，并且受到用户组好处的限制，这阻碍了它在实际推荐中的应用。本文将传统的垂直联邦推荐问题(VFR)扩展到一个更加现实的全垂直联邦推荐设置(Full-VFR) ，其目的是利用所有可用的数据，为全用户组提供服务。为了解决在实施完全 VFR 的挑战，我们提出了一个检索增强垂直联邦推荐(参考) ，一个突破性的倡议，探索检索增强机器学习方法在 VFL。具体来说，我们建立了一个通用的“检索和利用”算法，以提高所有各方的表示质量。我们设计了一个灵活的 VFL 联邦检索增强(RA)机制: (i)交叉 RA 来补充字段缺失; (ii)本地 RA 来促进用户组之间的相互理解。我们在公共和行业数据集上进行广泛的实验。在顺序和非顺序 CTR 预测任务中的结果表明，我们的方法比基线性能有了显著的提高，并且对所有用户组都有利。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReFer:+Retrieval-Enhanced+Vertical+Federated+Recommendation+for+Full+Set+User+Benefit)|0|
|[Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation](https://doi.org/10.1145/3626772.3657710)|Chung Park, Taesan Kim, Hyungjun Yoon, Junui Hong, Yelim Yu, Mincheol Cho, Minsung Choi, Jaegul Choo|SK Telecom / KAIST; SK Telelcom; SK Telelcom / KAIST; Korea Advanced Institute of Science and Technology|Cross-Domain Sequential Recommendation (CDSR) improves recommendation performance by utilizing information from multiple domains, which contrasts with Single-Domain Sequential Recommendation (SDSR) that relies on a historical interaction within a specific domain. However, CDSR may underperform compared to the SDSR approach in certain domains due to negative transfer, which occurs when there is a lack of relation between domains or different levels of data sparsity. To address the issue of negative transfer, our proposed CDSR model estimates the degree of negative transfer of each domain and adaptively assigns it as a weight factor to the prediction loss, to control gradient flows through domains with significant negative transfer. To this end, our model compares the performance of a model trained on multiple domains (CDSR) with a model trained solely on the specific domain (SDSR) to evaluate the negative transfer of each domain using our asymmetric cooperative network. In addition, to facilitate the transfer of valuable cues between the SDSR and CDSR tasks, we developed an auxiliary loss that maximizes the mutual information between the representation pairs from both tasks on a per-domain basis. This cooperative learning between SDSR and CDSR tasks is similar to the collaborative dynamics between pacers and runners in a marathon. Our model outperformed numerous previous works in extensive experiments on two real-world industrial datasets across ten service domains. We also have deployed our model in the recommendation system of our personal assistant app service, resulting in 21.4% increase in click-through rate compared to existing models, which is valuable to real-world business1.|跨域序列推荐(CDSR)通过利用来自多个域的信息来提高推荐性能，这与依赖于特定域内的历史交互的单域序列推荐(SDSR)形成了鲜明的对比。然而，CDSR 方法在某些领域的表现可能不如 SDSR 方法，这是由于负迁移，这种负迁移发生在领域之间缺乏联系或不同层次的数据稀疏时。为了解决负迁移问题，我们提出的 CDSR 模型估计每个域的负迁移程度，并自适应地将其作为预测损失的权重因子，以控制梯度流通过具有显著负迁移的域。为此，我们的模型比较了在多域(CDSR)训练的模型和单独在特定域(SDSR)训练的模型的性能，以评估使用我们的非对称合作网络的每个域的负迁移。此外，为了促进 SDSR 和 CDSR 任务之间有价值线索的传递，我们开发了一个辅助损失模型，该模型在每个领域的基础上最大化两个任务表征对之间的相互信息。SDSR 和 CDSR 任务之间的协作学习类似于马拉松中步行者和跑步者之间的协作动力学。我们的模型在十个服务领域的两个实际工业数据集上进行了广泛的实验，其性能优于以前的许多工作。我们也在个人助理应用程序服务的推荐系统中使用了我们的模型，与现有模型相比，点进率增加了21.4% ，这对于现实世界的商业来说是很有价值的。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pacer+and+Runner:+Cooperative+Learning+Framework+between+Single-+and+Cross-Domain+Sequential+Recommendation)|0|
|[Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3657713)|Hanyu Li, Weizhi Ma, Peijie Sun, Jiayu Li, Cunxiang Yin, Yancheng He, Guoqiang Xu, Min Zhang, Shaoping Ma|Tencent; Tsinghua University|Cross-domain recommender (CDR) systems aim to enhance the performance of the target domain by utilizing data from other related domains. However, irrelevant information from the source domain may instead degrade target domain performance, which is known as the negative transfer problem. There have been some attempts to address this problem, mostly by designing adaptive representations for overlapped users. Whereas, representation adaptions solely rely on the expressive capacity of the CDR model, lacking explicit constraint to filter the irrelevant source-domain collaborative information for the target domain. In this paper, we propose a novel Collaborative information regularized User Transformation (CUT) framework to tackle the negative transfer problem by directly filtering users' collaborative information. In CUT, user similarity in the target domain is adopted as a constraint for user transformation learning to filter the user collaborative information from the source domain. CUT first learns user similarity relationships from the target domain. Then, source-target information transfer is guided by the user similarity, where we design a user transformation layer to learn target-domain user representations and a contrastive loss to supervise the user collaborative information transferred. The results show significant performance improvement of CUT compared with SOTA single and cross-domain methods. Further analysis of the target-domain results illustrates that CUT can effectively alleviate the negative transfer problem.|跨域推荐(CDR)系统旨在通过利用其他相关域的数据来提高目标域的性能。然而，来自源域的不相关信息反而会降低目标域的性能，这就是所谓的负迁移问题。已经有一些尝试来解决这个问题，主要是通过为重叠用户设计自适应表示。然而，表示适配仅仅依赖于 CDR 模型的表达能力，缺乏明确的约束来过滤不相关的源域协同信息。本文提出了一种新的协同信息规范化用户转换(CUT)框架，通过直接过滤用户的协同信息来解决负迁移问题。在 CUT 中，采用目标域中的用户相似度作为用户转换学习的约束条件，对源域中的用户协作信息进行过滤。CUT 首先从目标域学习用户相似性关系。然后，以用户相似性为指导，设计了用户转换层来学习目标域用户表示，并通过对比度损失来监督用户协同信息的传输。结果表明，与 SOTA 单域和跨域方法相比，CUT 的性能有了显著的提高。对目标域结果的进一步分析表明，CUT 可以有效地缓解负迁移问题。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Aiming+at+the+Target:+Filter+Collaborative+Information+for+Cross-Domain+Recommendation)|0|
|[On the Negative Perception of Cross-domain Recommendations and Explanations](https://doi.org/10.1145/3626772.3657735)|Denis Kotkov, Alan Medlar, Yang Liu, Dorota Glowacka|University of Helsinki|Recommender systems typically operate within a single domain, for example, recommending books based on users' reading habits. If such data is unavailable, it may be possible to make cross-domain recommendations and recommend books based on user preferences from another domain, such as movies. However, despite considerable research on cross-domain recommendations, no studies have investigated their impact on users' behavioural intentions or system perceptions compared to single-domain recommendations. Similarly, while single-domain explanations have been shown to improve users' perceptions of recommendations, there are no comparable studies for the cross-domain case. In this article, we present a between-subject study (N=237) of users' behavioural intentions and perceptions of book recommendations. The study was designed to disentangle the effects of whether recommendations were single- or cross-domain from whether explanations were present or not. Our results show that cross-domain recommendations have lower trust and interest than single-domain recommendations, regardless of their quality. While these negative effects can be ameliorated by cross-domain explanations, they are still perceived as inferior to single-domain recommendations without explanations. Last, we show that explanations decrease interest in the single-domain case, but increase perceived transparency and scrutability in both single- and cross-domain recommendations. Our findings offer valuable insights into the impact of recommendation provenance on user experience and could inform the future development of cross-domain recommender systems.|推荐系统通常在单一领域内运作，例如，根据用户的阅读习惯推荐书籍。如果这样的数据是不可用的，它可能会作出跨领域的建议，并推荐书籍的基础上用户喜好从另一个领域，如电影。然而，尽管对跨领域建议进行了大量的研究，但没有研究调查它们对用户行为意图或系统感知的影响，与单领域建议相比。同样，虽然单一领域的解释已被证明可以改善用户对推荐的看法，但是对于跨领域的案例没有可比较的研究。在这篇文章中，我们提出了一个主题间的研究(N = 237)用户的行为意图和感知的书籍推荐。这项研究的目的是将建议是单一还是跨领域的影响与解释是否存在区分开来。我们的研究结果表明，无论其质量如何，跨域建议比单域建议具有更低的信任度和兴趣。虽然这些负面影响可以通过跨领域的解释得到改善，但它们仍然被认为不如没有解释的单领域建议。最后，我们表明，解释降低兴趣的单一领域的情况下，但增加感知的透明度和审查在单一和跨领域的建议。我们的研究结果为推荐来源对用户体验的影响提供了有价值的见解，并且可以为跨域推荐系统的未来发展提供信息。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Negative+Perception+of+Cross-domain+Recommendations+and+Explanations)|0|
|[Multi-Domain Sequential Recommendation via Domain Space Learning](https://doi.org/10.1145/3626772.3657685)|Junyoung Hwang, Hyunjun Ju, SeongKu Kang, Sanghwan Jang, Hwanjo Yu|Pohang University of Science and Technology; University of Illinois Urbana-Champaign; 42dot|This paper explores Multi-Domain Sequential Recommendation (MDSR), an advancement of Multi-Domain Recommendation that incorporates sequential context. Recent MDSR approach exploits domain-specific sequences, decoupled from mixed-domain histories, to model domain-specific sequential preference, and use mixeddomain histories to model domain-shared sequential preference. However, the approach faces challenges in accurately obtaining domain-specific sequential preferences in the target domain, especially when users only occasionally engage with it. In such cases, the history of users in the target domain is limited or not recent, leading the sequential recommender system to capture inaccurate domain-specific sequential preferences. To address this limitation, this paper introduces Multi-Domain Sequential Recommendation via Domain Space Learning (MDSR-DSL). Our approach utilizes cross-domain items to supplement missing sequential context in domain-specific sequences. It involves creating a "domain space" to maintain and utilize the unique characteristics of each domain and a domain-to-domain adaptation mechanism to transform item representations across domain spaces. To validate the effectiveness of MDSR-DSL, this paper extensively compares it with state-of-the-art MD(S)R methods and provides detailed analyses.|多域顺序推荐(MDSR)是结合顺序上下文的多域推荐的一种进步。最近的 MDSR 方法利用领域特定的序列，从混合领域历史解耦，建模领域特定的顺序偏好，并使用混合领域历史建模领域共享的顺序偏好。然而，该方法在准确获取目标域中特定于领域的顺序首选项时面临挑战，特别是当用户只是偶尔使用它时。在这种情况下，目标域的用户历史是有限的或不是最近的，导致顺序推荐系统捕获不准确的领域特定的顺序首选项。针对这一局限性，本文引入了基于领域空间学习的多领域序贯推荐(MDSR-DSL)。我们的方法利用跨领域的项目来补充领域特定序列中缺少的顺序上下文。它包括创建一个“域空间”来维护和利用每个域的独特特征，以及一个域到域的适应机制来跨域空间转换项表示。为了验证 MDSR-DSL 的有效性，本文将其与最新的 MD (S) R 方法进行了广泛的比较，并给出了详细的分析。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Domain+Sequential+Recommendation+via+Domain+Space+Learning)|0|
|[Behavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommendation Systems](https://doi.org/10.1145/3626772.3657924)|Dayu Yang, Fumian Chen, Hui Fang|University of Delaware|Large Language Models (LLMs) have demonstrated great potential in Conversational Recommender Systems (CRS). However, the application of LLMs to CRS has exposed a notable discrepancy in behavior between LLM-based CRS and human recommenders: LLMs often appear inflexible and passive, frequently rushing to complete the recommendation task without sufficient inquiry.This behavior discrepancy can lead to decreased accuracy in recommendations and lower user satisfaction. Despite its importance, existing studies in CRS lack a study about how to measure such behavior discrepancy. To fill this gap, we propose Behavior Alignment, a new evaluation metric to measure how well the recommendation strategies made by a LLM-based CRS are consistent with human recommenders'. Our experiment results show that the new metric is better aligned with human preferences and can better differentiate how systems perform than existing evaluation metrics. As Behavior Alignment requires explicit and costly human annotations on the recommendation strategies, we also propose a classification-based method to implicitly measure the Behavior Alignment based on the responses. The evaluation results confirm the robustness of the method.|大语言模型(LLM)在会话推荐系统(CRS)中显示出巨大的潜力。然而，LLM 在 CRS 中的应用暴露了基于 LLM 的 CRS 和人类推荐者之间显着的行为差异: LLM 往往显得不灵活和被动，经常在没有充分询问的情况下匆忙完成推荐任务。这种行为差异可能导致推荐的准确性下降和用户满意度降低。尽管 CRS 具有重要意义，但是现有的研究缺乏如何测量这种行为差异的研究。为了填补这个空白，我们提出了行为校准，一个新的评估指标，以衡量如何以 LLM 为基础的 CRS 的推荐策略是一致的人类推荐者的。我们的实验结果表明，与现有的评估指标相比，新的指标更符合人类的偏好，能够更好地区分系统的执行情况。由于行为对齐需要对推荐策略进行明确而昂贵的人工注释，我们还提出了一种基于分类的方法来隐式地度量基于响应的行为对齐。评价结果证实了该方法的鲁棒性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior+Alignment:+A+New+Perspective+of+Evaluating+LLM-based+Conversational+Recommendation+Systems)|0|
|[Bi-Objective Negative Sampling for Sensitivity-Aware Search](https://doi.org/10.1145/3626772.3657895)|Jack McKechnie, Graham McDonald, Craig Macdonald|University of Glasgow|Cross-encoders leverage fine-grained interactions between documents and queries for effective relevance ranking. Such ranking models are typically trained to satisfy the single objective of providing relevant information to the users. However, not all information should be made available. For example, documents containing sensitive information, such as personal or confidential information, should not be returned in the search results. Sensitivity-aware search (SAS) aims to develop retrieval models that can satisfy two objectives, namely: (1) providing the user with relevant search results, while (2) ensuring that no documents that contain sensitive information are included in the ranking. In this work, we propose three novel negative sampling strategies that enable cross-encoders to be trained to satisfy the bi-objective task of SAS. Additionally, we investigate and compare with filtering sensitive documents in ranking pipelines. Our experiments on a collection labelled for sensitivity show that our proposed negative sampling strategies lead to a ~37% increase in terms of cost-sensitive nDCG (nCSDCG) for SAS.|交叉编码器利用文档和查询之间的细粒度交互来进行有效的相关性排序。这种排名模型通常经过训练，以满足向用户提供相关信息的单一目标。然而，并非所有的信息都应该提供。例如，包含敏感信息(如个人或机密信息)的文档不应在搜索结果中返回。敏感性搜索(SAS)旨在开发能够满足两个目标的检索模型，即: (1)为用户提供相关的搜索结果，同时(2)确保没有包含敏感信息的文档被包含在排名中。在这项工作中，我们提出了三种新颖的负采样策略，使交叉编码器的训练，以满足 SAS 的双目标任务。此外，我们还研究和比较了在排序管道中过滤敏感文档的方法。我们对标记为敏感性的集合的实验表明，我们提出的阴性采样策略导致 SAS 的成本敏感性 nDCG (nCSDCG)增加约37% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bi-Objective+Negative+Sampling+for+Sensitivity-Aware+Search)|0|
|[Relevance Feedback Method For Patent Searching Using Vector Subspaces](https://doi.org/10.1145/3626772.3661365)|Sebastian Björkqvist|IPRally Technologies Oy|Searching for novelty-destroying prior art is an important part of patent application drafting and invalidation. The task is challenging due to the detailed information needed to determine whether a document is novelty-destroying or simply closely related, resulting in the original search results not always being fully on target. Allowing the user to provide feedback on the relevance of the initial search results and iterating on the search may thus improve the results significantly. We present a relevance feedback method based on computing the affine vector subspace spanned by the relevant document vectors. The method can be used with any dense retrieval system, and we demonstrate its effectiveness in improving recall in prior art searches. We compare the subspace-based method to the Rocchio algorithm and show that the method is less sensitive to changes in hyperparameters when the number of relevant documents increases.|查找毁新技术是专利申请起草和失效的重要组成部分。这项任务具有挑战性，因为确定一份文件是否具有新颖性或仅仅是密切相关所需的详细信息，导致原始搜索结果并不总是完全符合目标。因此，允许用户就初始搜索结果的相关性提供反馈并对搜索进行迭代，可以大大改进搜索结果。我们提出了一种基于计算相关文档向量所跨越的仿射向量子空间的关联反馈方法。该方法可以应用于任何密集检索系统，并证明了该方法在提高现有技术检索中的召回率方面的有效性。我们比较了基于子空间的方法和 Rocchio 算法，发现当相关文档数量增加时，该方法对超参数的变化不太敏感。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Relevance+Feedback+Method+For+Patent+Searching+Using+Vector+Subspaces)|0|
|[Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations](https://doi.org/10.1145/3626772.3657769)|Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini|Pinecone; ISTI-CNR; Dipartimento di Informatica, Università di Pisa|Learned sparse representations form an attractive class of contextual embeddings for text retrieval. That is so because they are effective models of relevance and are interpretable by design. Despite their apparent compatibility with inverted indexes, however, retrieval over sparse embeddings remains challenging. That is due to the distributional differences between learned embeddings and term frequency-based lexical models of relevance such as BM25. Recognizing this challenge, a great deal of research has gone into, among other things, designing retrieval algorithms tailored to the properties of learned sparse representations, including approximate retrieval systems. In fact, this task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on a large benchmark dataset by throughput and recall. In this work, we propose a novel organization of the inverted index that enables fast yet effective approximate retrieval over learned sparse embeddings. Our approach organizes inverted lists into geometrically-cohesive blocks, each equipped with a summary vector. During query processing, we quickly determine if a block must be evaluated using the summaries. As we show experimentally, single-threaded query processing using our method, Seismic, reaches sub-millisecond per-query latency on various sparse embeddings of the MS MARCO dataset while maintaining high recall. Our results indicate that Seismic is one to two orders of magnitude faster than state-of-the-art inverted index-based solutions and further outperforms the winning (graph-based) submissions to the BigANN Challenge by a significant margin.|学习的稀疏表示形成了一类有吸引力的文本检索上下文嵌入。之所以如此，是因为它们是有效的相关性模型，可以通过设计加以解释。然而，尽管它们与反向索引具有明显的兼容性，但是通过稀疏嵌入进行检索仍然具有挑战性。这是由于学习嵌入和基于词汇频率的关联词汇模型(如 BM25)之间的分布差异造成的。认识到这一挑战，大量的研究已经进入，除其他事项外，设计检索算法适合于学习稀疏表示的属性，包括近似检索系统。事实上，这项任务在 NeurIPS 2023最新的 BigANN 挑战中占有显著地位，在这个挑战中，通过吞吐量和召回率对大型基准数据集上的近似算法进行了评估。在这项工作中，我们提出了一种新的组织倒排索引，使快速而有效的近似检索学习稀疏嵌入。我们的方法将倒排的列表组织成具有几何内聚性的块，每个块配备一个汇总向量。在查询处理过程中，我们快速确定是否必须使用摘要计算块。正如我们的实验表明，使用我们的方法，地震，单线程查询处理达到亚毫秒每查询延迟各种稀疏嵌入的 MS MARCO 数据集，同时保持高召回率。我们的研究结果表明，地震数量级比最先进的基于倒排索引的解决方案快一到两倍，并进一步优于 BigANN 挑战赛的获胜者(基于图表的)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Inverted+Indexes+for+Approximate+Retrieval+over+Learned+Sparse+Representations)|0|
|[Can We Trust Recommender System Fairness Evaluation? The Role of Fairness and Relevance](https://doi.org/10.1145/3626772.3657832)|Theresia Veronika Rampisela, Tuukka Ruotsalo, Maria Maistro, Christina Lioma|University of Copenhagen|Relevance and fairness are two major objectives of recommender systems (RSs). Recent work proposes measures of RS fairness that are either independent from relevance (fairness-only) or conditioned on relevance (joint measures). While fairness-only measures have been studied extensively, we look into whether joint measures can be trusted. We collect all joint evaluation measures of RS relevance and fairness, and ask: How much do they agree with each other? To what extent do they agree with relevance/fairness measures? How sensitive are they to changes in rank position, or to increasingly fair and relevant recommendations? We empirically study for the first time the behaviour of these measures across 4 real-world datasets and 4 recommenders. We find that most of these measures: i) correlate weakly with one another and even contradict each other at times; ii) are less sensitive to rank position changes than relevance- and fairness-only measures, meaning that they are less granular than traditional RS measures; and iii) tend to compress scores at the low end of their range, meaning that they are not very expressive. We counter the above limitations with a set of guidelines on the appropriate usage of such measures, i.e., they should be used with caution due to their tendency to contradict each other and of having a very small empirical range.|相关性和公平性是推荐系统的两个主要目标。最近的研究提出了 RS 公平性的测量方法，这些测量方法要么独立于相关性(仅仅是公平性) ，要么以相关性(联合测量)为条件。虽然只有公平的措施已经得到了广泛的研究，但是我们研究的是联合措施是否可以信任。我们收集了所有 RS 相关性和公平性的联合评价指标，并问: 它们之间有多大程度的一致性？它们在多大程度上同意相关性/公平性措施？他们对职位的变化，或者对越来越公平和相关的建议有多敏感？我们首次实证研究了这些措施的行为在4个真实世界的数据集和4个推荐。我们发现这些测量中的大多数: i)彼此之间相关性很弱，有时甚至相互矛盾; ii)对排名位置变化的敏感性低于相关性和公平性测量，这意味着它们比传统的 RS 测量粒度更小; iii)倾向于压缩其范围的低端分数，这意味着它们不是非常具有表现力。针对上述限制，我们制定了一套关于适当使用此类措施的指导方针，即应谨慎使用这些措施，因为它们往往相互矛盾，而且经验范围很小。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+We+Trust+Recommender+System+Fairness+Evaluation?+The+Role+of+Fairness+and+Relevance)|0|
|[Sequential Recommendation with Latent Relations based on Large Language Model](https://doi.org/10.1145/3626772.3657762)|Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, Min Zhang|Meituan; Tsinghua University|Sequential recommender systems predict items that may interest users by modeling their preferences based on historical interactions. Traditional sequential recommendation methods rely on capturing implicit collaborative filtering signals among items. Recent relation-aware sequential recommendation models have achieved promising performance by explicitly incorporating item relations into the modeling of user historical sequences, where most relations are extracted from knowledge graphs. However, existing methods rely on manually predefined relations and suffer the sparsity issue, limiting the generalization ability in diverse scenarios with varied item relations. In this paper, we propose a novel relation-aware sequential recommendation framework with Latent Relation Discovery (LRD). Different from previous relation-aware models that rely on predefined rules, we propose to leverage the Large Language Model (LLM) to provide new types of relations and connections between items. The motivation is that LLM contains abundant world knowledge, which can be adopted to mine latent relations of items for recommendation. Specifically, inspired by that humans can describe relations between items using natural language, LRD harnesses the LLM that has demonstrated human-like knowledge to obtain language knowledge representations of items. These representations are fed into a latent relation discovery module based on the discrete state variational autoencoder (DVAE). Then the self-supervised relation discovery tasks and recommendation tasks are jointly optimized. Experimental results on multiple public datasets demonstrate our proposed latent relations discovery method can be incorporated with existing relation-aware sequential recommendation models and significantly improve the performance. Further analysis experiments indicate the effectiveness and reliability of the discovered latent relations.|顺序推荐系统通过基于历史交互对用户偏好进行建模来预测用户可能感兴趣的项目。传统的顺序推荐方法依赖于捕捉项目之间隐含的协同过滤信号。最近的关系感知序列推荐模型已经取得了良好的性能，明确地结合项目关系到用户历史序列的建模，其中大多数关系是从知识图提取。然而，现有的方法依赖于人工预定义的关系，并且存在稀疏性问题，限制了在不同项目关系的不同场景中的泛化能力。本文提出了一种新的基于潜在关系发现(LRD)的关系感知序列推荐框架。与以前依赖于预定义规则的关系感知模型不同，我们建议利用大语言模型(LLM)来提供新类型的关系和项之间的连接。其动机是 LLM 包含了丰富的世界知识，可以用来挖掘推荐项目的潜在关系。具体来说，受到人类可以使用自然语言描述项目之间关系的启发，LRD 利用已经证明类似于人类的知识的 LLM 来获得项目的语言知识表示。这些表示被反馈到基于离散状态变分自动编码器(DVAE)的潜在关系发现模块中。然后对自监督关系发现任务和推荐任务进行联合优化。在多个公共数据集上的实验结果表明，本文提出的潜在关系发现方法可以与现有的关系感知顺序推荐模型相结合，从而显著提高推荐性能。进一步的分析实验表明了所发现的潜在关系的有效性和可靠性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Sequential+Recommendation+with+Latent+Relations+based+on+Large+Language+Model)|0|
|[Enhancing Sequential Recommenders with Augmented Knowledge from Aligned Large Language Models](https://doi.org/10.1145/3626772.3657782)|Yankun Ren, Zhongde Chen, Xinxing Yang, Longfei Li, Cong Jiang, Lei Cheng, Bo Zhang, Linjian Mo, Jun Zhou|Ant Group|Recommender systems are widely used in various online platforms. In the context of sequential recommendation, it is essential to accurately capture the chronological patterns in user activities to generate relevant recommendations. Conventional ID-based sequential recommenders have shown promise but lack comprehensive real-world knowledge about items, limiting their effectiveness. Recent advancements in Large Language Models (LLMs) offer the potential to bridge this gap by leveraging the extensive real-world knowledge encapsulated in LLMs. However, integrating LLMs into sequential recommender systems comes with its own challenges, including inadequate representation of sequential behavior patterns and long inference latency. In this paper, we propose SeRALM (Enhancing <u>Se</u>quential <u>R</u>ecommenders with Augmented Knowledge from <u>A</u>ligned Large <u>L</u>anguage <u>M</u>odels) to address these challenges. SeRALM integrates LLMs with conventional ID-based sequential recommenders for sequential recommendation tasks. We combine text-format knowledge generated by LLMs with item IDs and feed this enriched data into ID-based recommenders, benefitting from the strengths of both paradigms. Moreover, we develop a theoretically underpinned alignment training method to refine LLMs' generation using feedback from ID-based recommenders for better knowledge augmentation. We also present an asynchronous technique to expedite the alignment training process. Experimental results on public benchmarks demonstrate that SeRALM significantly improves the performances of ID-based sequential recommenders. Further, a series of ablation studies and analyses corroborate SeRALM's proficiency in steering LLMs to generate more pertinent and advantageous knowledge across diverse scenarios.|推荐系统广泛应用于各种在线平台。在顺序推荐的背景下，准确地捕获用户活动中的顺序模式以生成相关的推荐是至关重要的。传统的基于 ID 的顺序推荐已经显示出希望，但是缺乏关于项目的全面的现实世界知识，限制了它们的有效性。大型语言模型(LLM)中的最新进展提供了通过利用 LLM 中封装的广泛的现实世界知识来弥补这一差距的潜力。然而，将 LLM 集成到顺序推荐系统中也有其自身的挑战，包括顺序行为模式的不充分表示和长的推理延迟。在这篇论文中，我们提出了 SeRALM (增强 < u > Se </u > 量 < u > R </u > 从 < u > A </u > 线性大 < u > L </u > 语言 < u > M </u > 模型的增强知识推荐)来解决这些挑战。SerRALM 将 LLM 与传统的基于 ID 的顺序推荐器集成在一起，用于顺序推荐任务。我们将 LLM 生成的文本格式知识与项目 ID 结合起来，并将这些丰富的数据提供给基于 ID 的推荐程序，这两种范例的优势使我们受益匪浅。此外，我们开发了一个理论上支持的对齐训练方法来细化 LLM 的生成，使用基于 ID 的推荐者的反馈来更好地增强知识。我们还提出了一种异步技术，以加快对准训练过程。对公共基准测试的实验结果表明，基于 ID 的顺序推荐算法的性能得到了明显的改善。此外，一系列的消融研究和分析证实了 SerRALM 在指导 LLM 方面的能力，以便在不同的情况下产生更相关和更有利的知识。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Sequential+Recommenders+with+Augmented+Knowledge+from+Aligned+Large+Language+Models)|0|
|[Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment](https://doi.org/10.1145/3626772.3657709)|Xinyu Zhu, Lilin Zhang, Ning Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+Fair+Representation+Learning+for+Personalized+Fairness+in+Recommendations+via+Information+Alignment)|0|
|[MealRec+: A Meal Recommendation Dataset with Meal-Course Affiliation for Personalization and Healthiness](https://doi.org/10.1145/3626772.3657857)|Ming Li, Lin Li, Xiaohui Tao, Jimmy Xiangji Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MealRec+:+A+Meal+Recommendation+Dataset+with+Meal-Course+Affiliation+for+Personalization+and+Healthiness)|0|
|[IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT](https://doi.org/10.1145/3626772.3657725)|Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Jie Wang, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IISAN:+Efficiently+Adapting+Multimodal+Representation+for+Sequential+Recommendation+with+Decoupled+PEFT)|0|
|[FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation](https://doi.org/10.1145/3626772.3657853)|Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FeB4RAG:+Evaluating+Federated+Search+in+the+Context+of+Retrieval+Augmented+Generation)|0|
|[Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation](https://doi.org/10.1145/3626772.3657695)|Zhe Xu, Daoyuan Chen, Jiayi Kuang, Zihao Yi, Yaliang Li, Ying Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Demonstration+Retrieval+and+Cognitive+Understanding+for+Emotional+Support+Conversation)|0|
|[Broadening the View: Demonstration-augmented Prompt Learning for Conversational Recommendation](https://doi.org/10.1145/3626772.3657755)|Huy Dao, Yang Deng, Dung D. Le, Lizi Liao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Broadening+the+View:+Demonstration-augmented+Prompt+Learning+for+Conversational+Recommendation)|0|
|[ProCIS: A Benchmark for Proactive Retrieval in Conversations](https://doi.org/10.1145/3626772.3657869)|Chris Samarinas, Hamed Zamani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProCIS:+A+Benchmark+for+Proactive+Retrieval+in+Conversations)|0|
|[An Empirical Analysis on Multi-turn Conversational Recommender Systems](https://doi.org/10.1145/3626772.3657893)|Lu Zhang, Chen Li, Yu Lei, Zhu Sun, Guanfeng Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Empirical+Analysis+on+Multi-turn+Conversational+Recommender+Systems)|0|
|[SM-RS: Single- and Multi-Objective Recommendations with Contextual Impressions and Beyond-Accuracy Propensity Scores](https://doi.org/10.1145/3626772.3657863)|Patrik Dokoupil, Ladislav Peska, Ludovico Boratto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SM-RS:+Single-+and+Multi-Objective+Recommendations+with+Contextual+Impressions+and+Beyond-Accuracy+Propensity+Scores)|0|
|[To Search or to Recommend: Predicting Open-App Motivation with Neural Hawkes Process](https://doi.org/10.1145/3626772.3657732)|Zhongxiang Sun, Zihua Si, Xiao Zhang, Xiaoxue Zang, Yang Song, Hongteng Xu, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Search+or+to+Recommend:+Predicting+Open-App+Motivation+with+Neural+Hawkes+Process)|0|
|[Counterfactual Ranking Evaluation with Flexible Click Models](https://doi.org/10.1145/3626772.3657810)|Alexander Buchholz, Ben London, Giuseppe Di Benedetto, Jan Malte Lichtenberg, Yannik Stein, Thorsten Joachims||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Ranking+Evaluation+with+Flexible+Click+Models)|0|
|[Deep Pattern Network for Click-Through Rate Prediction](https://doi.org/10.1145/3626772.3657777)|Hengyu Zhang, Junwei Pan, Dapeng Liu, Jie Jiang, Xiu Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Pattern+Network+for+Click-Through+Rate+Prediction)|0|
|[AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations](https://doi.org/10.1145/3626772.3657724)|Wei Wu, Chao Wang, Dazhong Shen, Chuan Qin, Liyi Chen, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AFDGCF:+Adaptive+Feature+De-correlation+Graph+Collaborative+Filtering+for+Recommendations)|0|
|[TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems](https://doi.org/10.1145/3626772.3657721)|Peiyan Zhang, Yuchen Yan, Xi Zhang, Chaozhuo Li, Senzhang Wang, Feiran Huang, Sunghun Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TransGNN:+Harnessing+the+Collaborative+Power+of+Transformers+and+Graph+Neural+Networks+for+Recommender+Systems)|0|
|[Lightweight Embeddings for Graph Collaborative Filtering](https://doi.org/10.1145/3626772.3657820)|Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, Hongzhi Yin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lightweight+Embeddings+for+Graph+Collaborative+Filtering)|0|
|[Graded Relevance Scoring of Written Essays with Dense Retrieval](https://doi.org/10.1145/3626772.3657744)|Salam Albatarni, Sohaila Eltanbouly, Tamer Elsayed||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graded+Relevance+Scoring+of+Written+Essays+with+Dense+Retrieval)|0|
|[Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models](https://doi.org/10.1145/3626772.3657841)|Catherine Chen, Jack Merullo, Carsten Eickhoff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Axiomatic+Causal+Interventions+for+Reverse+Engineering+Relevance+Computation+in+Neural+Retrieval+Models)|0|
|[Optimizing Learning-to-Rank Models for Ex-Post Fair Relevance](https://doi.org/10.1145/3626772.3657751)|Sruthi Gorantla, Eshaan Bhansali, Amit Deshpande, Anand Louis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimizing+Learning-to-Rank+Models+for+Ex-Post+Fair+Relevance)|0|
|[Scaling Sequential Recommendation Models with Transformers](https://doi.org/10.1145/3626772.3657816)|Pablo Zivic, Hernán Ceferino Vázquez, Jorge Sánchez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Sequential+Recommendation+Models+with+Transformers)|0|
|[SelfGNN: Self-Supervised Graph Neural Networks for Sequential Recommendation](https://doi.org/10.1145/3626772.3657716)|Yuxi Liu, Lianghao Xia, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SelfGNN:+Self-Supervised+Graph+Neural+Networks+for+Sequential+Recommendation)|0|
|[Revisit Targeted Model Poisoning on Federated Recommendation: Optimize via Multi-objective Transport](https://doi.org/10.1145/3626772.3657764)|Jiajie Su, Chaochao Chen, Weiming Liu, Zibin Lin, Shuheng Shen, Weiqiang Wang, Xiaolin Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisit+Targeted+Model+Poisoning+on+Federated+Recommendation:+Optimize+via+Multi-objective+Transport)|0|
|[LoRec: Combating Poisons with Large Language Model for Robust Sequential Recommendation](https://doi.org/10.1145/3626772.3657684)|Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LoRec:+Combating+Poisons+with+Large+Language+Model+for+Robust+Sequential+Recommendation)|0|
|[Treatment Effect Estimation for User Interest Exploration on Recommender Systems](https://doi.org/10.1145/3626772.3657736)|Jiaju Chen, Wenjie Wang, Chongming Gao, Peng Wu, Jianxiong Wei, Qingsong Hua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Treatment+Effect+Estimation+for+User+Interest+Exploration+on+Recommender+Systems)|0|
|[Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization](https://doi.org/10.1145/3626772.3657705)|Pancheng Wang, Shasha Li, Dong Li, Kehan Long, Jintao Tang, Ting Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangling+Instructive+Information+from+Ranked+Multiple+Candidates+for+Multi-Document+Scientific+Summarization)|0|
|[Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check](https://doi.org/10.1145/3626772.3657980)|Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Boosting+Conversational+Question+Answering+with+Fine-Grained+Retrieval-Augmentation+and+Self-Check)|0|
|[Can Query Expansion Improve Generalization of Strong Cross-Encoder Rankers?](https://doi.org/10.1145/3626772.3657979)|Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+Query+Expansion+Improve+Generalization+of+Strong+Cross-Encoder+Rankers?)|0|
|[EASE-DR: Enhanced Sentence Embeddings for Dense Retrieval](https://doi.org/10.1145/3626772.3657925)|Xixi Zhou, Yang Gao, Xin Jie, Xiaoxu Cai, Jiajun Bu, Haishuai Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EASE-DR:+Enhanced+Sentence+Embeddings+for+Dense+Retrieval)|0|
|[Explainable Uncertainty Attribution for Sequential Recommendation](https://doi.org/10.1145/3626772.3657900)|Carles Balsells Rodas, Fan Yang, Zhishen Huang, Yan Gao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainable+Uncertainty+Attribution+for+Sequential+Recommendation)|0|
|[FedUD: Exploiting Unaligned Data for Cross-Platform Federated Click-Through Rate Prediction](https://doi.org/10.1145/3626772.3657941)|Wentao Ouyang, Rui Dong, Ri Tao, Xiangzheng Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FedUD:+Exploiting+Unaligned+Data+for+Cross-Platform+Federated+Click-Through+Rate+Prediction)|0|
|[Generalizable Tip-of-the-Tongue Retrieval with LLM Re-ranking](https://doi.org/10.1145/3626772.3657917)|Luís Borges, Rohan Jha, Jamie Callan, Bruno Martins||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generalizable+Tip-of-the-Tongue+Retrieval+with+LLM+Re-ranking)|0|
|[Grasping Both Query Relevance and Essential Content for Query-focused Summarization](https://doi.org/10.1145/3626772.3657958)|Ye Xiong, Hidetaka Kamigaito, Soichiro Murakami, Peinan Zhang, Hiroya Takamura, Manabu Okumura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grasping+Both+Query+Relevance+and+Essential+Content+for+Query-focused+Summarization)|0|
|[MoME: Mixture-of-Masked-Experts for Efficient Multi-Task Recommendation](https://doi.org/10.1145/3626772.3657922)|Jiahui Xu, Lu Sun, Dengji Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MoME:+Mixture-of-Masked-Experts+for+Efficient+Multi-Task+Recommendation)|0|
|[Multi-Layer Ranking with Large Language Models for News Source Recommendation](https://doi.org/10.1145/3626772.3657966)|Wenjia Zhang, Lin Gui, Rob Procter, Yulan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Layer+Ranking+with+Large+Language+Models+for+News+Source+Recommendation)|0|
|[Neural Click Models for Recommender Systems](https://doi.org/10.1145/3626772.3657939)|Mikhail Shirokikh, Ilya Shenbin, Anton Alekseev, Anna Volodkevich, Alexey Vasilev, Andrey V. Savchenko, Sergey I. Nikolenko||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Click+Models+for+Recommender+Systems)|0|
|[SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval](https://doi.org/10.1145/3626772.3657910)|Zihao Li, Yuyi Ao, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SpherE:+Expressive+and+Interpretable+Knowledge+Graph+Embedding+for+Set+Retrieval)|0|
|[CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models](https://doi.org/10.1145/3626772.3657672)|Peiyuan Gong, Jiamian Li, Jiaxin Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CoSearchAgent:+A+Lightweight+Collaborative+Search+Agent+with+Large+Language+Models)|0|
|[MeMemo: On-device Retrieval Augmentation for Private and Personalized Text Generation](https://doi.org/10.1145/3626772.3657662)|Zijie J. Wang, Duen Horng Chau||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MeMemo:+On-device+Retrieval+Augmentation+for+Private+and+Personalized+Text+Generation)|0|
|[Monitoring the Evolution of Behavioural Embeddings in Social Media Recommendation](https://doi.org/10.1145/3626772.3661368)|Srijan Saket, Olivier Jeunen, Md. Danish Kalim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Monitoring+the+Evolution+of+Behavioural+Embeddings+in+Social+Media+Recommendation)|0|
|[Embedding Based Deduplication in E-commerce AutoComplete](https://doi.org/10.1145/3626772.3661373)|Shaodan Zhai, Yuwei Chen, Yixue Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Embedding+Based+Deduplication+in+E-commerce+AutoComplete)|0|
|[Using and Evaluating Quantum Computing for Information Retrieval and Recommender Systems](https://doi.org/10.1145/3626772.3661378)|Maurizio Ferrari Dacrema, Andrea Pasin, Paolo Cremonesi, Nicola Ferro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+and+Evaluating+Quantum+Computing+for+Information+Retrieval+and+Recommender+Systems)|0|
|[Reinforcing Long-Term Performance in Recommender Systems with User-Oriented Exploration Policy](https://doi.org/10.1145/3626772.3657714)|Changshuo Zhang, Sirui Chen, Xiao Zhang, Sunhao Dai, Weijie Yu, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcing+Long-Term+Performance+in+Recommender+Systems+with+User-Oriented+Exploration+Policy)|0|
|[Unsupervised Cross-Domain Image Retrieval with Semantic-Attended Mixture-of-Experts](https://doi.org/10.1145/3626772.3657826)|Kai Wang, Jiayang Liu, Xing Xu, Jingkuan Song, Xin Liu, Heng Tao Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Cross-Domain+Image+Retrieval+with+Semantic-Attended+Mixture-of-Experts)|0|
|[Multilingual Meta-Distillation Alignment for Semantic Retrieval](https://doi.org/10.1145/3626772.3657812)|Meryem M'hamdi, Jonathan May, Franck Dernoncourt, Trung Bui, Seunghyun Yoon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multilingual+Meta-Distillation+Alignment+for+Semantic+Retrieval)|0|
|[Dataset and Models for Item Recommendation Using Multi-Modal User Interactions](https://doi.org/10.1145/3626772.3657881)|Simone Borg Bruun, Krisztian Balog, Maria Maistro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dataset+and+Models+for+Item+Recommendation+Using+Multi-Modal+User+Interactions)|0|
|[Behavior-Contextualized Item Preference Modeling for Multi-Behavior Recommendation](https://doi.org/10.1145/3626772.3657696)|Mingshi Yan, Fan Liu, Jing Sun, Fuming Sun, Zhiyong Cheng, Yahong Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior-Contextualized+Item+Preference+Modeling+for+Multi-Behavior+Recommendation)|0|
|[Exploring the Individuality and Collectivity of Intents behind Interactions for Graph Collaborative Filtering](https://doi.org/10.1145/3626772.3657738)|Yi Zhang, Lei Sang, Yiwen Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Individuality+and+Collectivity+of+Intents+behind+Interactions+for+Graph+Collaborative+Filtering)|0|
|[Content-based Graph Reconstruction for Cold-start Item Recommendation](https://doi.org/10.1145/3626772.3657801)|Jinri Kim, Eungi Kim, Kwangeun Yeo, Yujin Jeon, Chanwoo Kim, Sewon Lee, Joonseok Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Content-based+Graph+Reconstruction+for+Cold-start+Item+Recommendation)|0|
|[Unbiased Learning-to-Rank Needs Unconfounded Propensity Estimation](https://doi.org/10.1145/3626772.3657772)|Dan Luo, Lixin Zou, Qingyao Ai, Zhiyu Chen, Chenliang Li, Dawei Yin, Brian D. Davison||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Learning-to-Rank+Needs+Unconfounded+Propensity+Estimation)|0|
|[Scenario-Adaptive Fine-Grained Personalization Network: Tailoring User Behavior Representation to the Scenario Context](https://doi.org/10.1145/3626772.3657803)|Moyu Zhang, Yongxiang Tang, Jinxin Hu, Yu Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scenario-Adaptive+Fine-Grained+Personalization+Network:+Tailoring+User+Behavior+Representation+to+the+Scenario+Context)|0|
|[EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention](https://doi.org/10.1145/3626772.3657805)|Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, JiRong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EulerFormer:+Sequential+User+Behavior+Modeling+with+Complex+Vector+Attention)|0|
|[Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors](https://doi.org/10.1145/3626772.3657974)|Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, Linjian Mo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Breaking+the+Length+Barrier:+LLM-Enhanced+CTR+Prediction+in+Long+Textual+User+Behaviors)|0|
|[Multi-intent-aware Session-based Recommendation](https://doi.org/10.1145/3626772.3657928)|Minjin Choi, Hyeyoung Kim, Hyunsouk Cho, Jongwuk Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-intent-aware+Session-based+Recommendation)|0|
|[PLAID SHIRTTT for Large-Scale Streaming Dense Retrieval](https://doi.org/10.1145/3626772.3657964)|Dawn J. Lawrie, Efsun Selin Kayi, Eugene Yang, James Mayfield, Douglas W. Oard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PLAID+SHIRTTT+for+Large-Scale+Streaming+Dense+Retrieval)|0|
|[Towards Ethical Item Ranking: A Paradigm Shift from User-Centric to Item-Centric Approaches](https://doi.org/10.1145/3626772.3657977)|Guilherme Ramos, Mirko Marras, Ludovico Boratto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Ethical+Item+Ranking:+A+Paradigm+Shift+from+User-Centric+to+Item-Centric+Approaches)|0|
|[A Large-scale Offer Alignment Model for Partitioning Filtering and Matching Product Offers](https://doi.org/10.1145/3626772.3661351)|Wenyu Huang, André Melo, Jeff Z. Pan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Large-scale+Offer+Alignment+Model+for+Partitioning+Filtering+and+Matching+Product+Offers)|0|
|[Interest Clock: Time Perception in Real-Time Streaming Recommendation System](https://doi.org/10.1145/3626772.3661369)|Yongchun Zhu, Jingwu Chen, Ling Chen, Yitan Li, Feng Zhang, Zuotao Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interest+Clock:+Time+Perception+in+Real-Time+Streaming+Recommendation+System)|0|
|[Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback](https://doi.org/10.1145/3626772.3657689)|Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unsupervised+Large+Language+Model+Alignment+for+Information+Retrieval+via+Contrastive+Feedback)|0|
|[Amazon-KG: A Knowledge Graph Enhanced Cross-Domain Recommendation Dataset](https://doi.org/10.1145/3626772.3657880)|Yuhan Wang, Qing Xie, Mengzi Tang, Lin Li, Jingling Yuan, Yongjian Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Amazon-KG:+A+Knowledge+Graph+Enhanced+Cross-Domain+Recommendation+Dataset)|0|
|[Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive Multimodal Knowledge Graph Completion](https://doi.org/10.1145/3626772.3657838)|Yu Zhao, Ying Zhang, Baohang Zhou, Xinying Qian, Kehui Song, Xiangrui Cai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contrast+then+Memorize:+Semantic+Neighbor+Retrieval-Enhanced+Inductive+Multimodal+Knowledge+Graph+Completion)|0|
|[The Treatment of Ties in Rank-Biased Overlap](https://doi.org/10.1145/3626772.3657700)|Matteo Corsi, Julián Urbano||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Treatment+of+Ties+in+Rank-Biased+Overlap)|0|
|[What Matters in a Measure? A Perspective from Large-Scale Search Evaluation](https://doi.org/10.1145/3626772.3657845)|Paul Thomas, Gabriella Kazai, Nick Craswell, Seth Spielman||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+Matters+in+a+Measure?+A+Perspective+from+Large-Scale+Search+Evaluation)|0|
|[CaDRec: Contextualized and Debiased Recommender Model](https://doi.org/10.1145/3626772.3657799)|Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Jiyi Li, Dongjin Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaDRec:+Contextualized+and+Debiased+Recommender+Model)|0|
|[Going Beyond Popularity and Positivity Bias: Correcting for Multifactorial Bias in Recommender Systems](https://doi.org/10.1145/3626772.3657749)|Jin Huang, Harrie Oosterhuis, Masoud Mansoury, Herke van Hoof, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Going+Beyond+Popularity+and+Positivity+Bias:+Correcting+for+Multifactorial+Bias+in+Recommender+Systems)|0|
|[Configurable Fairness for New Item Recommendation Considering Entry Time of Items](https://doi.org/10.1145/3626772.3657694)|Huizhong Guo, Dongxia Wang, Zhu Sun, Haonan Zhang, Jinfeng Li, Jie Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Configurable+Fairness+for+New+Item+Recommendation+Considering+Entry+Time+of+Items)|0|
|[Generative Retrieval via Term Set Generation](https://doi.org/10.1145/3626772.3657797)|Peitian Zhang, Zheng Liu, Yujia Zhou, Zhicheng Dou, Fangchao Liu, Zhao Cao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Retrieval+via+Term+Set+Generation)|0|
|[MIRROR: A Multi-View Reciprocal Recommender System for Online Recruitment](https://doi.org/10.1145/3626772.3657776)|Zhi Zheng, Xiao Hu, Shanshan Gao, Hengshu Zhu, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIRROR:+A+Multi-View+Reciprocal+Recommender+System+for+Online+Recruitment)|0|
|[Who To Align With: Feedback-Oriented Multi-Modal Alignment in Recommendation Systems](https://doi.org/10.1145/3626772.3657701)|Yang Li, Qi'ao Zhao, Chen Lin, Jinsong Su, Zhilin Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Who+To+Align+With:+Feedback-Oriented+Multi-Modal+Alignment+in+Recommendation+Systems)|0|
|[EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation](https://doi.org/10.1145/3626772.3657890)|Shaorun Zhang, Zhiyu He, Ziyi Ye, Peijie Sun, Qingyao Ai, Min Zhang, Yiqun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EEG-SVRec:+An+EEG+Dataset+with+User+Multidimensional+Affective+Engagement+Labels+in+Short+Video+Recommendation)|0|
|[Multimodality Invariant Learning for Multimedia-Based New Item Recommendation](https://doi.org/10.1145/3626772.3658596)|Haoyue Bai, Le Wu, Min Hou, Miaomiao Cai, Zhuangzhuang He, Yuyang Zhou, Richang Hong, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodality+Invariant+Learning+for+Multimedia-Based+New+Item+Recommendation)|0|
|[Semi-supervised Prototype Semantic Association Learning for Robust Cross-modal Retrieval](https://doi.org/10.1145/3626772.3657756)|Junsheng Wang, Tiantian Gong, Yan Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Semi-supervised+Prototype+Semantic+Association+Learning+for+Robust+Cross-modal+Retrieval)|0|
|[Hypergraph Convolutional Network for User-Oriented Fairness in Recommender Systems](https://doi.org/10.1145/3626772.3657737)|Zhongxuan Han, Chaochao Chen, Xiaolin Zheng, Li Zhang, Yuyuan Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hypergraph+Convolutional+Network+for+User-Oriented+Fairness+in+Recommender+Systems)|0|
|[Hierarchical Semantics Alignment for 3D Human Motion Retrieval](https://doi.org/10.1145/3626772.3657804)|Yang Yang, Haoyu Shi, Huaiwen Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Semantics+Alignment+for+3D+Human+Motion+Retrieval)|0|
|[A Large Scale Test Corpus for Semantic Table Search](https://doi.org/10.1145/3626772.3657877)|Aristotelis Leventidis, Martin Pekár Christensen, Matteo Lissandrini, Laura Di Rocco, Katja Hose, Renée J. Miller||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Large+Scale+Test+Corpus+for+Semantic+Table+Search)|0|
|[JDivPS: A Diversified Product Search Dataset](https://doi.org/10.1145/3626772.3657888)|Zhirui Deng, Zhicheng Dou, Yutao Zhu, Xubo Qin, Pengchao Cheng, Jiangxu Wu, Hao Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JDivPS:+A+Diversified+Product+Search+Dataset)|0|
|[An E-Commerce Dataset Revealing Variations during Sales](https://doi.org/10.1145/3626772.3657870)|Jianfu Zhang, Qingtao Yu, Yizhou Chen, Guoliang Zhou, Yawen Liu, Yawei Sun, Chen Liang, Guangda Huzhang, Yabo Ni, Anxiang Zeng, Han Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+E-Commerce+Dataset+Revealing+Variations+during+Sales)|0|
|[Exploring Multi-Scenario Multi-Modal CTR Prediction with a Large Scale Dataset](https://doi.org/10.1145/3626772.3657865)|Zhaoxin Huan, Ke Ding, Ang Li, Xiaolu Zhang, Xu Min, Yong He, Liang Zhang, Jun Zhou, Linjian Mo, Jinjie Gu, Zhongyi Liu, Wenliang Zhong, Guannan Zhang, Chenliang Li, Fajie Yuan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Multi-Scenario+Multi-Modal+CTR+Prediction+with+a+Large+Scale+Dataset)|0|
|[Dimension Importance Estimation for Dense Information Retrieval](https://doi.org/10.1145/3626772.3657691)|Guglielmo Faggioli, Nicola Ferro, Raffaele Perego, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dimension+Importance+Estimation+for+Dense+Information+Retrieval)|0|
|[Large Language Models for Next Point-of-Interest Recommendation](https://doi.org/10.1145/3626772.3657840)|Peibo Li, Maarten de Rijke, Hao Xue, Shuang Ao, Yang Song, Flora D. Salim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Next+Point-of-Interest+Recommendation)|0|
|[The Impact of Group Membership Bias on the Quality and Fairness of Exposure in Ranking](https://doi.org/10.1145/3626772.3657752)|Ali Vardasbi, Maarten de Rijke, Fernando Diaz, Mostafa Dehghani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Impact+of+Group+Membership+Bias+on+the+Quality+and+Fairness+of+Exposure+in+Ranking)|0|
|[Grand: A Fast and Accurate Graph Retrieval Framework via Knowledge Distillation](https://doi.org/10.1145/3626772.3657773)|Lin Lan, Pinghui Wang, Rui Shi, Tingqing Liu, Juxiang Zeng, Feiyang Sun, Yang Ren, Jing Tao, Xiaohong Guan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grand:+A+Fast+and+Accurate+Graph+Retrieval+Framework+via+Knowledge+Distillation)|0|
|[Unmasking Privacy: A Reproduction and Evaluation Study of Obfuscation-based Perturbation Techniques for Collaborative Filtering](https://doi.org/10.1145/3626772.3657858)|Alex Martinez, Mihnea Tufis, Ludovico Boratto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unmasking+Privacy:+A+Reproduction+and+Evaluation+Study+of+Obfuscation-based+Perturbation+Techniques+for+Collaborative+Filtering)|0|
|[GPT4Rec: Graph Prompt Tuning for Streaming Recommendation](https://doi.org/10.1145/3626772.3657720)|Peiyan Zhang, Yuchen Yan, Xi Zhang, Liying Kang, Chaozhuo Li, Feiran Huang, Senzhang Wang, Sunghun Kim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GPT4Rec:+Graph+Prompt+Tuning+for+Streaming+Recommendation)|0|
|[I3: Intent-Introspective Retrieval Conditioned on Instructions](https://doi.org/10.1145/3626772.3657745)|Kaihang Pan, Juncheng Li, Wenjie Wang, Hao Fei, Hongye Song, Wei Ji, Jun Lin, Xiaozhong Liu, TatSeng Chua, Siliang Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=I3:+Intent-Introspective+Retrieval+Conditioned+on+Instructions)|0|
|[Disentangling ID and Modality Effects for Session-based Recommendation](https://doi.org/10.1145/3626772.3657748)|Xiaokun Zhang, Bo Xu, Zhaochun Ren, Xiaochen Wang, Hongfei Lin, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangling+ID+and+Modality+Effects+for+Session-based+Recommendation)|0|
|[Large Language Models are Learnable Planners for Long-Term Recommendation](https://doi.org/10.1145/3626772.3657683)|Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+are+Learnable+Planners+for+Long-Term+Recommendation)|0|
|[Identifiability of Cross-Domain Recommendation via Causal Subspace Disentanglement](https://doi.org/10.1145/3626772.3657758)|Jing Du, Zesheng Ye, Bin Guo, Zhiwen Yu, Lina Yao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Identifiability+of+Cross-Domain+Recommendation+via+Causal+Subspace+Disentanglement)|0|
|[DeCoCDR: Deployable Cloud-Device Collaboration for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3657786)|Yu Li, Yi Zhang, Zimu Zhou, Qiang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeCoCDR:+Deployable+Cloud-Device+Collaboration+for+Cross-Domain+Recommendation)|0|
|[Mutual Information-based Preference Disentangling and Transferring for Non-overlapped Multi-target Cross-domain Recommendations](https://doi.org/10.1145/3626772.3657780)|Zhi Li, Daichi Amagata, Yihong Zhang, Takahiro Hara, Shuichiro Haruta, Kei Yonekawa, Mori Kurokawa||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mutual+Information-based+Preference+Disentangling+and+Transferring+for+Non-overlapped+Multi-target+Cross-domain+Recommendations)|0|
|[LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset](https://doi.org/10.1145/3626772.3657887)|Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, Yiqun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LeCaRDv2:+A+Large-Scale+Chinese+Legal+Case+Retrieval+Dataset)|0|
|[Behavior Pattern Mining-based Multi-Behavior Recommendation](https://doi.org/10.1145/3626772.3657973)|Haojie Li, Zhiyong Cheng, Xu Yu, Jinhuan Liu, Guanfeng Liu, Junwei Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Behavior+Pattern+Mining-based+Multi-Behavior+Recommendation)|0|
|[Dense Retrieval with Continuous Explicit Feedback for Systematic Review Screening Prioritisation](https://doi.org/10.1145/3626772.3657921)|Xinyu Mao, Shengyao Zhuang, Bevan Koopman, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dense+Retrieval+with+Continuous+Explicit+Feedback+for+Systematic+Review+Screening+Prioritisation)|0|
|[Cross-reconstructed Augmentation for Dual-target Cross-domain Recommendation](https://doi.org/10.1145/3626772.3657902)|Qingyang Mao, Qi Liu, Zhi Li, Likang Wu, Bing Lv, Zheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cross-reconstructed+Augmentation+for+Dual-target+Cross-domain+Recommendation)|0|
|[Distillation for Multilingual Information Retrieval](https://doi.org/10.1145/3626772.3657955)|Eugene Yang, Dawn J. Lawrie, James Mayfield||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distillation+for+Multilingual+Information+Retrieval)|0|
|[Estimating the Hessian Matrix of Ranking Objectives for Stochastic Learning to Rank with Gradient Boosted Trees](https://doi.org/10.1145/3626772.3657918)|Jingwei Kang, Maarten de Rijke, Harrie Oosterhuis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Estimating+the+Hessian+Matrix+of+Ranking+Objectives+for+Stochastic+Learning+to+Rank+with+Gradient+Boosted+Trees)|0|
|[Information Diffusion Prediction via Cascade-Retrieved In-context Learning](https://doi.org/10.1145/3626772.3657909)|Ting Zhong, Jienan Zhang, Zhangtao Cheng, Fan Zhou, Xueqin Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Information+Diffusion+Prediction+via+Cascade-Retrieved+In-context+Learning)|0|
|[Masked Graph Transformer for Large-Scale Recommendation](https://doi.org/10.1145/3626772.3657971)|Huiyuan Chen, Zhe Xu, ChinChia Michael Yeh, Vivian Lai, Yan Zheng, Minghua Xu, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Masked+Graph+Transformer+for+Large-Scale+Recommendation)|0|
|[Modeling Domains as Distributions with Uncertainty for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3657930)|Xianghui Zhu, Mengqun Jin, Hengyu Zhang, Chang Meng, Daoxin Zhang, Xiu Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Domains+as+Distributions+with+Uncertainty+for+Cross-Domain+Recommendation)|0|
|[SCM4SR: Structural Causal Model-based Data Augmentation for Robust Session-based Recommendation](https://doi.org/10.1145/3626772.3657940)|Muskan Gupta, Priyanka Gupta, Jyoti Narwariya, Lovekesh Vig, Gautam Shroff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCM4SR:+Structural+Causal+Model-based+Data+Augmentation+for+Robust+Session-based+Recommendation)|0|
|[USimAgent: Large Language Models for Simulating Search Users](https://doi.org/10.1145/3626772.3657963)|Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, Jiaxin Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=USimAgent:+Large+Language+Models+for+Simulating+Search+Users)|0|
|[ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation](https://doi.org/10.1145/3626772.3661348)|Chaoqun Hou, Yuanhang Zhou, Yi Cao, Tong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ECAT:+A+Entire+space+Continual+and+Adaptive+Transfer+Learning+Framework+for+Cross-Domain+Recommendation)|0|
|[Minimizing Live Experiments in Recommender Systems: User Simulation to Evaluate Preference Elicitation Policies](https://doi.org/10.1145/3626772.3661358)|ChihWei Hsu, Martin Mladenov, Ofer Meshi, James Pine, Hubert Pham, Shane Li, Xujian Liang, Anton Polishko, Li Yang, Ben Scheetz, Craig Boutilier||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Minimizing+Live+Experiments+in+Recommender+Systems:+User+Simulation+to+Evaluate+Preference+Elicitation+Policies)|0|
|[A Semantic Search Engine for Helping Patients Find Doctors and Locations in a Large Healthcare Organization](https://doi.org/10.1145/3626772.3661349)|Mayank Kejriwal, Hamid Haidarian, MinHsueh Chiu, Andy Xiang, Deep Shrestha, Faizan Javed||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Semantic+Search+Engine+for+Helping+Patients+Find+Doctors+and+Locations+in+a+Large+Healthcare+Organization)|0|
|[Clinical Trial Retrieval via Multi-grained Similarity Learning](https://doi.org/10.1145/3626772.3661366)|Junyu Luo, Cheng Qian, Lucas Glass, Fenglong Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Clinical+Trial+Retrieval+via+Multi-grained+Similarity+Learning)|0|
|[Search under Uncertainty: Cognitive Biases and Heuristics: A Tutorial on Testing, Mitigating and Accounting for Cognitive Biases in Search Experiments](https://doi.org/10.1145/3626772.3661382)|Jiqun Liu, Leif Azzopardi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Search+under+Uncertainty:+Cognitive+Biases+and+Heuristics:+A+Tutorial+on+Testing,+Mitigating+and+Accounting+for+Cognitive+Biases+in+Search+Experiments)|0|
|[TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision](https://doi.org/10.1145/3626772.3657788)|Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, Weinan Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TRAD:+Enhancing+LLM+Agents+with+Step-Wise+Thought+Retrieval+and+Aligned+Decision)|0|
|[Representation Learning and Information Retrieval](https://doi.org/10.1145/3626772.3657995)|Yiming Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Representation+Learning+and+Information+Retrieval)|0|
|["In-Context Learning" or: How I learned to stop worrying and love "Applied Information Retrieval"](https://doi.org/10.1145/3626772.3657842)|Andrew Parry, Debasis Ganguly, Manish Chandra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="In-Context+Learning"+or:+How+I+learned+to+stop+worrying+and+love+"Applied+Information+Retrieval")|0|
|[LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval](https://doi.org/10.1145/3626772.3657740)|Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, Changsheng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LDRE:+LLM-based+Divergent+Reasoning+and+Ensemble+for+Zero-Shot+Composed+Image+Retrieval)|0|
|[EditKG: Editing Knowledge Graph for Recommendation](https://doi.org/10.1145/3626772.3657723)|Gu Tang, Xiaoying Gan, Jinghe Wang, Bin Lu, Lyuwen Wu, Luoyi Fu, Chenghu Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EditKG:+Editing+Knowledge+Graph+for+Recommendation)|0|
|[GUITAR: Gradient Pruning toward Fast Neural Ranking](https://doi.org/10.1145/3626772.3657728)|Weijie Zhao, Shulong Tan, Ping Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GUITAR:+Gradient+Pruning+toward+Fast+Neural+Ranking)|0|
|[Revisiting Document Expansion and Filtering for Effective First-Stage Retrieval](https://doi.org/10.1145/3626772.3657850)|Watheq Mansour, Shengyao Zhuang, Guido Zuccon, Joel Mackenzie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Revisiting+Document+Expansion+and+Filtering+for+Effective+First-Stage+Retrieval)|0|
|[Simple but Effective Raw-Data Level Multimodal Fusion for Composed Image Retrieval](https://doi.org/10.1145/3626772.3657727)|Haokun Wen, Xuemeng Song, Xiaolin Chen, Yinwei Wei, Liqiang Nie, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Simple+but+Effective+Raw-Data+Level+Multimodal+Fusion+for+Composed+Image+Retrieval)|0|
|[Browsing and Searching Metadata of TREC](https://doi.org/10.1145/3626772.3657873)|Timo Breuer, Ellen M. Voorhees, Ian Soboroff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Browsing+and+Searching+Metadata+of+TREC)|0|
|[ACORDAR 2.0: A Test Collection for Ad Hoc Dataset Retrieval with Densely Pooled Datasets and Question-Style Queries](https://doi.org/10.1145/3626772.3657866)|Qiaosheng Chen, Weiqing Luo, Zixian Huang, Tengteng Lin, Xiaxia Wang, Ahmet Soylu, Basil Ell, Baifan Zhou, Evgeny Kharlamov, Gong Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACORDAR+2.0:+A+Test+Collection+for+Ad+Hoc+Dataset+Retrieval+with+Densely+Pooled+Datasets+and+Question-Style+Queries)|0|
|[Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling](https://doi.org/10.1145/3626772.3657767)|Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reinforcement+Learning-based+Recommender+Systems+with+Large+Language+Models+for+State+Reward+and+Action+Modeling)|0|
|[OpenP5: An Open-Source Platform for Developing, Training, and Evaluating LLM-based Recommender Systems](https://doi.org/10.1145/3626772.3657883)|Shuyuan Xu, Wenyue Hua, Yongfeng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenP5:+An+Open-Source+Platform+for+Developing,+Training,+and+Evaluating+LLM-based+Recommender+Systems)|0|
|[Fair Recommendations with Limited Sensitive Attributes: A Distributionally Robust Optimization Approach](https://doi.org/10.1145/3626772.3657822)|Tianhao Shi, Yang Zhang, Jizhi Zhang, Fuli Feng, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fair+Recommendations+with+Limited+Sensitive+Attributes:+A+Distributionally+Robust+Optimization+Approach)|0|
|[Large Language Models and Future of Information Retrieval: Opportunities and Challenges](https://doi.org/10.1145/3626772.3657848)|ChengXiang Zhai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+and+Future+of+Information+Retrieval:+Opportunities+and+Challenges)|0|
|[Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding](https://doi.org/10.1145/3626772.3657746)|Hansi Zeng, Chen Luo, Hamed Zamani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Planning+Ahead+in+Generative+Retrieval:+Guiding+Autoregressive+Generation+through+Simultaneous+Decoding)|0|
|[Course Recommender Systems Need to Consider the Job Market](https://doi.org/10.1145/3626772.3657847)|Jibril Frej, Anna Dai, Syrielle Montariol, Antoine Bosselut, Tanja Käser||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Course+Recommender+Systems+Need+to+Consider+the+Job+Market)|0|
|[Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients](https://doi.org/10.1145/3626772.3657785)|Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leave+No+Patient+Behind:+Enhancing+Medication+Recommendation+for+Rare+Disease+Patients)|0|
|[MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation](https://doi.org/10.1145/3626772.3657867)|Andreea Iana, Goran Glavas, Heiko Paulheim||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MIND+Your+Language:+A+Multilingual+Dataset+for+Cross-lingual+News+Recommendation)|0|
|[Steering Large Language Models for Cross-lingual Information Retrieval](https://doi.org/10.1145/3626772.3657819)|Ping Guo, Yubing Ren, Yue Hu, Yanan Cao, Yunpeng Li, Heyan Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Steering+Large+Language+Models+for+Cross-lingual+Information+Retrieval)|0|
|[DAC: Quantized Optimal Transport Reward-based Reinforcement Learning Approach to Detoxify Query Auto-Completion](https://doi.org/10.1145/3626772.3657779)|Aishwarya Maheswaran, Kaushal Kumar Maurya, Manish Gupta, Maunendra Sankar Desarkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DAC:+Quantized+Optimal+Transport+Reward-based+Reinforcement+Learning+Approach+to+Detoxify+Query+Auto-Completion)|0|
|[IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues](https://doi.org/10.1145/3626772.3657760)|Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IM-RAG:+Multi-Round+Retrieval-Augmented+Generation+Through+Learning+Inner+Monologues)|0|
|[Towards Human-centered Proactive Conversational Agents](https://doi.org/10.1145/3626772.3657843)|Yang Deng, Lizi Liao, Zhonghua Zheng, Grace Hui Yang, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Human-centered+Proactive+Conversational+Agents)|0|
|[TREC iKAT 2023: A Test Collection for Evaluating Conversational and Interactive Knowledge Assistants](https://doi.org/10.1145/3626772.3657860)|Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffrey Dalton, Leif Azzopardi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TREC+iKAT+2023:+A+Test+Collection+for+Evaluating+Conversational+and+Interactive+Knowledge+Assistants)|0|
|[UGNCL: Uncertainty-Guided Noisy Correspondence Learning for Efficient Cross-Modal Matching](https://doi.org/10.1145/3626772.3657806)|Quanxing Zha, Xin Liu, Yiuming Cheung, Xing Xu, Nannan Wang, Jianjia Cao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=UGNCL:+Uncertainty-Guided+Noisy+Correspondence+Learning+for+Efficient+Cross-Modal+Matching)|0|
|[DHMAE: A Disentangled Hypergraph Masked Autoencoder for Group Recommendation](https://doi.org/10.1145/3626772.3657699)|Yingqi Zhao, Haiwei Zhang, Qijie Bai, Changli Nie, Xiaojie Yuan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DHMAE:+A+Disentangled+Hypergraph+Masked+Autoencoder+for+Group+Recommendation)|0|
|[Are We Really Achieving Better Beyond-Accuracy Performance in Next Basket Recommendation?](https://doi.org/10.1145/3626772.3657835)|Ming Li, Yuanna Liu, Sami Jullien, Mozhdeh Ariannezhad, Andrew Yates, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+We+Really+Achieving+Better+Beyond-Accuracy+Performance+in+Next+Basket+Recommendation?)|0|
|[AutoDCS: Automated Decision Chain Selection in Deep Recommender Systems](https://doi.org/10.1145/3626772.3657818)|Dugang Liu, Shenxian Xian, Yuhao Wu, Chaohua Yang, Xing Tang, Xiuqiang He, Zhong Ming||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoDCS:+Automated+Decision+Chain+Selection+in+Deep+Recommender+Systems)|0|
|[EasyRL4Rec: An Easy-to-use Library for Reinforcement Learning Based Recommender Systems](https://doi.org/10.1145/3626772.3657868)|Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, Min Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EasyRL4Rec:+An+Easy-to-use+Library+for+Reinforcement+Learning+Based+Recommender+Systems)|0|
|[Explainability for Transparent Conversational Information-Seeking](https://doi.org/10.1145/3626772.3657768)|Weronika Lajewska, Damiano Spina, Johanne Trippas, Krisztian Balog||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explainability+for+Transparent+Conversational+Information-Seeking)|0|
|[Evaluating Search System Explainability with Psychometrics and Crowdsourcing](https://doi.org/10.1145/3626772.3657796)|Catherine Chen, Carsten Eickhoff||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Evaluating+Search+System+Explainability+with+Psychometrics+and+Crowdsourcing)|0|
|[Enhancing Dataset Search with Compact Data Snippets](https://doi.org/10.1145/3626772.3657837)|Qiaosheng Chen, Jiageng Chen, Xiao Zhou, Gong Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Dataset+Search+with+Compact+Data+Snippets)|0|
|[When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications](https://doi.org/10.1145/3626772.3657722)|Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, Yefeng Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=When+MOE+Meets+LLMs:+Parameter+Efficient+Fine-tuning+for+Multi-task+Medical+Applications)|0|
|[OEHR: An Orthopedic Electronic Health Record Dataset](https://doi.org/10.1145/3626772.3657885)|Yibo Xie, Kaifan Wang, Jiawei Zheng, Feiyan Liu, Xiaoli Wang, Guofeng Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OEHR:+An+Orthopedic+Electronic+Health+Record+Dataset)|0|
|[SIGformer: Sign-aware Graph Transformer for Recommendation](https://doi.org/10.1145/3626772.3657747)|Sirui Chen, Jiawei Chen, Sheng Zhou, Bohao Wang, Shen Han, Chanfei Su, Yuqing Yuan, Can Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SIGformer:+Sign-aware+Graph+Transformer+for+Recommendation)|0|
|[Scaling Laws For Dense Retrieval](https://doi.org/10.1145/3626772.3657743)|Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Laws+For+Dense+Retrieval)|0|
|[Diffusion Models for Generative Outfit Recommendation](https://doi.org/10.1145/3626772.3657719)|Yiyan Xu, Wenjie Wang, Fuli Feng, Yunshan Ma, Jizhi Zhang, Xiangnan He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diffusion+Models+for+Generative+Outfit+Recommendation)|0|
|[Collaborative Filtering Based on Diffusion Models: Unveiling the Potential of High-Order Connectivity](https://doi.org/10.1145/3626772.3657742)|Yu Hou, JinDuk Park, WonYong Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaborative+Filtering+Based+on+Diffusion+Models:+Unveiling+the+Potential+of+High-Order+Connectivity)|0|
|[Graph Signal Diffusion Model for Collaborative Filtering](https://doi.org/10.1145/3626772.3657759)|Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Signal+Diffusion+Model+for+Collaborative+Filtering)|0|
|[Multi-granular Adversarial Attacks against Black-box Neural Ranking Models](https://doi.org/10.1145/3626772.3657704)|YuAn Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-granular+Adversarial+Attacks+against+Black-box+Neural+Ranking+Models)|0|
|[Optimal Transport Enhanced Cross-City Site Recommendation](https://doi.org/10.1145/3626772.3657757)|Xinhang Li, Xiangyu Zhao, Zihao Wang, Yang Duan, Yong Zhang, Chunxiao Xing||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Optimal+Transport+Enhanced+Cross-City+Site+Recommendation)|0|
|[Disentangled Contrastive Hypergraph Learning for Next POI Recommendation](https://doi.org/10.1145/3626772.3657726)|Yantong Lai, Yijun Su, Lingwei Wei, Tianqi He, Haitao Wang, Gaode Chen, Daren Zha, Qiang Liu, Xingxing Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Disentangled+Contrastive+Hypergraph+Learning+for+Next+POI+Recommendation)|0|
|[CLLP: Contrastive Learning Framework Based on Latent Preferences for Next POI Recommendation](https://doi.org/10.1145/3626772.3657730)|Hongli Zhou, Zhihao Jia, Haiyang Zhu, Zhizheng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLLP:+Contrastive+Learning+Framework+Based+on+Latent+Preferences+for+Next+POI+Recommendation)|0|
|[OpenSiteRec: An Open Dataset for Site Recommendation](https://doi.org/10.1145/3626772.3657875)|Xinhang Li, Xiangyu Zhao, Yejing Wang, Yu Liu, Chong Chen, Cheng Long, Yong Zhang, Chunxiao Xing||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=OpenSiteRec:+An+Open+Dataset+for+Site+Recommendation)|0|
|[Fairness-Aware Exposure Allocation via Adaptive Reranking](https://doi.org/10.1145/3626772.3657794)|Thomas Jänich, Graham McDonald, Iadh Ounis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fairness-Aware+Exposure+Allocation+via+Adaptive+Reranking)|0|
|[A Taxation Perspective for Fair Re-ranking](https://doi.org/10.1145/3626772.3657766)|Chen Xu, Xiaopeng Ye, Wenjie Wang, Liang Pang, Jun Xu, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Taxation+Perspective+for+Fair+Re-ranking)|0|
|[A Dual-Embedding Based DQN for Worker Recruitment in Spatial Crowdsourcing with Social Network](https://doi.org/10.1145/3626772.3657718)|Yucen Gao, Wei Liu, Jianxiong Guo, Xiaofeng Gao, Guihai Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Dual-Embedding+Based+DQN+for+Worker+Recruitment+in+Spatial+Crowdsourcing+with+Social+Network)|0|
|[Efficient Community Search Based on Relaxed k-Truss Index](https://doi.org/10.1145/3626772.3657708)|Xiaoqin Xie, Shuangyuan Liu, Jiaqi Zhang, Shuai Han, Wei Wang, Wu Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient+Community+Search+Based+on+Relaxed+k-Truss+Index)|0|
|[Untargeted Adversarial Attack on Knowledge Graph Embeddings](https://doi.org/10.1145/3626772.3657702)|Tianzhe Zhao, Jiaoyan Chen, Yanchi Ru, Qika Lin, Yuxia Geng, Jun Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Untargeted+Adversarial+Attack+on+Knowledge+Graph+Embeddings)|0|
|[Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval](https://doi.org/10.1145/3626772.3657792)|Guangyuan Ma, Xing Wu, Zijia Lin, Songlin Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Drop+your+Decoder:+Pre-training+with+Bag-of-Word+Prediction+for+Dense+Passage+Retrieval)|0|
|[M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval](https://doi.org/10.1145/3626772.3657833)|Xingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng Yu, Ming Yang, Qingpei Guo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=M2-RAAP:+A+Multi-Modal+Recipe+for+Advancing+Adaptation-based+Pre-training+towards+Effective+and+Efficient+Zero-shot+Video-text+Retrieval)|0|
|[CaLa: Complementary Association Learning for Augmenting Comoposed Image Retrieval](https://doi.org/10.1145/3626772.3657823)|Xintong Jiang, Yaxiong Wang, Mengjian Li, Yujiao Wu, Bingwen Hu, Xueming Qian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaLa:+Complementary+Association+Learning+for+Augmenting+Comoposed+Image+Retrieval)|0|
|[CFIR:  Fast and Effective Long-Text To Image Retrieval for Large Corpora](https://doi.org/10.1145/3626772.3657741)|Zijun Long, Xuri Ge, Richard McCreadie, Joemon M. Jose||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CFIR:++Fast+and+Effective+Long-Text+To+Image+Retrieval+for+Large+Corpora)|0|
|[CaseLink: Inductive Graph Learning for Legal Case Retrieval](https://doi.org/10.1145/3626772.3657693)|Yanran Tang, Ruihong Qiu, Hongzhi Yin, Xue Li, Zi Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CaseLink:+Inductive+Graph+Learning+for+Legal+Case+Retrieval)|0|
|[Explicitly Integrating Judgment Prediction with Legal Document Retrieval: A Law-Guided Generative Approach](https://doi.org/10.1145/3626772.3657717)|Weicong Qin, Zelin Cao, Weijie Yu, Zihua Si, Sirui Chen, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Explicitly+Integrating+Judgment+Prediction+with+Legal+Document+Retrieval:+A+Law-Guided+Generative+Approach)|0|
|[A Persona-Infused Cross-Task Graph Network for Multimodal Emotion Recognition with Emotion Shift Detection in Conversations](https://doi.org/10.1145/3626772.3657944)|Geng Tu, Feng Xiong, Bin Liang, Ruifeng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Persona-Infused+Cross-Task+Graph+Network+for+Multimodal+Emotion+Recognition+with+Emotion+Shift+Detection+in+Conversations)|0|
|[Analyzing and Mitigating Repetitions in Trip Recommendation](https://doi.org/10.1145/3626772.3657970)|Wenzheng Shu, Kangqi Xu, Wenxin Tai, Ting Zhong, Yong Wang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+and+Mitigating+Repetitions+in+Trip+Recommendation)|0|
|[Cluster-based Partial Dense Retrieval Fused with Sparse Text Retrieval](https://doi.org/10.1145/3626772.3657972)|Yingrui Yang, Parker Carlson, Shanxiu He, Yifan Qiao, Tao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cluster-based+Partial+Dense+Retrieval+Fused+with+Sparse+Text+Retrieval)|0|
|[Contextualization with SPLADE for High Recall Retrieval](https://doi.org/10.1145/3626772.3657919)|Eugene Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextualization+with+SPLADE+for+High+Recall+Retrieval)|0|
|[Convex Feature Embedding for Face and Voice Association](https://doi.org/10.1145/3626772.3657975)|Jiwoo Kang, Taewan Kim, YoungHo Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Convex+Feature+Embedding+for+Face+and+Voice+Association)|0|
|[Enhancing Criminal Case Matching through Diverse Legal Factors](https://doi.org/10.1145/3626772.3657960)|Jie Zhao, Ziyu Guan, Wei Zhao, Yue Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Criminal+Case+Matching+through+Diverse+Legal+Factors)|0|
|[Faster Learned Sparse Retrieval with Block-Max Pruning](https://doi.org/10.1145/3626772.3657906)|Antonio Mallia, Torsten Suel, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Faster+Learned+Sparse+Retrieval+with+Block-Max+Pruning)|0|
|[Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://doi.org/10.1145/3626772.3657951)|Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fine-Tuning+LLaMA+for+Multi-Stage+Text+Retrieval)|0|
|[Graph Diffusive Self-Supervised Learning for Social Recommendation](https://doi.org/10.1145/3626772.3657962)|Jiuqiang Li, Hongjun Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Diffusive+Self-Supervised+Learning+for+Social+Recommendation)|0|
|[Improving In-Context Learning via Sequentially Selection and Preference Alignment for Few-Shot Aspect-Based Sentiment Analysis](https://doi.org/10.1145/3626772.3657932)|Qianlong Wang, Keyang Ding, Xuan Luo, Ruifeng Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+In-Context+Learning+via+Sequentially+Selection+and+Preference+Alignment+for+Few-Shot+Aspect-Based+Sentiment+Analysis)|0|
|[Language Fairness in Multilingual Information Retrieval](https://doi.org/10.1145/3626772.3657943)|Eugene Yang, Thomas Jänich, James Mayfield, Dawn J. Lawrie||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Language+Fairness+in+Multilingual+Information+Retrieval)|0|
|[Large Language Models Based Stemming for Information Retrieval: Promises, Pitfalls and Failures](https://doi.org/10.1145/3626772.3657949)|Shuai Wang, Shengyao Zhuang, Guido Zuccon||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+Based+Stemming+for+Information+Retrieval:+Promises,+Pitfalls+and+Failures)|0|
|[MACA: Memory-aided Coarse-to-fine Alignment for Text-based Person Search](https://doi.org/10.1145/3626772.3657915)|Liangxu Su, Rong Quan, Zhiyuan Qi, Jie Qin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MACA:+Memory-aided+Coarse-to-fine+Alignment+for+Text-based+Person+Search)|0|
|[Negative as Positive: Enhancing Out-of-distribution Generalization for Graph Contrastive Learning](https://doi.org/10.1145/3626772.3657927)|Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Negative+as+Positive:+Enhancing+Out-of-distribution+Generalization+for+Graph+Contrastive+Learning)|0|
|[On Backbones and Training Regimes for Dense Retrieval in African Languages](https://doi.org/10.1145/3626772.3657952)|Akintunde Oladipo, Mofetoluwa Adeyemi, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+Backbones+and+Training+Regimes+for+Dense+Retrieval+in+African+Languages)|0|
|[Predicting Micro-video Popularity via Multi-modal Retrieval Augmentation](https://doi.org/10.1145/3626772.3657929)|Ting Zhong, Jian Lang, Yifan Zhang, Zhangtao Cheng, Kunpeng Zhang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Predicting+Micro-video+Popularity+via+Multi-modal+Retrieval+Augmentation)|0|
|[Searching for Physical Documents in Archival Repositories](https://doi.org/10.1145/3626772.3657896)|Tokinori Suzuki, Douglas W. Oard, Emi Ishita, Yoichi Tomiura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Searching+for+Physical+Documents+in+Archival+Repositories)|0|
|[Self-Explainable Next POI Recommendation](https://doi.org/10.1145/3626772.3657967)|Kai Yang, Yi Yang, Qiang Gao, Ting Zhong, Yong Wang, Fan Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Explainable+Next+POI+Recommendation)|0|
|[Synthetic Test Collections for Retrieval Evaluation](https://doi.org/10.1145/3626772.3657942)|Hossein A. Rahmani, Nick Craswell, Emine Yilmaz, Bhaskar Mitra, Daniel Campos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthetic+Test+Collections+for+Retrieval+Evaluation)|0|
|[SPLATE: Sparse Late Interaction Retrieval](https://doi.org/10.1145/3626772.3657968)|Thibault Formal, Stéphane Clinchant, Hervé Déjean, Carlos Lassance||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SPLATE:+Sparse+Late+Interaction+Retrieval)|0|
|[The Surprising Effectiveness of Rankers trained on Expanded Queries](https://doi.org/10.1145/3626772.3657938)|Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Surprising+Effectiveness+of+Rankers+trained+on+Expanded+Queries)|0|
|[Turbo-CF: Matrix Decomposition-Free Graph Filtering for Fast Recommendation](https://doi.org/10.1145/3626772.3657916)|JinDuk Park, YongMin Shin, WonYong Shin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Turbo-CF:+Matrix+Decomposition-Free+Graph+Filtering+for+Fast+Recommendation)|0|
|[Unifying Graph Retrieval and Prompt Tuning for Graph-Grounded Text Classification](https://doi.org/10.1145/3626772.3657934)|Le Dai, Yu Yin, Enhong Chen, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unifying+Graph+Retrieval+and+Prompt+Tuning+for+Graph-Grounded+Text+Classification)|0|
|[Weighted KL-Divergence for Document Ranking Model Refinement](https://doi.org/10.1145/3626772.3657946)|Yingrui Yang, Yifan Qiao, Shanxiu He, Tao Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Weighted+KL-Divergence+for+Document+Ranking+Model+Refinement)|0|
|[Using Large Language Models for Math Information Retrieval](https://doi.org/10.1145/3626772.3657907)|Behrooz Mansouri, Reihaneh Maarefdoust||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Large+Language+Models+for+Math+Information+Retrieval)|0|
|[A Question-Answering Assistant over Personal Knowledge Graph](https://doi.org/10.1145/3626772.3657665)|Lingyuan Liu, Huifang Du, Xiaolian Zhang, Mengying Guo, Haofen Wang, Meng Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Question-Answering+Assistant+over+Personal+Knowledge+Graph)|0|
|[ConvLogRecaller: Real-Time Conversational Lifelog Recaller](https://doi.org/10.1145/3626772.3657659)|YuanChi Lee, AnZi Yen, HenHsen Huang, HsinHsi Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ConvLogRecaller:+Real-Time+Conversational+Lifelog+Recaller)|0|
|[CLIP-Branches:  Interactive Fine-Tuning for Text-Image Retrieval](https://doi.org/10.1145/3626772.3657678)|Christian Lülf, Denis Mayr Lima Martins, Marcos Antonio Vaz Salles, Yongluan Zhou, Fabian Gieseke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CLIP-Branches:++Interactive+Fine-Tuning+for+Text-Image+Retrieval)|0|
|[Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation](https://doi.org/10.1145/3626772.3657673)|Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Img2Loc:+Revisiting+Image+Geolocalization+using+Multi-modality+Foundation+Models+and+Image-based+Retrieval-Augmented+Generation)|0|
|[JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs](https://doi.org/10.1145/3626772.3657677)|Wanying Ding, Manoj Cherukumalli, Santosh Chikoti, Vinay K. Chaudhri||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=JPEC:+A+Novel+Graph+Neural+Network+for+Competitor+Retrieval+in+Financial+Knowledge+Graphs)|0|
|[MACRec: A Multi-Agent Collaboration Framework for Recommendation](https://doi.org/10.1145/3626772.3657669)|Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MACRec:+A+Multi-Agent+Collaboration+Framework+for+Recommendation)|0|
|[ModelGalaxy: A Versatile Model Retrieval Platform](https://doi.org/10.1145/3626772.3657676)|Wenling Zhang, Yixiao Li, Zhaotian Li, Hailong Sun, Xiang Gao, Xudong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ModelGalaxy:+A+Versatile+Model+Retrieval+Platform)|0|
|[RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation](https://doi.org/10.1145/3626772.3657660)|Viju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, Roman Teucher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RAG-Ex:+A+Generic+Framework+for+Explaining+Retrieval+Augmented+Generation)|0|
|[ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement](https://doi.org/10.1145/3626772.3657680)|Saurabh Bhausaheb Zinjad, Amrita Bhattacharjee, Amey Bhilegaonkar, Huan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ResumeFlow:+An+LLM-facilitated+Pipeline+for+Personalized+Resume+Generation+and+Refinement)|0|
|[ScholarNodes: Applying Content-based Filtering to Recommend Interdisciplinary Communities within Scholarly Social Networks](https://doi.org/10.1145/3626772.3657668)|Md Asaduzzaman Noor, Jason A. Clark, John W. Sheppard||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ScholarNodes:+Applying+Content-based+Filtering+to+Recommend+Interdisciplinary+Communities+within+Scholarly+Social+Networks)|0|
|[Synthetic Query Generation using Large Language Models for Virtual Assistants](https://doi.org/10.1145/3626772.3661355)|Sonal Sannigrahi, Thiago FragaSilva, Youssef Oualil, Christophe Van Gysel||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Synthetic+Query+Generation+using+Large+Language+Models+for+Virtual+Assistants)|0|
|[A Study on Unsupervised Question and Answer Generation for Legal Information Retrieval and Precedents Understanding](https://doi.org/10.1145/3626772.3661354)|Johny Moreira, Altigran S. da Silva, Edleno Silva de Moura, Leandro Bezerra Marinho||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Study+on+Unsupervised+Question+and+Answer+Generation+for+Legal+Information+Retrieval+and+Precedents+Understanding)|0|
|[Reflections on the Coding Ability of LLMs for Analyzing Market Research Surveys](https://doi.org/10.1145/3626772.3661362)|Shi Zong, Santosh Kolagati, Amit Chaudhary, Josh Seltzer, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Reflections+on+the+Coding+Ability+of+LLMs+for+Analyzing+Market+Research+Surveys)|0|
|[Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering](https://doi.org/10.1145/3626772.3661370)|Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, Zheng Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Retrieval-Augmented+Generation+with+Knowledge+Graphs+for+Customer+Service+Question+Answering)|0|
|[Striking the Right Chord: A Comprehensive Approach to Amazon Music Search Spell Correction](https://doi.org/10.1145/3626772.3661344)|Siddharth Sharma, Shiyun Yang, Ajinkya Walimbe, Tarun Sharma, Joaquin Delgado||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Striking+the+Right+Chord:+A+Comprehensive+Approach+to+Amazon+Music+Search+Spell+Correction)|0|
|[SLH-BIA: Short-Long Hawkes Process for Buy It Again Recommendations at Scale](https://doi.org/10.1145/3626772.3661374)|Rankyung Park, Amit Pande, David Relyea, Pushkar Chennu, Prathyusha Kanmanth Reddy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SLH-BIA:+Short-Long+Hawkes+Process+for+Buy+It+Again+Recommendations+at+Scale)|0|
|[Are Embeddings Enough? SIRIP Panel on the Future of Embeddings in Industry IR Systems](https://doi.org/10.1145/3626772.3661360)|Jon Degenhardt, Tracy Holloway King||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Are+Embeddings+Enough?+SIRIP+Panel+on+the+Future+of+Embeddings+in+Industry+IR+Systems)|0|
|[Large Language Model Powered Agents for Information Retrieval](https://doi.org/10.1145/3626772.3661375)|An Zhang, Yang Deng, Yankai Lin, Xu Chen, JiRong Wen, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Model+Powered+Agents+for+Information+Retrieval)|0|
|[High Recall Retrieval Via Technology-Assisted Review](https://doi.org/10.1145/3626772.3661376)|Lenora Gray, David D. Lewis, Jeremy Pickens, Eugene Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=High+Recall+Retrieval+Via+Technology-Assisted+Review)|0|
|[Large Language Models for Recommendation: Past, Present, and Future](https://doi.org/10.1145/3626772.3661383)|Keqin Bao, Jizhi Zhang, Xinyu Lin, Yang Zhang, Wenjie Wang, Fuli Feng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Recommendation:+Past,+Present,+and+Future)|0|
|[Recent Advances in Generative Information Retrieval](https://doi.org/10.1145/3626772.3661379)|Yubao Tang, Ruqing Zhang, Zhaochun Ren, Jiafeng Guo, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Generative+Information+Retrieval)|0|
|[Robust Information Retrieval](https://doi.org/10.1145/3626772.3661380)|YuAn Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Information+Retrieval)|0|
|[IR-RAG @ SIGIR24: Information Retrieval's Role in RAG Systems](https://doi.org/10.1145/3626772.3657984)|Fabio Petroni, Federico Siciliano, Fabrizio Silvestri, Giovanni Trappolini||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IR-RAG+@+SIGIR24:+Information+Retrieval's+Role+in+RAG+Systems)|0|
|[A Predictive Framework for Query Reformulation](https://doi.org/10.1145/3626772.3657653)|Reyhaneh Goli||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Predictive+Framework+for+Query+Reformulation)|0|
|[Multimodal Representation and Retrieval [MRR 2024]](https://doi.org/10.1145/3626772.3657987)|Xinliang Zhu, Arnab Dhua, Douglas Gray, I. Zeki Yalniz, Tan Yu, Mohamed Elhoseiny, Bryan Plummer||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multimodal+Representation+and+Retrieval+[MRR+2024])|0|
|[Axiomatic Guidance for Efficient and Controlled Neural Search](https://doi.org/10.1145/3626772.3657651)|Andrew Parry||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Axiomatic+Guidance+for+Efficient+and+Controlled+Neural+Search)|0|
|[Personalized Large Language Models through Parameter Efficient Fine-Tuning Techniques](https://doi.org/10.1145/3626772.3657657)|Marco Braga||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Personalized+Large+Language+Models+through+Parameter+Efficient+Fine-Tuning+Techniques)|0|
|[Towards a Framework for Legal Case Retrieval](https://doi.org/10.1145/3626772.3657650)|Tebo LeburuDingalo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+a+Framework+for+Legal+Case+Retrieval)|0|
|[Rethinking the Evaluation of Dialogue Systems: Effects of User Feedback on Crowdworkers and LLMs](https://doi.org/10.1145/3626772.3657712)|Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+the+Evaluation+of+Dialogue+Systems:+Effects+of+User+Feedback+on+Crowdworkers+and+LLMs)|0|
|[General-Purpose User Modeling with Behavioral Logs: A Snapchat Case Study](https://doi.org/10.1145/3626772.3657908)|Qixiang Fang, Zhihan Zhou, Francesco Barbieri, Yozen Liu, Leonardo Neves, Dong Nguyen, Daniel L. Oberski, Maarten W. Bos, Ron Dotsch||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=General-Purpose+User+Modeling+with+Behavioral+Logs:+A+Snapchat+Case+Study)|0|
|[Neural Passage Quality Estimation for Static Pruning](https://doi.org/10.1145/3626772.3657765)|Xuejun Chang, Debabrata Mishra, Craig Macdonald, Sean MacAvaney||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Passage+Quality+Estimation+for+Static+Pruning)|0|
|[COMI: COrrect and MItigate Shortcut Learning Behavior in Deep Neural Networks](https://doi.org/10.1145/3626772.3657729)|Lili Zhao, Qi Liu, Linan Yue, Wei Chen, Liyi Chen, Ruijun Sun, Chao Song||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COMI:+COrrect+and+MItigate+Shortcut+Learning+Behavior+in+Deep+Neural+Networks)|0|
|[LLM-enhanced Cascaded Multi-level Learning on Temporal Heterogeneous Graphs](https://doi.org/10.1145/3626772.3657731)|Fengyi Wang, Guanghui Zhu, Chunfeng Yuan, Yihua Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM-enhanced+Cascaded+Multi-level+Learning+on+Temporal+Heterogeneous+Graphs)|0|
|[Self-Improving Teacher Cultivates Better Student: Distillation Calibration for Multimodal Large Language Models](https://doi.org/10.1145/3626772.3657692)|Xinwei Li, Li Lin, Shuai Wang, Chen Qian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Improving+Teacher+Cultivates+Better+Student:+Distillation+Calibration+for+Multimodal+Large+Language+Models)|0|
|[Deep Automated Mechanism Design for Integrating Ad Auction and Allocation in Feed](https://doi.org/10.1145/3626772.3657774)|Xuejian Li, Ze Wang, Bingqi Zhu, Fei He, Yongkang Wang, Xingxing Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Automated+Mechanism+Design+for+Integrating+Ad+Auction+and+Allocation+in+Feed)|0|
|[TGOnline: Enhancing Temporal Graph Learning with Adaptive Online Meta-Learning](https://doi.org/10.1145/3626772.3657791)|Ruijie Wang, Jingyuan Huang, Yutong Zhang, Jinyang Li, Yufeng Wang, Wanyu Zhao, Shengzhong Liu, Charith Mendis, Tarek F. Abdelzaher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TGOnline:+Enhancing+Temporal+Graph+Learning+with+Adaptive+Online+Meta-Learning)|0|
|[Intent Distribution based Bipartite Graph Representation Learning](https://doi.org/10.1145/3626772.3657739)|Haojie Li, Wei Wei, Guanfeng Liu, Jinhuan Liu, Feng Jiang, Junwei Du||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intent+Distribution+based+Bipartite+Graph+Representation+Learning)|0|
|[MTMS: Multi-teacher Multi-stage Knowledge Distillation for Reasoning-Based Machine Reading Comprehension](https://doi.org/10.1145/3626772.3657824)|Zhuo Zhao, Zhiwen Xie, Guangyou Zhou, Jimmy Xiangji Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MTMS:+Multi-teacher+Multi-stage+Knowledge+Distillation+for+Reasoning-Based+Machine+Reading+Comprehension)|0|
|[Exploring the Trade-Off within Visual Information for MultiModal Sentence Summarization](https://doi.org/10.1145/3626772.3657753)|Minghuan Yuan, Shiyao Cui, Xinghua Zhang, Shicheng Wang, Hongbo Xu, Tingwen Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+the+Trade-Off+within+Visual+Information+for+MultiModal+Sentence+Summarization)|0|
|[ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages](https://doi.org/10.1145/3626772.3657891)|Bhawna Piryani, Jamshid Mozafari, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ChroniclingAmericaQA:+A+Large-scale+Question+Answering+Dataset+based+on+Historical+American+Newspaper+Pages)|0|
|[BRB-KMeans: Enhancing Binary Data Clustering for Binary Product Quantization](https://doi.org/10.1145/3626772.3657898)|Suwon Lee, SangMin Choi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=BRB-KMeans:+Enhancing+Binary+Data+Clustering+for+Binary+Product+Quantization)|0|
|[Distance Sampling-based Paraphraser Leveraging ChatGPT for Text Data Manipulation](https://doi.org/10.1145/3626772.3657976)|Yoori Oh, Yoseob Han, Kyogu Lee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distance+Sampling-based+Paraphraser+Leveraging+ChatGPT+for+Text+Data+Manipulation)|0|
|[Fake News Detection via Multi-scale Semantic Alignment and Cross-modal Attention](https://doi.org/10.1145/3626772.3657905)|Jiandong Wang, Hongguang Zhang, Chun Liu, Xiongjun Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fake+News+Detection+via+Multi-scale+Semantic+Alignment+and+Cross-modal+Attention)|0|
|[Label Hierarchical Structure-Aware Multi-Label Few-Shot Intent Detection via Prompt Tuning](https://doi.org/10.1145/3626772.3657947)|Xiaotong Zhang, Xinyi Li, Han Liu, Xinyue Liu, Xianchao Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Label+Hierarchical+Structure-Aware+Multi-Label+Few-Shot+Intent+Detection+via+Prompt+Tuning)|0|
|[MKV: Mapping Key Semantics into Vectors for Rumor Detection](https://doi.org/10.1145/3626772.3657937)|Yang Li, Liguang Liu, Jiacai Guo, LapKei Lee, Fu Lee Wang, Zhenguo Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MKV:+Mapping+Key+Semantics+into+Vectors+for+Rumor+Detection)|0|
|[PAG-LLM: Paraphrase and Aggregate with Large Language Models for Minimizing Intent Classification Errors](https://doi.org/10.1145/3626772.3657959)|Vikas Yadav, Zheng Tang, Vijay Srinivasan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PAG-LLM:+Paraphrase+and+Aggregate+with+Large+Language+Models+for+Minimizing+Intent+Classification+Errors)|0|
|[Self-Referential Review: Exploring the Impact of Self-Reference Effect in Review](https://doi.org/10.1145/3626772.3657969)|Kyusik Kim, Hyungwoo Song, Bongwon Suh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Self-Referential+Review:+Exploring+the+Impact+of+Self-Reference+Effect+in+Review)|0|
|[Unbiased Validation of Technology-Assisted Review for eDiscovery](https://doi.org/10.1145/3626772.3657903)|Gordon V. Cormack, Maura R. Grossman, Andrew Harbison, Tom O'Halloran, Bronagh McManus||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Validation+of+Technology-Assisted+Review+for+eDiscovery)|0|
|[Homogeneous-listing-augmented Self-supervised Multimodal Product Title Refinement](https://doi.org/10.1145/3626772.3661347)|Jiaqi Deng, Kaize Shi, Huan Huo, Dingxian Wang, Guandong Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Homogeneous-listing-augmented+Self-supervised+Multimodal+Product+Title+Refinement)|0|
|[GATS: Generative Audience Targeting System for Online Advertising](https://doi.org/10.1145/3626772.3661372)|Cong Jiang, Zhongde Chen, Bo Zhang, Yankun Ren, Xin Dong, Lei Cheng, Xinxing Yang, Longfei Li, Jun Zhou, Linjian Mo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GATS:+Generative+Audience+Targeting+System+for+Online+Advertising)|0|
|[ScienceDirect Topic Pages: A Knowledge Base of Scientific Concepts Across Various Science Domains](https://doi.org/10.1145/3626772.3661353)|Artemis Çapari, Hosein Azarbonyad, Georgios Tsatsaronis, Zubair Afzal, Judson Dunham||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ScienceDirect+Topic+Pages:+A+Knowledge+Base+of+Scientific+Concepts+Across+Various+Science+Domains)|0|
|[GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration](https://doi.org/10.1145/3626772.3657655)|Ben Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GOLF:+Goal-Oriented+Long-term+liFe+tasks+supported+by+human-AI+collaboration)|0|
|[CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks](https://doi.org/10.1145/3626772.3657778)|Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CorpusLM:+Towards+a+Unified+Language+Model+on+Corpus+for+Knowledge-Intensive+Tasks)|0|
|[Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph](https://doi.org/10.1145/3626772.3657706)|Zhiyu Fang, ShuaiLong Lei, Xiaobin Zhu, Chun Yang, ShiXue Zhang, XuCheng Yin, Jingyan Qin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Transformer-based+Reasoning+for+Learning+Evolutionary+Chain+of+Events+on+Temporal+Knowledge+Graph)|0|
|[NativE: Multi-modal Knowledge Graph Completion in the Wild](https://doi.org/10.1145/3626772.3657800)|Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, Huajun Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NativE:+Multi-modal+Knowledge+Graph+Completion+in+the+Wild)|0|
|[MetaHKG: Meta Hyperbolic Learning for Few-shot Temporal Reasoning](https://doi.org/10.1145/3626772.3657711)|Ruijie Wang, Yutong Zhang, Jinyang Li, Shengzhong Liu, Dachun Sun, Tianchen Wang, Tianshi Wang, Yizhuo Chen, Denizhan Kara, Tarek F. Abdelzaher||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MetaHKG:+Meta+Hyperbolic+Learning+for+Few-shot+Temporal+Reasoning)|0|
|[YAGO 4.5: A Large and Clean Knowledge Base with a Rich Taxonomy](https://doi.org/10.1145/3626772.3657876)|Fabian M. Suchanek, Mehwish Alam, Thomas Bonald, Lihu Chen, PierreHenri Paris, Jules Soria||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=YAGO+4.5:+A+Large+and+Clean+Knowledge+Base+with+a+Rich+Taxonomy)|0|
|[Uncontextualized significance considered dangerous](https://doi.org/10.1145/3626772.3657827)|Nicola Ferro, Mark Sanderson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Uncontextualized+significance+considered+dangerous)|0|
|[CIRAL: A Test Collection for CLIR Evaluations in African Languages](https://doi.org/10.1145/3626772.3657884)|Mofetoluwa Adeyemi, Akintunde Oladipo, Xinyu Zhang, David AlfonsoHermelo, Mehdi Rezagholizadeh, Boxing Chen, AbdulHakeem Omotayo, Idris Abdulmumin, Naome A. Etori, Toyib Babatunde Musa, Samuel Fanijo, Oluwabusayo Olufunke Awoyomi, Saheed Abdullahi Salahudeen, Labaran Adamu Mohammed, Daud Olamide Abolade, Falalu Ibrahim Lawan, Maryam Sabo Abubakar, Ruqayya Nasir Iro, Amina Abubakar Imam, Shafie Abdi Mohamed, Hanad Mohamud Mohamed, Tunde Oluwaseyi Ajayi, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CIRAL:+A+Test+Collection+for+CLIR+Evaluations+in+African+Languages)|0|
|[IDGenRec: LLM-RecSys Alignment with Textual ID Learning](https://doi.org/10.1145/3626772.3657821)|Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDGenRec:+LLM-RecSys+Alignment+with+Textual+ID+Learning)|0|
|[Enhanced Packed Marker with Entity Information for Aspect Sentiment Triplet Extraction](https://doi.org/10.1145/3626772.3657734)|You Li, Xupeng Zeng, Yixiao Zeng, Yuming Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhanced+Packed+Marker+with+Entity+Information+for+Aspect+Sentiment+Triplet+Extraction)|0|
|[Exogenous and Endogenous Data Augmentation for Low-Resource Complex Named Entity Recognition](https://doi.org/10.1145/3626772.3657754)|Xinghua Zhang, Gaode Chen, Shiyao Cui, Jiawei Sheng, Tingwen Liu, Hongbo Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exogenous+and+Endogenous+Data+Augmentation+for+Low-Resource+Complex+Named+Entity+Recognition)|0|
|[ACE-2005-PT: Corpus for Event Extraction in Portuguese](https://doi.org/10.1145/3626772.3657872)|Luís Filipe Cunha, Purificação Silvano, Ricardo Campos, Alípio Jorge||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ACE-2005-PT:+Corpus+for+Event+Extraction+in+Portuguese)|0|
|[Universal Adversarial Perturbations for Vision-Language Pre-trained Models](https://doi.org/10.1145/3626772.3657781)|PengFei Zhang, Zi Huang, Guangdong Bai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Universal+Adversarial+Perturbations+for+Vision-Language+Pre-trained+Models)|0|
|[Adaptive In-Context Learning with Large Language Models for Bundle Generation](https://doi.org/10.1145/3626772.3657808)|Zhu Sun, Kaidong Feng, Jie Yang, Xinghua Qu, Hui Fang, YewSoon Ong, Wenyuan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Adaptive+In-Context+Learning+with+Large+Language+Models+for+Bundle+Generation)|0|
|[Beyond Accuracy: Investigating Error Types in GPT-4 Responses to USMLE Questions](https://doi.org/10.1145/3626772.3657882)|Soumyadeep Roy, Aparup Khatua, Fatemeh Ghoochani, Uwe Hadler, Wolfgang Nejdl, Niloy Ganguly||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Beyond+Accuracy:+Investigating+Error+Types+in+GPT-4+Responses+to+USMLE+Questions)|0|
|[SuicidEmoji: Derived Emoji Dataset and Tasks for Suicide-Related Social Content](https://doi.org/10.1145/3626772.3657852)|Tianlin Zhang, Kailai Yang, Shaoxiong Ji, Boyang Liu, Qianqian Xie, Sophia Ananiadou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SuicidEmoji:+Derived+Emoji+Dataset+and+Tasks+for+Suicide-Related+Social+Content)|0|
|[LADy 💃: A Benchmark Toolkit for Latent Aspect Detection Enriched with Backtranslation Augmentation](https://doi.org/10.1145/3626772.3657894)|Farinam Hemmatizadeh, Christine Wong, Alice Yu, Hossein Fani||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LADy+💃:+A+Benchmark+Toolkit+for+Latent+Aspect+Detection+Enriched+with+Backtranslation+Augmentation)|0|
|[A Reproducibility Study of PLAID](https://doi.org/10.1145/3626772.3657856)|Sean MacAvaney, Nicola Tonellotto||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Reproducibility+Study+of+PLAID)|0|
|[Bootstrap Deep Metric for Seed Expansion in Attributed Networks](https://doi.org/10.1145/3626772.3657687)|Chunquan Liang, Yifan Wang, Qiankun Chen, Xinyuan Feng, Luyue Wang, Mei Li, Hongming Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bootstrap+Deep+Metric+for+Seed+Expansion+in+Attributed+Networks)|0|
|[Improving the Accuracy of Locally Differentially Private Community Detection by Order-consistent Data Perturbation](https://doi.org/10.1145/3626772.3657836)|Taolin Guo, Shunshun Peng, Zhejian Zhang, Mengmeng Yang, KwokYan Lam||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Improving+the+Accuracy+of+Locally+Differentially+Private+Community+Detection+by+Order-consistent+Data+Perturbation)|0|
|[CIQA: A Coding Inspired Question Answering Model](https://doi.org/10.1145/3626772.3657830)|Mousa Arraf, Kira Radinsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CIQA:+A+Coding+Inspired+Question+Answering+Model)|0|
|[Let Me Show You Step by Step: An Interpretable Graph Routing Network for Knowledge-based Visual Question Answering](https://doi.org/10.1145/3626772.3657790)|Duokang Wang, Linmei Hu, Rui Hao, Yingxia Shao, Xin Lv, Liqiang Nie, Juanzi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Let+Me+Show+You+Step+by+Step:+An+Interpretable+Graph+Routing+Network+for+Knowledge-based+Visual+Question+Answering)|0|
|[Flexible and Adaptable Summarization via Expertise Separation](https://doi.org/10.1145/3626772.3657789)|Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qingqing Zhu, Rui Yan, Xin Gao, Xiangliang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Flexible+and+Adaptable+Summarization+via+Expertise+Separation)|0|
|[ArabicaQA: A Comprehensive Dataset for Arabic Question Answering](https://doi.org/10.1145/3626772.3657889)|Abdelrahman Abdallah, Mahmoud SalahEldin Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ArabicaQA:+A+Comprehensive+Dataset+for+Arabic+Question+Answering)|0|
|[TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions](https://doi.org/10.1145/3626772.3657855)|Jamshid Mozafari, Anubhav Jangra, Adam Jatowt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TriviaHG:+A+Dataset+for+Automatic+Hint+Generation+from+Factoid+Questions)|0|
|[Capability-aware Prompt Reformulation Learning for Text-to-Image Generation](https://doi.org/10.1145/3626772.3657787)|Jingtao Zhan, Qingyao Ai, Yiqun Liu, Jia Chen, Shaoping Ma||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capability-aware+Prompt+Reformulation+Learning+for+Text-to-Image+Generation)|0|
|[Short Video Ordering via Position Decoding and Successor Prediction](https://doi.org/10.1145/3626772.3657795)|Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Ziyao Chen, Qing Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Short+Video+Ordering+via+Position+Decoding+and+Successor+Prediction)|0|
|[Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models](https://doi.org/10.1145/3626772.3657698)|Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Event+Grounded+Criminal+Court+View+Generation+with+Cooperative+(Large)+Language+Models)|0|
|[Legal Statute Identification: A Case Study using State-of-the-Art Datasets and Methods](https://doi.org/10.1145/3626772.3657879)|Shounak Paul, Rajas Bhatt, Pawan Goyal, Saptarshi Ghosh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Legal+Statute+Identification:+A+Case+Study+using+State-of-the-Art+Datasets+and+Methods)|0|
|[CivilSum: A Dataset for Abstractive Summarization of Indian Court Decisions](https://doi.org/10.1145/3626772.3657859)|Manuj Malik, Zheng Zhao, Marcio Fonseca, Shrisha Rao, Shay B. Cohen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CivilSum:+A+Dataset+for+Abstractive+Summarization+of+Indian+Court+Decisions)|0|
|[Analyzing Fusion Methods Using the Condorcet Rule](https://doi.org/10.1145/3626772.3657912)|Liron Tyomkin, Oren Kurland||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Analyzing+Fusion+Methods+Using+the+Condorcet+Rule)|0|
|[Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange](https://doi.org/10.1145/3626772.3657945)|Ankit Satpute, Noah Gießing, André GreinerPetter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Can+LLMs+Master+Math?+Investigating+Large+Language+Models+on+Math+Stack+Exchange)|0|
|[Combining Large Language Models and Crowdsourcing for Hybrid Human-AI Misinformation Detection](https://doi.org/10.1145/3626772.3657965)|Xia Zeng, David La Barbera, Kevin Roitero, Arkaitz Zubiaga, Stefano Mizzaro||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Combining+Large+Language+Models+and+Crowdsourcing+for+Hybrid+Human-AI+Misinformation+Detection)|0|
|[Counterfactual Augmentation for Robust Authorship Representation Learning](https://doi.org/10.1145/3626772.3657956)|Hieu Man, Thien Huu Nguyen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Counterfactual+Augmentation+for+Robust+Authorship+Representation+Learning)|0|
|[Enhancing Task Performance in Continual Instruction Fine-tuning Through Format Uniformity](https://doi.org/10.1145/3626772.3657920)|Xiaoyu Tan, Leijun Cheng, Xihe Qiu, Shaojie Shi, Yuan Cheng, Wei Chu, Yinghui Xu, Yuan Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Task+Performance+in+Continual+Instruction+Fine-tuning+Through+Format+Uniformity)|0|
|[From Text to Context: An Entailment Approach for News Stakeholder Classification](https://doi.org/10.1145/3626772.3657901)|Alapan Kuila, Sudeshna Sarkar||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Text+to+Context:+An+Entailment+Approach+for+News+Stakeholder+Classification)|0|
|[Graph Reasoning Enhanced Language Models for Text-to-SQL](https://doi.org/10.1145/3626772.3657961)|Zheng Gong, Ying Sun||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph+Reasoning+Enhanced+Language+Models+for+Text-to-SQL)|0|
|[IdmGAE: Importance-Inspired Dynamic Masking for Graph Autoencoders](https://doi.org/10.1145/3626772.3657913)|Ge Chen, Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu, Cuicui Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IdmGAE:+Importance-Inspired+Dynamic+Masking+for+Graph+Autoencoders)|0|
|[Inferring Climate Change Stances from Multimodal Tweets](https://doi.org/10.1145/3626772.3657950)|Nan Bai, Ricardo da Silva Torres, Anna Fensel, Tamara Metze, Art Dewulf||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inferring+Climate+Change+Stances+from+Multimodal+Tweets)|0|
|[Instruction-Guided Bullet Point Summarization of Long Financial Earnings Call Transcripts](https://doi.org/10.1145/3626772.3657948)|Subhendu Khatuya, Koushiki Sinha, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Instruction-Guided+Bullet+Point+Summarization+of+Long+Financial+Earnings+Call+Transcripts)|0|
|[Modeling Scholarly Collaboration and Temporal Dynamics in Citation Networks for Impact Prediction](https://doi.org/10.1145/3626772.3657926)|Pengwei Yan, Yangyang Kang, Zhuoren Jiang, Kaisong Song, Tianqianjin Lin, Changlong Sun, Xiaozhong Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Modeling+Scholarly+Collaboration+and+Temporal+Dynamics+in+Citation+Networks+for+Impact+Prediction)|0|
|[Multi-view Mixed Attention for Contrastive Learning on Hypergraphs](https://doi.org/10.1145/3626772.3657897)|Jongsoo Lee, DongKyu Chae||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-view+Mixed+Attention+for+Contrastive+Learning+on+Hypergraphs)|0|
|[Old IR Methods Meet RAG](https://doi.org/10.1145/3626772.3657935)|Oz Huly, Idan Pogrebinsky, David Carmel, Oren Kurland, Yoelle Maarek||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Old+IR+Methods+Meet+RAG)|0|
|[Prediction of the Realisation of an Information Need: An EEG Study](https://doi.org/10.1145/3626772.3657981)|Niall McGuire, Yashar Moshfeghi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Prediction+of+the+Realisation+of+an+Information+Need:+An+EEG+Study)|0|
|[R-ODE: Ricci Curvature Tells When You Will be Informed](https://doi.org/10.1145/3626772.3657954)|Li Sun, Jingbin Hu, Mengjie Li, Hao Peng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=R-ODE:+Ricci+Curvature+Tells+When+You+Will+be+Informed)|0|
|[PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking](https://doi.org/10.1145/3626772.3657904)|Yuzhang Xie, Jiaying Lu, Joyce Ho, Fadi B. Nahab, Xiao Hu, Carl Yang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PromptLink:+Leveraging+Large+Language+Models+for+Cross-Source+Biomedical+Concept+Linking)|0|
|[ReCODE: Modeling Repeat Consumption with Neural ODE](https://doi.org/10.1145/3626772.3657936)|Sunhao Dai, Changle Qu, Sirui Chen, Xiao Zhang, Jun Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ReCODE:+Modeling+Repeat+Consumption+with+Neural+ODE)|0|
|[RLStop: A Reinforcement Learning Stopping Method for TAR](https://doi.org/10.1145/3626772.3657911)|Reem Bin Hezam, Mark Stevenson||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RLStop:+A+Reinforcement+Learning+Stopping+Method+for+TAR)|0|
|[Timeline Summarization in the Era of LLMs](https://doi.org/10.1145/3626772.3657899)|Daivik Sojitra, Raghav Jain, Sriparna Saha, Adam Jatowt, Manish Gupta||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Timeline+Summarization+in+the+Era+of+LLMs)|0|
|[TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning](https://doi.org/10.1145/3626772.3657978)|Jing Zhu, Xiang Song, Vassilis N. Ioannidis, Danai Koutra, Christos Faloutsos||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TouchUp-G:+Improving+Feature+Representation+through+Graph-Centric+Finetuning)|0|
|[An Integrated Data Processing Framework for Pretraining Foundation Models](https://doi.org/10.1145/3626772.3657671)|Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Integrated+Data+Processing+Framework+for+Pretraining+Foundation+Models)|0|
|[Detecting and Explaining Emotions in Video Advertisements](https://doi.org/10.1145/3626772.3657664)|Joachim Vanneste, Manisha Verma, Debasis Ganguly||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Detecting+and+Explaining+Emotions+in+Video+Advertisements)|0|
|[FactCheck Editor: Multilingual Text Editor with End-to-End fact-checking](https://doi.org/10.1145/3626772.3657663)|Vinay Setty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FactCheck+Editor:+Multilingual+Text+Editor+with+End-to-End+fact-checking)|0|
|[Shadowfax: Harnessing Textual Knowledge Base Population](https://doi.org/10.1145/3626772.3657666)|Maxime Prieur, Cédric du Mouza, Guillaume Gadek, Bruno Grilhères||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Shadowfax:+Harnessing+Textual+Knowledge+Base+Population)|0|
|[SynDy: Synthetic Dynamic Dataset Generation Framework for Misinformation Tasks](https://doi.org/10.1145/3626772.3657667)|Michael Shliselberg, Ashkan Kazemi, Scott A. Hale, Shiri DoriHacohen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SynDy:+Synthetic+Dynamic+Dataset+Generation+Framework+for+Misinformation+Tasks)|0|
|[TextData: Save What You Know and Find What You Don't](https://doi.org/10.1145/3626772.3657681)|Kevin Ros, Kedar Takwane, Ashwin Patil, Rakshana Jayaprakash, ChengXiang Zhai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TextData:+Save+What+You+Know+and+Find+What+You+Don't)|0|
|[Towards Robust QA Evaluation via Open LLMs](https://doi.org/10.1145/3626772.3657675)|Ehsan Kamalloo, Shivani Upadhyay, Jimmy Lin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Robust+QA+Evaluation+via+Open+LLMs)|0|
|[Truth-O-Meter: Handling Multiple Inconsistent Sources Repairing LLM Hallucinations](https://doi.org/10.1145/3626772.3657679)|Boris Galitsky, Anton Chernyavskiy, Dmitry I. Ilvovsky||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Truth-O-Meter:+Handling+Multiple+Inconsistent+Sources+Repairing+LLM+Hallucinations)|0|
|["Ask Me Anything": How Comcast Uses LLMs to Assist Agents in Real Time](https://doi.org/10.1145/3626772.3661345)|Scott Rome, Tianwen Chen, Raphael Tang, Luwei Zhou, Ferhan Ture||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="Ask+Me+Anything":+How+Comcast+Uses+LLMs+to+Assist+Agents+in+Real+Time)|0|
|[unKR: A Python Library for Uncertain Knowledge Graph Reasoning by Representation Learning](https://doi.org/10.1145/3626772.3657661)|Jingting Wang, Tianxing Wu, Shilin Chen, Yunchang Liu, Shutong Zhu, Wei Li, Jingyi Xu, Guilin Qi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=unKR:+A+Python+Library+for+Uncertain+Knowledge+Graph+Reasoning+by+Representation+Learning)|0|
|[A Field Guide to Automatic Evaluation of LLM-Generated Summaries](https://doi.org/10.1145/3626772.3661346)|Tempest A. van Schaik, Brittany Pugh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Field+Guide+to+Automatic+Evaluation+of+LLM-Generated+Summaries)|0|
|[Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models](https://doi.org/10.1145/3626772.3661361)|Vinay Setty||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Surprising+Efficacy+of+Fine-Tuned+Transformers+for+Fact-Checking+over+Larger+Language+Models)|0|
|[Enhancing Baidu Multimodal Advertisement with Chinese Text-to-Image Generation via Bilingual Alignment and Caption Synthesis](https://doi.org/10.1145/3626772.3661350)|Kang Zhao, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wen Tao, Cong Han, Shuanglong Li, Lin Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Enhancing+Baidu+Multimodal+Advertisement+with+Chinese+Text-to-Image+Generation+via+Bilingual+Alignment+and+Caption+Synthesis)|0|
|[Misinformation Mitigation Praxis: Lessons Learned and Future Directions from Co·Insights](https://doi.org/10.1145/3626772.3661352)|Scott A. Hale, Kiran Garimella, Shiri DoriHacohen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Misinformation+Mitigation+Praxis:+Lessons+Learned+and+Future+Directions+from+Co·Insights)|0|
|[Graph-Based Audience Expansion Model for Marketing Campaigns](https://doi.org/10.1145/3626772.3661363)|Md. Mostafizur Rahman, Daisuke Kikuta, Yu Hirate, Toyotaro Suzumura||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Graph-Based+Audience+Expansion+Model+for+Marketing+Campaigns)|0|
|[Empowering Large Language Models: Tool Learning for Real-World Interaction](https://doi.org/10.1145/3626772.3661381)|Hongru Wang, Yujia Qin, Yankai Lin, Jeff Z. Pan, KamFai Wong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empowering+Large+Language+Models:+Tool+Learning+for+Real-World+Interaction)|0|
|[Large Language Models for Tabular Data: Progresses and Future Directions](https://doi.org/10.1145/3626772.3661384)|Haoyu Dong, Zhiruo Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Large+Language+Models+for+Tabular+Data:+Progresses+and+Future+Directions)|0|
|[Preventing and Detecting Misinformation Generated by Large Language Models](https://doi.org/10.1145/3626772.3661377)|Aiwei Liu, Qiang Sheng, Xuming Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preventing+and+Detecting+Misinformation+Generated+by+Large+Language+Models)|0|
|[LLM4Eval: Large Language Model for Evaluation in IR](https://doi.org/10.1145/3626772.3657992)|Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLM4Eval:+Large+Language+Model+for+Evaluation+in+IR)|0|
|[Machine Generated Explanations and Their Evaluation](https://doi.org/10.1145/3626772.3657652)|Edward Richards||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Machine+Generated+Explanations+and+Their+Evaluation)|0|
|[Leveraging LLMs for Detecting and Modeling the Propagation of Misinformation in Social Networks](https://doi.org/10.1145/3626772.3657654)|Payel Santra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+LLMs+for+Detecting+and+Modeling+the+Propagation+of+Misinformation+in+Social+Networks)|0|
|[Mosaicing Prevention in Declassification](https://doi.org/10.1145/3626772.3657656)|Nathaniel Rollings||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mosaicing+Prevention+in+Declassification)|0|
