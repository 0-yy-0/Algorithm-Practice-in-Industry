# WSDM2024 Paper List

|论文|作者|组织|摘要|翻译|代码|引用数|
|---|---|---|---|---|---|---|
|[RecJPQ: Training Large-Catalogue Sequential Recommenders](https://doi.org/10.1145/3616855.3635821)|Aleksandr V. Petrov, Craig Macdonald||Sequential recommender systems rank items based on the likelihood of their
next appearance in user-item interactions. Current models such as BERT4Rec and
SASRec generate sequence embeddings and compute scores for catalogue items, but
the increasing catalogue size makes training these models costly. The Joint
Product Quantisation method, originally proposed for passage retrieval,
markedly reduces the size of the retrieval index with minimal effect on model
effectiveness by replacing passage embeddings with a limited number of shared
centroid embeddings. This paper introduces RecJPQ, a novel adaptation of JPQ
for sequential recommendations. We apply RecJPQ to SASRec, BERT4Rec, and
GRU4rec models on three large-scale sequential datasets. Our results showed
that RecJPQ could notably reduce the model size (e.g., 48x reduction for the
Gowalla dataset with no effectiveness degradation). RecJPQ can also improve
model performance through a regularisation effect (e.g. +0.96% NDCG@10
improvement on the Booking.com dataset).|连续推荐系统根据项目在用户-项目交互中下一次出现的可能性对项目进行排序。当前的模型如 BERT4Rec 和 SASRec 生成序列嵌入并计算目录项的分数，但目录大小的增加使得这些模型的训练成本较高。联合乘积量化方法最初用于文本检索，通过用有限数量的共享质心嵌入代替文本嵌入，在对模型有效性影响最小的情况下显著减少了检索索引的大小。本文介绍了 RecJPQ，它是 JPQ 对顺序推荐的一个新的改进。我们将 RecJPQ 应用于三个大规模连续数据集上的 SASRec、 BERT4Rec 和 GRU4rec 模型。我们的研究结果表明，RecJPQ 可以显著减少模型的大小(例如，Gowalla 数据集减少了48倍，而且没有效率降低)。RecJPQ 还可以通过正则化效应提高模型性能(例如，Booking.com 数据集上的 NDCG 提高了0.96%)。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RecJPQ:+Training+Large-Catalogue+Sequential+Recommenders)|2|
|[Understanding User Behavior in Carousel Recommendation Systems for Click Modeling and Learning to Rank](https://doi.org/10.1145/3616855.3635734)|Santiago de LeonMartinez||Carousels (also-known as multilists) have become the standard user interface for e-commerce platforms replacing the ranked list, the previous standard for recommender systems. While the research community has begun to focus on carousels, there are many unanswered questions and undeveloped areas when compared to the literature for ranked lists, which includes information retrieval research on the presentation of web search results. This work is an extended abstract for the RecSys 2023 Doctoral Symposium outlining a PhD project, with the main contribution of addressing the undeveloped areas in carousel recommenders: 1) the formulation of new click models and 2) learning to rank with click data.   We present two significant barriers for this contribution and the field: lack of public datasets and lack of eye tracking user studies of browsing behavior. Clicks, the standard feedback collected by recommender systems, are insufficient to understand the whole interaction process of a user with a recommender requiring system designers to make assumptions, especially on browsing behavior. Eye tracking provides a means to elucidate the process and test these assumptions. Thus, to address these barriers and encourage future work, we will conduct an eye tracking user study within a carousel movie recommendation setting and make the dataset publicly available. Moreover, the insights learned on browsing behavior will help motivate the formulation of new click models and learning to rank.|旋转木马(也称为多列表)已经成为电子商务平台的标准用户界面，取代了以前推荐系统的标准排名列表。虽然研究团体已经开始关注旋转木马，但与排名列表的文献相比，还有许多未解答的问题和未开发的领域，其中包括对网络搜索结果呈现的信息检索研究。本文是 RecSys 2023年博士研讨会的扩展摘要，概述了一个博士项目，主要贡献在于解决传送带推荐中的不发达领域: 1)制定新的点击模型，2)学习根据点击数据进行排名。我们提出了两个重要的障碍，这个贡献和领域: 缺乏公共数据集和缺乏眼球跟踪用户研究的浏览行为。点击，推荐系统收集的标准反馈，不足以理解用户与推荐系统的整个交互过程，需要系统设计者做出假设，尤其是在浏览行为方面。眼球追踪提供了一种方法来阐明这一过程，并检验这些假设。因此，为了解决这些障碍并鼓励未来的工作，我们将在旋转木马电影推荐设置中进行眼动跟踪用户研究，并使数据集公开可用。此外，学到的浏览行为的洞察力将有助于激励制定新的点击模型和学习排名。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Understanding+User+Behavior+in+Carousel+Recommendation+Systems+for+Click+Modeling+and+Learning+to+Rank)|1|
|[Vector Search with OpenAI Embeddings: Lucene Is All You Need](https://doi.org/10.1145/3616855.3635691)|Jasper Xian, Tommaso Teofili, Ronak Pradeep, Jimmy Lin||We provide a reproducible, end-to-end demonstration of vector search with OpenAI embeddings using Lucene on the popular MS MARCO passage ranking test collection. The main goal of our work is to challenge the prevailing narrative that a dedicated vector store is necessary to take advantage of recent advances in deep neural networks as applied to search. Quite the contrary, we show that hierarchical navigable small-world network (HNSW) indexes in Lucene are adequate to provide vector search capabilities in a standard bi-encoder architecture. This suggests that, from a simple cost-benefit analysis, there does not appear to be a compelling reason to introduce a dedicated vector store into a modern "AI stack" for search, since such applications have already received substantial investments in existing, widely deployed infrastructure.|我们提供了一个可重复的，端到端的向量搜索演示与 OpenAI 嵌入使用 Lucene 在流行的 MS MARCO 通道排序测试集合。我们工作的主要目标是挑战流行的说法，即专用矢量存储是必要的，以利用深层神经网络的最新进展，适用于搜索。恰恰相反，我们证明 Lucene 的分层导航小世界网络(HNSW)索引足以在标准的双编码器架构中提供向量搜索功能。这表明，从一个简单的成本-收益分析来看，似乎没有令人信服的理由将一个专门的矢量存储引入现代“人工智能堆栈”中进行搜索，因为此类应用已经在现有的、广泛部署的基础设施中获得了大量投资。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Vector+Search+with+OpenAI+Embeddings:+Lucene+Is+All+You+Need)|1|
|[Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions](https://doi.org/10.1145/3616855.3635856)|Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, Mohammad Aliannejadi||Conversational question-answering (CQA) systems aim to create interactive search systems that effectively retrieve information by interacting with users. To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). Despite its effectiveness, challenges exist as human annotation is time-consuming, inconsistent, and not scalable. To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation, we propose a simulation framework that employs zero-shot learner LLMs for simulating teacher-student interactions. Our framework involves two LLMs interacting on a specific topic, with the first LLM acting as a student, generating questions to explore a given search topic. The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic. We implement both the student and teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the simulated data from various perspectives. We begin by evaluating the teacher's performance through both automatic and human assessment. Next, we evaluate the performance of the student, analyzing and comparing the disparities between questions generated by the LLM and those generated by humans. Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets. Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete. The student LLM generates more diverse questions, covering more aspects of a given topic.|会话问答(CQA)系统旨在创建交互式搜索系统，通过与用户交互有效地检索信息。为了复制人与人之间的对话，现有的工作使用人工注释器来扮演提问者(学生)和回答者(老师)的角色。尽管它很有效，但由于人工注释耗时、不一致且不可伸缩，因此存在挑战。为了解决这一问题，并研究大语言模型(LLM)在 CQA 模拟中的适用性，我们提出了一个模拟框架，使用零射程学习者 LLM 来模拟师生交互。我们的框架涉及两个 LLM 在特定主题上的交互，第一个 LLM 作为学生，生成问题来探索给定的搜索主题。第二个法学硕士课程通过回答问题来扮演教师的角色，并配备了额外的信息，包括关于给定主题的课文。我们通过零击提示 GPT-4模型来实现学生和老师。为了评估 LLM 在模拟 CQA 交互中的有效性，并了解 LLM 和人类生成的会话之间的差异，我们从不同的角度评估了模拟数据。我们首先通过自动评估和人工评估来评估教师的表现。接下来，我们评估学生的表现，分析和比较由 LLM 产生的问题和由人类产生的问题之间的差异。此外，我们进行了广泛的分析，通过对两个数据集的最先进的阅读理解模型进行基准测试，彻底检查 LLM 的性能。我们的研究结果表明，教师 LLM 生成更长的答案，往往是更准确和完整的。学生 LLM 生成更多不同的问题，涵盖给定主题的更多方面。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Let+the+LLMs+Talk:+Simulating+Human-to-Human+Conversational+QA+via+Zero-Shot+LLM-to-LLM+Interactions)|1|
|[MADM: A Model-agnostic Denoising Module for Graph-based Social Recommendation](https://doi.org/10.1145/3616855.3635784)|Wenze Ma, Yuexian Wang, Yanmin Zhu, Zhaobo Wang, Mengyuan Jing, Xuhao Zhao, Jiadi Yu, Feilong Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MADM:+A+Model-agnostic+Denoising+Module+for+Graph-based+Social+Recommendation)|1|
|[A Multi-Granularity-Aware Aspect Learning Model for Multi-Aspect Dense Retrieval](https://doi.org/10.1145/3616855.3635770)|Xiaojie Sun, Keping Bi, Jiafeng Guo, Sihui Yang, Qishen Zhang, Zhongyi Liu, Guannan Zhang, Xueqi Cheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Multi-Granularity-Aware+Aspect+Learning+Model+for+Multi-Aspect+Dense+Retrieval)|1|
|[LLMRec: Large Language Models with Graph Augmentation for Recommendation](https://doi.org/10.1145/3616855.3635853)|Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LLMRec:+Large+Language+Models+with+Graph+Augmentation+for+Recommendation)|1|
|[Likelihood-Based Methods Improve Parameter Estimation in Opinion Dynamics Models](https://doi.org/10.1145/3616855.3635785)|Jacopo Lenti, Corrado Monti, Gianmarco De Francisci Morales||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Likelihood-Based+Methods+Improve+Parameter+Estimation+in+Opinion+Dynamics+Models)|1|
|[K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization](https://doi.org/10.1145/3616855.3635772)|Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Yi Xu, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin, Junxian He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=K2:+A+Foundation+Language+Model+for+Geoscience+Knowledge+Understanding+and+Utilization)|1|
|[Collaboration and Transition: Distilling Item Transitions into Multi-Query Self-Attention for Sequential Recommendation](https://doi.org/10.1145/3616855.3635787)|Tianyu Zhu, Yansong Shi, Yuan Zhang, Yihong Wu, Fengran Mo, JianYun Nie||Modern recommender systems employ various sequential modules such as
self-attention to learn dynamic user interests. However, these methods are less
effective in capturing collaborative and transitional signals within user
interaction sequences. First, the self-attention architecture uses the
embedding of a single item as the attention query, which is inherently
challenging to capture collaborative signals. Second, these methods typically
follow an auto-regressive framework, which is unable to learn global item
transition patterns. To overcome these limitations, we propose a new method
called Multi-Query Self-Attention with Transition-Aware Embedding Distillation
(MQSA-TED). First, we propose an $L$-query self-attention module that employs
flexible window sizes for attention queries to capture collaborative signals.
In addition, we introduce a multi-query self-attention method that balances the
bias-variance trade-off in modeling user preferences by combining long and
short-query self-attentions. Second, we develop a transition-aware embedding
distillation module that distills global item-to-item transition patterns into
item embeddings, which enables the model to memorize and leverage transitional
signals and serves as a calibrator for collaborative signals. Experimental
results on four real-world datasets show the superiority of our proposed method
over state-of-the-art sequential recommendation methods.|现代推荐系统采用自我关注等多种顺序模块来学习动态用户兴趣。然而，这些方法在捕获用户交互序列中的协作和过渡信号方面效率较低。首先，自我注意体系结构使用嵌入单个条目作为注意查询，这对于捕获协作信号具有内在的挑战性。其次，这些方法通常遵循一个自动回归框架，该框架不能学习全局项目转换模式。为了克服这些局限性，本文提出了一种新的基于过渡意识的多查询自注意(MQSA-TED)方法。首先，我们提出了一个 $L $- query 自我注意模块，该模块使用灵活的窗口大小来进行注意查询以捕获协作信号。此外，本文还提出了一种结合长查询和短查询自注意的多查询自注意方法，平衡了偏差-方差权衡。其次，我们开发了一个具有过渡意识的嵌入蒸馏模块，该模块将全局项目到项目的过渡模式提取为项目嵌入，使模型能够记忆和利用过渡信号，并作为协作信号的校准器。在四个实际数据集上的实验结果表明，本文提出的方法优于目前最先进的顺序推荐方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Collaboration+and+Transition:+Distilling+Item+Transitions+into+Multi-Query+Self-Attention+for+Sequential+Recommendation)|0|
|[Contextual MAB Oriented Embedding Denoising for Sequential Recommendation](https://doi.org/10.1145/3616855.3635798)|Zhichao Feng, Pengfei Wang, Kaiyuan Li, Chenliang Li, Shangguang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Contextual+MAB+Oriented+Embedding+Denoising+for+Sequential+Recommendation)|0|
|[User Behavior Enriched Temporal Knowledge Graphs for Sequential Recommendation](https://doi.org/10.1145/3616855.3635762)|Hengchang Hu, Wei Guo, Xu Liu, Yong Liu, Ruiming Tang, Rui Zhang, MinYen Kan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Behavior+Enriched+Temporal+Knowledge+Graphs+for+Sequential+Recommendation)|0|
|[Global Heterogeneous Graph and Target Interest Denoising for Multi-behavior Sequential Recommendation](https://doi.org/10.1145/3616855.3635857)|Xuewei Li, Hongwei Chen, Jian Yu, Mankun Zhao, Tianyi Xu, Wenbin Zhang, Mei Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Global+Heterogeneous+Graph+and+Target+Interest+Denoising+for+Multi-behavior+Sequential+Recommendation)|0|
|[Attribute Simulation for Item Embedding Enhancement in Multi-interest Recommendation](https://doi.org/10.1145/3616855.3635841)|Yaokun Liu, Xiaowang Zhang, Minghui Zou, Zhiyong Feng||Although multi-interest recommenders have achieved significant progress in
the matching stage, our research reveals that existing models tend to exhibit
an under-clustered item embedding space, which leads to a low discernibility
between items and hampers item retrieval. This highlights the necessity for
item embedding enhancement. However, item attributes, which serve as effective
and straightforward side information for enhancement, are either unavailable or
incomplete in many public datasets due to the labor-intensive nature of manual
annotation tasks. This dilemma raises two meaningful questions: 1. Can we
bypass manual annotation and directly simulate complete attribute information
from the interaction data? And 2. If feasible, how to simulate attributes with
high accuracy and low complexity in the matching stage?
  In this paper, we first establish an inspiring theoretical feasibility that
the item-attribute correlation matrix can be approximated through elementary
transformations on the item co-occurrence matrix. Then based on formula
derivation, we propose a simple yet effective module, SimEmb (Item Embedding
Enhancement via Simulated Attribute), in the multi-interest recommendation of
the matching stage to implement our findings. By simulating attributes with the
co-occurrence matrix, SimEmb discards the item ID-based embedding and employs
the attribute-weighted summation for item embedding enhancement. Comprehensive
experiments on four benchmark datasets demonstrate that our approach notably
enhances the clustering of item embedding and significantly outperforms SOTA
models with an average improvement of 25.59% on Recall@20.|虽然多兴趣推荐系统在匹配阶段已经取得了显著的进展，但是我们的研究发现现有的模型倾向于表现出一种欠聚类的项目嵌入空间，导致项目之间的差异性较低，从而阻碍了项目的检索。这突出了项目嵌入增强的必要性。然而，由于人工注释任务的劳动密集性，在许多公共数据集中，作为有效和直接的增强辅助信息的项属性要么不可用，要么不完整。这个困境提出了两个有意义的问题: 1。我们是否可以绕过手动注释，直接从交互数据模拟完整的属性信息？二。如果可行，如何在匹配阶段模拟高精度、低复杂度的属性？在本文中，我们首先建立了一个鼓舞人心的理论可行性，即项目-属性相关矩阵可以通过项目共现矩阵的初等变换来近似。然后在公式推导的基础上，在匹配阶段的多兴趣推荐中，提出了一个简单而有效的模块——模拟属性项嵌入增强模块(SimEmb) ，以实现我们的研究结果。通过使用共生矩阵模拟属性，SimEmb 放弃了基于项目 ID 的嵌入，采用属性加权和的方法对项目进行嵌入增强。通过对四个基准数据集的综合实验表明，该方法显著提高了项目嵌入的聚类性能，并明显优于 SOTA 模型，在 Recall@20上平均提高了25.59% 。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Attribute+Simulation+for+Item+Embedding+Enhancement+in+Multi-interest+Recommendation)|0|
|[Deep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation](https://doi.org/10.1145/3616855.3635829)|Zhibo Xiao, Luwei Yang, Tao Zhang, Wen Jiang, Wei Ning, Yujiu Yang||The recommendation has been playing a key role in many industries, e.g.,
e-commerce, streaming media, social media, etc. Recently, a new recommendation
scenario, called Trigger-Induced Recommendation (TIR), where users are able to
explicitly express their instant interests via trigger items, is emerging as an
essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.
Without explicitly modeling the user's instant interest, traditional
recommendation methods usually obtain sub-optimal results in TIR. Even though
there are a few methods considering the trigger and target items simultaneously
to solve this problem, they still haven't taken into account temporal
information of user behaviors, the dynamic change of user instant interest when
the user scrolls down and the interactions between the trigger and target
items. To tackle these problems, we propose a novel method – Deep Evolutional
Instant Interest Network (DEI2N), for click-through rate prediction in TIR
scenarios. Specifically, we design a User Instant Interest Modeling Layer to
predict the dynamic change of the intensity of instant interest when the user
scrolls down. Temporal information is utilized in user behavior modeling.
Moreover, an Interaction Layer is introduced to learn better interactions
between the trigger and target items. We evaluate our method on several offline
and real-world industrial datasets. Experimental results show that our proposed
DEI2N outperforms state-of-the-art baselines. In addition, online A/B testing
demonstrates the superiority over the existing baseline in real-world
production environments.|推荐在许多行业都发挥了重要作用，例如电子商务、流媒体、社交媒体等。最近，一个新的推荐场景，称为触发诱导推荐(TIR) ，用户可以通过触发条目明确表达他们的即时兴趣，正在成为许多电子商务平台，如阿里巴巴和亚马逊的重要角色。传统的推荐方法在没有明确建立用户即时兴趣模型的情况下，往往在 TIR 中得到次优的推荐结果。针对这一问题，目前虽然有很多方法同时考虑了触发条目和目标条目，但都没有考虑到用户行为的时间信息、用户向下滚动时瞬间兴趣的动态变化以及触发条目和目标条目之间的交互作用。为了解决这些问题，我们提出了一种新的方法-深度进化即时兴趣网络(DEI2N) ，用于 TIR 场景中的点进率预测。具体来说，我们设计了一个用户即时兴趣建模层来预测用户向下滚动时即时兴趣强度的动态变化。时态信息用于用户行为建模。此外，还引入了交互层来学习触发器和目标项之间更好的交互。我们评估我们的方法在几个离线和现实世界的工业数据集。实验结果表明，我们提出的 DEI2N 性能优于最先进的基线。此外，在线 A/B 测试证明了在现实生产环境中优于现有基线的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Deep+Evolutional+Instant+Interest+Network+for+CTR+Prediction+in+Trigger-Induced+Recommendation)|0|
|[PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models](https://doi.org/10.1145/3616855.3635791)|WeiCheng Chang, JyunYu Jiang, Jiong Zhang, Mutasem AlDarabsah, Choon Hui Teo, ChoJui Hsieh, HsiangFu Yu, S. V. N. Vishwanathan||Embedding-based Retrieval Models (ERMs) have emerged as a promising framework for large-scale text retrieval problems due to powerful large language models. Nevertheless, fine-tuning ERMs to reach state-of-the-art results can be expensive due to the extreme scale of data as well as the complexity of multi-stages pipelines (e.g., pre-training, fine-tuning, distillation). In this work, we propose the PEFA framework, namely ParamEter-Free Adapters, for fast tuning of ERMs without any backward pass in the optimization. At index building stage, PEFA equips the ERM with a non-parametric k-nearest neighbor (kNN) component. At inference stage, PEFA performs a convex combination of two scoring functions, one from the ERM and the other from the kNN. Based on the neighborhood definition, PEFA framework induces two realizations, namely PEFA-XL (i.e., extra large) using double ANN indices and PEFA-XS (i.e., extra small) using a single ANN index. Empirically, PEFA achieves significant improvement on two retrieval applications. For document retrieval, regarding Recall@100 metric, PEFA improves not only pre-trained ERMs on Trivia-QA by an average of 13.2%, but also fine-tuned ERMs on NQ-320K by an average of 5.5%, respectively. For product search, PEFA improves the Recall@100 of the fine-tuned ERMs by an average of 5.3% and 14.5%, for PEFA-XS and PEFA-XL, respectively. Our code is available at https://github.com/ amzn/pecos/tree/mainline/examples/pefa-wsdm24|基于嵌入式的检索模型(ERM)由于其强大的语言模型功能而成为解决大规模文本检索问题的有力工具。然而，由于数据的极端规模以及多阶段管道的复杂性(例如，预训练、微调、精馏) ，为达到最先进的结果而对 ERM 进行微调可能是昂贵的。在这项工作中，我们提出了 PEFA 框架，即参数自由适配器，用于快速调优 ERM，而不需要在优化过程中进行任何反向传递。在索引建立阶段，PEFA 为 ERM 配备了一个非参数 k- 最近邻(kNN)分量。在推理阶段，PEFA 执行两个评分功能的凸组合，一个来自机构风险管理，另一个来自 kNN。在邻域定义的基础上，PEFA 框架引入了两种实现方法，即使用双神经网络指标的 PEFA-XL (即超大)和使用单神经网络指标的 PEFA-XS (即超小)。经验表明，PEFA 在两个检索应用程序上取得了显著的改进。就文献检索而言，在召回@100指标方面，PEFA 不仅平均提高了 Trivia-QA 上预先训练的 ERM 的13.2% ，而且平均提高了 NQ-320k 上的 ERM 的5.5% 。对于产品搜索，对于 PEFA-XS 和 PEFA-XL，PEFA 分别使经过微调的 ERM 的召回率平均提高5.3% 和14.5% 。我们的代码可以在 amzn/pecos/tree/mainline/example/pefa-wsdm24 https://github.com/找到|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEFA:+Parameter-Free+Adapters+for+Large-scale+Embedding-based+Retrieval+Models)|0|
|[User Consented Federated Recommender System Against Personalized Attribute Inference Attack](https://doi.org/10.1145/3616855.3635830)|Qi Hu, Yangqiu Song||Recommender systems can be privacy-sensitive. To protect users' private
historical interactions, federated learning has been proposed in distributed
learning for user representations. Using federated recommender (FedRec)
systems, users can train a shared recommendation model on local devices and
prevent raw data transmissions and collections. However, the recommendation
model learned by a common FedRec may still be vulnerable to private information
leakage risks, particularly attribute inference attacks, which means that the
attacker can easily infer users' personal attributes from the learned model.
Additionally, traditional FedRecs seldom consider the diverse privacy
preference of users, leading to difficulties in balancing the recommendation
utility and privacy preservation. Consequently, FedRecs may suffer from
unnecessary recommendation performance loss due to over-protection and private
information leakage simultaneously. In this work, we propose a novel
user-consented federated recommendation system (UC-FedRec) to flexibly satisfy
the different privacy needs of users by paying a minimum recommendation
accuracy price. UC-FedRec allows users to self-define their privacy preferences
to meet various demands and makes recommendations with user consent.
Experiments conducted on different real-world datasets demonstrate that our
framework is more efficient and flexible compared to baselines.|推荐系统可能对隐私敏感。为了保护用户的私有历史交互，联邦学习被提出用于用户表示的分布式学习。使用联邦推荐(FedRec)系统，用户可以在本地设备上培训共享推荐模型，并防止原始数据传输和收集。然而，一个普通的联邦快递推荐模型可能仍然容易受到私人信息泄露的攻击，特别是属性推理攻击，这意味着攻击者可以很容易地从推荐模型中推断出用户的个人属性。此外，传统的 FedRecs 很少考虑用户的不同隐私偏好，导致难以平衡推荐实用程序和隐私保护。因此，由于过度保护和私人信息泄露，美联储可能同时遭受不必要的推荐性能损失。在本研究中，我们提出一个新的使用者同意的联邦推荐系统(UC-FedRec) ，以支付最低的推荐准确度价格，灵活地满足使用者不同的隐私需求。UC-FedRec 允许用户自定义他们的隐私偏好，以满足不同的需求，并在用户同意的情况下提出建议。在不同的实际数据集上进行的实验表明，我们的框架比基线更有效和灵活。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=User+Consented+Federated+Recommender+System+Against+Personalized+Attribute+Inference+Attack)|0|
|[Multi-Intent Attribute-Aware Text Matching in Searching](https://doi.org/10.1145/3616855.3635813)|Mingzhe Li, Xiuying Chen, Jing Xiang, Qishen Zhang, Changsheng Ma, Chenchen Dai, Jinxiong Chang, Zhongyi Liu, Guannan Zhang||Text matching systems have become a fundamental service in most searching
platforms. For instance, they are responsible for matching user queries to
relevant candidate items, or rewriting the user-input query to a pre-selected
high-performing one for a better search experience. In practice, both the
queries and items often contain multiple attributes, such as the category of
the item and the location mentioned in the query, which represent condensed key
information that is helpful for matching. However, most of the existing works
downplay the effectiveness of attributes by integrating them into text
representations as supplementary information. Hence, in this work, we focus on
exploring the relationship between the attributes from two sides. Since
attributes from two ends are often not aligned in terms of number and type, we
propose to exploit the benefit of attributes by multiple-intent modeling. The
intents extracted from attributes summarize the diverse needs of queries and
provide rich content of items, which are more refined and abstract, and can be
aligned for paired inputs. Concretely, we propose a multi-intent
attribute-aware matching model (MIM), which consists of three main components:
attribute-aware encoder, multi-intent modeling, and intent-aware matching. In
the attribute-aware encoder, the text and attributes are weighted and processed
through a scaled attention mechanism with regard to the attributes' importance.
Afterward, the multi-intent modeling extracts intents from two ends and aligns
them. Herein, we come up with a distribution loss to ensure the learned intents
are diverse but concentrated, and a kullback-leibler divergence loss that
aligns the learned intents. Finally, in the intent-aware matching, the intents
are evaluated by a self-supervised masking task, and then incorporated to
output the final matching result.|文本匹配系统已经成为大多数搜索平台的基础服务。例如，它们负责将用户查询与相关候选项匹配，或者将用户输入查询重写为预先选定的高性能查询，以获得更好的搜索体验。实际上，查询和项通常都包含多个属性，例如项的类别和查询中提到的位置，这些属性表示有助于匹配的压缩键信息。然而，现有的大多数作品通过将属性作为补充信息集成到文本表示中来淡化属性的有效性。因此，本文着重从两个方面探讨属性之间的关系。由于来自两端的属性通常在数量和类型方面不一致，我们建议通过多意图建模来利用属性的优势。从属性中提取的意图总结了查询的不同需求，并提供了丰富的项目内容，这些内容更加精炼和抽象，并且可以对成对的输入进行对齐。具体地说，我们提出了一个多意图属性感知匹配模型(MIM) ，它由三个主要部分组成: 属性感知编码器、多意图建模和意图感知匹配。在属性感知编码器中，文本和属性根据属性的重要性通过缩放注意机制进行加权和处理。然后，多意图建模从两端提取意图并对齐它们。在这里，我们提出了一个分布损失，以确保学习意图的多样性，但集中，和一个 kullback-leibler 散度损失，调整了学习意图。最后，在意图感知匹配中，通过自监督掩蔽任务对意图进行评估，然后合并到一起输出最终的匹配结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Intent+Attribute-Aware+Text+Matching+in+Searching)|0|
|[Mixed Attention Network for Cross-domain Sequential Recommendation](https://doi.org/10.1145/3616855.3635801)|Guanyu Lin, Chen Gao, Yu Zheng, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, Zhiheng Li, Depeng Jin, Yong Li, Meng Wang||In modern recommender systems, sequential recommendation leverages
chronological user behaviors to make effective next-item suggestions, which
suffers from data sparsity issues, especially for new users. One promising line
of work is the cross-domain recommendation, which trains models with data
across multiple domains to improve the performance in data-scarce domains.
Recent proposed cross-domain sequential recommendation models such as PiNet and
DASL have a common drawback relying heavily on overlapped users in different
domains, which limits their usage in practical recommender systems. In this
paper, we propose a Mixed Attention Network (MAN) with local and global
attention modules to extract the domain-specific and cross-domain information.
Firstly, we propose a local/global encoding layer to capture the
domain-specific/cross-domain sequential pattern. Then we propose a mixed
attention layer with item similarity attention, sequence-fusion attention, and
group-prototype attention to capture the local/global item similarity, fuse the
local/global item sequence, and extract the user groups across different
domains, respectively. Finally, we propose a local/global prediction layer to
further evolve and combine the domain-specific and cross-domain interests.
Experimental results on two real-world datasets (each with two domains)
demonstrate the superiority of our proposed model. Further study also
illustrates that our proposed method and components are model-agnostic and
effective, respectively. The code and data are available at
https://github.com/Guanyu-Lin/MAN.|在现代推荐系统中，顺序推荐利用按时间顺序排列的用户行为来提出有效的下一项推荐，这种推荐存在数据稀疏问题，尤其是对于新用户。一个很有前途的工作领域是跨域推荐，它用跨多个域的数据来训练模型，以提高数据稀缺域的性能。最近提出的跨域顺序推荐模型，如 PiNet 和 DASL，有一个共同的缺点，即严重依赖于不同领域的重叠用户，这限制了它们在实际推荐系统中的使用。本文提出了一种基于局部和全局注意模块的混合注意网络(MAN)来提取特定领域和跨领域的信息。首先，我们提出了一个局部/全局编码层来捕获域特定/跨域的序列模式。然后提出了一个具有项目相似性注意、序列融合注意和群体原型注意的混合注意层，分别捕获局部/全局项目相似性，融合局部/全局项目序列，提取不同领域的用户群。最后，我们提出了一个局部/全局预测层，以进一步发展和结合特定领域和跨领域的利益。在两个实际数据集上的实验结果表明了该模型的优越性。进一步的研究还表明，我们提出的方法和组件是模型无关的和有效的，分别。代码和数据可在 https://github.com/guanyu-lin/man 查阅。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mixed+Attention+Network+for+Cross-domain+Sequential+Recommendation)|0|
|[Multi-Sequence Attentive User Representation Learning for Side-information Integrated Sequential Recommendation](https://doi.org/10.1145/3616855.3635815)|Xiaolin Lin, Jinwei Luo, Junwei Pan, Weike Pan, Zhong Ming, Xun Liu, Shudong Huang, Jie Jiang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Sequence+Attentive+User+Representation+Learning+for+Side-information+Integrated+Sequential+Recommendation)|0|
|[Intent Contrastive Learning with Cross Subsequences for Sequential Recommendation](https://doi.org/10.1145/3616855.3635773)|Xiuyuan Qin, Huanhuan Yuan, Pengpeng Zhao, Guanfeng Liu, Fuzhen Zhuang, Victor S. Sheng||The user purchase behaviors are mainly influenced by their intentions (e.g., buying clothes for decoration, buying brushes for painting, etc.). Modeling a user's latent intention can significantly improve the performance of recommendations. Previous works model users' intentions by considering the predefined label in auxiliary information or introducing stochastic data augmentation to learn purposes in the latent space. However, the auxiliary information is sparse and not always available for recommender systems, and introducing stochastic data augmentation may introduce noise and thus change the intentions hidden in the sequence. Therefore, leveraging user intentions for sequential recommendation (SR) can be challenging because they are frequently varied and unobserved. In this paper, Intent contrastive learning with Cross Subsequences for sequential Recommendation (ICSRec) is proposed to model users' latent intentions. Specifically, ICSRec first segments a user's sequential behaviors into multiple subsequences by using a dynamic sliding operation and takes these subsequences into the encoder to generate the representations for the user's intentions. To tackle the problem of no explicit labels for purposes, ICSRec assumes different subsequences with the same target item may represent the same intention and proposes a coarse-grain intent contrastive learning to push these subsequences closer. Then, fine-grain intent contrastive learning is mentioned to capture the fine-grain intentions of subsequences in sequential behaviors. Extensive experiments conducted on four real-world datasets demonstrate the superior performance of the proposed ICSRec model compared with baseline methods.|用户的购买行为主要受其购买意图的影响(如购买衣服装饰、购买画笔等)。对用户的潜在意图进行建模可以显著提高推荐的性能。以往的研究通过考虑辅助信息中的预定义标签或引入随机数据增量在潜在空间中学习目的来模拟用户的意图。然而，辅助信息是稀疏的，并不总是可用于推荐系统，引入随机数据增强可能会引入噪声，从而改变意图隐藏在序列。因此，利用用户意图进行顺序推荐(SR)可能是具有挑战性的，因为它们经常变化和未被观察到。本文提出了一种基于交叉子序列的序列推荐意图对比学习(ICSRec)方法来模拟用户的潜在意图。具体来说，ICSRec 首先使用动态滑动操作将用户的连续行为分割成多个子序列，并将这些子序列带入编码器以生成用户意图的表示。为了解决目的不明确标签的问题，ICSRec 假设具有相同目标项的不同子序列可以表示相同的意图，并提出了一种粗粒度意图对比学习方法来使这些子序列更加接近。然后，提出细粒度意图对比学习来捕捉序列行为中子序列的细粒度意图。在四个实际数据集上进行的大量实验表明，与基线方法相比，所提出的 ICSRec 模型具有更好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Intent+Contrastive+Learning+with+Cross+Subsequences+for+Sequential+Recommendation)|0|
|[Debiasing Sequential Recommenders through Distributionally Robust Optimization over System Exposure](https://doi.org/10.1145/3616855.3635848)|Jiyuan Yang, Yue Ding, Yidan Wang, Pengjie Ren, Zhumin Chen, Fei Cai, Jun Ma, Rui Zhang, Zhaochun Ren, Xin Xin||Sequential recommendation (SR) models are typically trained on user-item
interactions which are affected by the system exposure bias, leading to the
user preference learned from the biased SR model not being fully consistent
with the true user preference. Exposure bias refers to the fact that user
interactions are dependent upon the partial items exposed to the user. Existing
debiasing methods do not make full use of the system exposure data and suffer
from sub-optimal recommendation performance and high variance. In this paper,
we propose to debias sequential recommenders through Distributionally Robust
Optimization (DRO) over system exposure data. The key idea is to utilize DRO to
optimize the worst-case error over an uncertainty set to safeguard the model
against distributional discrepancy caused by the exposure bias. The main
challenge to apply DRO for exposure debiasing in SR lies in how to construct
the uncertainty set and avoid the overestimation of user preference on biased
samples. Moreover, how to evaluate the debiasing effect on biased test set is
also an open question. To this end, we first introduce an exposure simulator
trained upon the system exposure data to calculate the exposure distribution,
which is then regarded as the nominal distribution to construct the uncertainty
set of DRO. Then, we introduce a penalty to items with high exposure
probability to avoid the overestimation of user preference for biased samples.
Finally, we design a debiased self-normalized inverse propensity score (SNIPS)
evaluator for evaluating the debiasing effect on the biased offline test set.
We conduct extensive experiments on two real-world datasets to verify the
effectiveness of the proposed methods. Experimental results demonstrate the
superior exposure debiasing performance of proposed methods. Codes and data are
available at \url{https://github.com/nancheng58/DebiasedSR_DRO}.|序贯推荐(SR)模型通常针对受系统暴露偏差影响的用户-项目交互进行训练，导致从偏向 SR 模型中学到的用户偏好与真实用户偏好不完全一致。暴露偏差是指用户交互依赖于暴露给用户的部分项目这一事实。现有的去偏方法没有充分利用系统曝光数据，推荐性能不理想，方差较大。本文提出了一种基于系统曝光数据的分布式鲁棒优化(DRO)方法来降低序列推荐器的偏差。其核心思想是利用 DRO 对不确定集上的最坏情况误差进行优化，以保护模型不受曝光偏差引起的分布差异的影响。如何构造不确定性集合，避免偏差样本对用户偏好的过高估计，是应用 DRO 进行 SR 曝光消偏的主要挑战。此外，如何评价偏置测试集的去偏效果也是一个悬而未决的问题。为此，我们首先介绍了一个基于系统曝光数据训练的曝光模拟器来计算曝光分布，然后将其视为标称分布来构造 DRO 的不确定度集。然后，我们引入了一个惩罚项目的高暴露概率，以避免过高估计的用户偏好偏差样本。最后，我们设计了一个去偏的自标准化逆倾向得分(SNIPS)评估器来评估有偏离离线测试集的去偏效果。为了验证该方法的有效性，我们在两个实际数据集上进行了大量的实验。实验结果表明，该方法具有较好的曝光消偏性能。代码和数据可在 url { https://github.com/nancheng58/debiasedsr_dro }获得。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Debiasing+Sequential+Recommenders+through+Distributionally+Robust+Optimization+over+System+Exposure)|0|
|[Applications of LLMs in E-Commerce Search and Product Knowledge Graph: The DoorDash Case Study](https://doi.org/10.1145/3616855.3635738)|Sudeep Das, Raghav Saboo, Chaitanya S. K. Vadrevu, Bruce Wang, Steven Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Applications+of+LLMs+in+E-Commerce+Search+and+Product+Knowledge+Graph:+The+DoorDash+Case+Study)|0|
|[AAGenRec: A Novel Approach for Mitigating Inter-task Interference in Multi-task Optimization of Sequential Behavior Modeling](https://doi.org/10.1145/3616855.3635746)|Jiawei Zhang, Shimin Yang, Liang Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AAGenRec:+A+Novel+Approach+for+Mitigating+Inter-task+Interference+in+Multi-task+Optimization+of+Sequential+Behavior+Modeling)|0|
|[To Copy, or not to Copy; That is a Critical Issue of the Output Softmax Layer in Neural Sequential Recommenders](https://doi.org/10.1145/3616855.3635755)|HawShiuan Chang, Nikhil Agarwal, Andrew McCallum||Recent studies suggest that the existing neural models have difficulty handling repeated items in sequential recommendation tasks. However, our understanding of this difficulty is still limited. In this study, we substantially advance this field by identifying a major source of the problem: the single hidden state embedding and static item embeddings in the output softmax layer. Specifically, the similarity structure of the global item embeddings in the softmax layer sometimes forces the single hidden state embedding to be close to new items when copying is a better choice, while sometimes forcing the hidden state to be close to the items from the input inappropriately. To alleviate the problem, we adapt the recently-proposed softmax alternatives such as softmax-CPR to sequential recommendation tasks and demonstrate that the new softmax architectures unleash the capability of the neural encoder on learning when to copy and when to exclude the items from the input sequence. By only making some simple modifications on the output softmax layer for SASRec and GRU4Rec, softmax-CPR achieves consistent improvement in 12 datasets. With almost the same model size, our best method not only improves the average NDCG@10 of GRU4Rec in 5 datasets with duplicated items by 10% (4%-17% individually) but also improves 7 datasets without duplicated items by 24% (8%-39%)!|最近的研究表明，现有的神经模型难以处理重复项目的顺序推荐任务。然而，我们对这个困难的理解仍然是有限的。在这项研究中，我们通过识别问题的一个主要来源: 单一的隐藏状态嵌入和静态项嵌入在输出 softmax 层大大推进了这个领域。具体来说，Softmax 层中全局项嵌入的相似性结构有时迫使单个隐藏状态嵌入接近新项，而复制是更好的选择，有时迫使隐藏状态不适当地接近来自输入的项。为了缓解这一问题，我们将最近提出的 softmax 替代方案(如 softmax-CPR)适用于顺序推荐任务，并证明新的 softmax 架构释放了神经编码器学习何时复制和何时从输入序列中排除项目的能力。通过对 SASRec 和 GRU4Rec 的输出 softmax 层进行一些简单的修改，softmax-CPR 在12个数据集中实现了一致的改进。在几乎相同的模型大小下，我们的最佳方法不仅将5个重复项目数据集中 GRU4Rec 的平均 NDCG@10提高了10% (单独4%-17%) ，而且将7个没有重复项目的数据集提高了24% (8%-39%) ！|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=To+Copy,+or+not+to+Copy;+That+is+a+Critical+Issue+of+the+Output+Softmax+Layer+in+Neural+Sequential+Recommenders)|0|
|[Budgeted Embedding Table For Recommender Systems](https://doi.org/10.1145/3616855.3635778)|Yunke Qu, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin||At the heart of contemporary recommender systems (RSs) are latent factor models that provide quality recommendation experience to users. These models use embedding vectors, which are typically of a uniform and fixed size, to represent users and items. As the number of users and items continues to grow, this design becomes inefficient and hard to scale. Recent lightweight embedding methods have enabled different users and items to have diverse embedding sizes, but are commonly subject to two major drawbacks. Firstly, they limit the embedding size search to optimizing a heuristic balancing the recommendation quality and the memory complexity, where the trade-off coefficient needs to be manually tuned for every memory budget requested. The implicitly enforced memory complexity term can even fail to cap the parameter usage, making the resultant embedding table fail to meet the memory budget strictly. Secondly, most solutions, especially reinforcement learning based ones derive and optimize the embedding size for each each user/item on an instance-by-instance basis, which impedes the search efficiency. In this paper, we propose Budgeted Embedding Table (BET), a novel method that generates table-level actions (i.e., embedding sizes for all users and items) that is guaranteed to meet pre-specified memory budgets. Furthermore, by leveraging a set-based action formulation and engaging set representation learning, we present an innovative action search strategy powered by an action fitness predictor that efficiently evaluates each table-level action. Experiments have shown state-of-the-art performance on two real-world datasets when BET is paired with three popular recommender models under different memory budgets.|当代推荐系统(RS)的核心是为用户提供高质量推荐体验的潜在因素模型。这些模型使用嵌入向量来表示用户和项目，嵌入向量通常具有统一和固定的大小。随着用户和项目数量的持续增长，这种设计变得效率低下且难以扩展。最近的轻量级嵌入方法允许不同的用户和项具有不同的嵌入大小，但是通常有两个主要缺点。首先，它们将嵌入大小搜索限制为优化一种启发式算法，以平衡推荐质量和内存复杂度，其中需要针对每个请求的内存预算手动调整折衷系数。隐式强制的内存复杂度项甚至可能无法限制参数的使用，使得结果嵌入表无法严格满足内存预算。其次，大多数解决方案，尤其是基于强化学习的解决方案，会逐个实例地推导和优化每个用户/条目的嵌入大小，这会影响搜索效率。在本文中，我们提出了预算嵌入表(BET) ，一种新的方法，生成表级的行为(即，嵌入大小的所有用户和项目) ，是保证满足预先指定的内存预算。此外，通过利用基于集合的动作制定和参与集合表示学习，我们提出了一个创新的动作搜索策略，由动作适应性预测器驱动，有效地评估每个表级动作。实验表明，当 BET 与三个流行的推荐模型在不同的内存预算下配对时，在两个真实世界的数据集上有最先进的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Budgeted+Embedding+Table+For+Recommender+Systems)|0|
|[AutoPooling: Automated Pooling Search for Multi-valued Features in Recommendations](https://doi.org/10.1145/3616855.3635808)|He Wei, Yuekui Yang, Shaoping Ma, Haiyang Wu, Yangyang Tang, Meixi Liu, Yang Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=AutoPooling:+Automated+Pooling+Search+for+Multi-valued+Features+in+Recommendations)|0|
|[Linear Recurrent Units for Sequential Recommendation](https://doi.org/10.1145/3616855.3635760)|Zhenrui Yue, Yueqi Wang, Zhankui He, Huimin Zeng, Julian J. McAuley, Dong Wang||State-of-the-art sequential recommendation relies heavily on self-attention-based recommender models. Yet such models are computationally expensive and often too slow for real-time recommendation. Furthermore, the self-attention operation is performed at a sequence-level, thereby making low-cost incremental inference challenging. Inspired by recent advances in efficient language modeling, we propose linear recurrent units for sequential recommendation (LRURec). Similar to recurrent neural networks, LRURec offers rapid inference and can achieve incremental inference on sequential inputs. By decomposing the linear recurrence operation and designing recursive parallelization in our framework, LRURec provides the additional benefits of reduced model size and parallelizable training. Moreover, we optimize the architecture of LRURec by implementing a series of modifications to address the lack of non-linearity and improve training dynamics. To validate the effectiveness of our proposed LRURec, we conduct extensive experiments on multiple real-world datasets and compare its performance against state-of-the-art sequential recommenders. Experimental results demonstrate the effectiveness of LRURec, which consistently outperforms baselines by a significant margin. Results also highlight the efficiency of LRURec with our parallelized training paradigm and fast inference on long sequences, showing its potential to further enhance user experience in sequential recommendation.|最先进的顺序推荐在很大程度上依赖于基于自我注意的推荐模型。然而，这样的模型在计算上是昂贵的，而且对于实时推荐来说往往太慢了。此外，自我注意操作是在序列级上执行的，因此使得低成本的增量推理具有挑战性。受到最近在有效语言建模方面的进展的启发，我们提出了用于顺序推荐的线性递归单元(LRURec)。类似于递归神经网络，LRURec 提供了快速推理，并可以实现对顺序输入的增量推理。通过在我们的框架中分解线性递归操作和设计递归并行，LRURec 提供了减少模型大小和可并行训练的额外好处。此外，我们通过实现一系列的修改来优化 LRURec 的体系结构，以解决缺乏非线性和改善训练动态性的问题。为了验证我们提出的 LRURec 的有效性，我们在多个真实世界的数据集上进行了广泛的实验，并将其性能与最先进的顺序推荐进行了比较。实验结果证明了 LRURec 算法的有效性，该算法的性能始终优于基线算法。结果还突出了 LRURec 的效率与我们的并行训练范式和快速推断的长序列，显示了它的潜力，进一步提高用户体验的顺序推荐。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Linear+Recurrent+Units+for+Sequential+Recommendation)|0|
|[CharmBana: Progressive Responses with Real-Time Internet Search for Knowledge-Powered Conversations](https://doi.org/10.1145/3616855.3635702)|Revanth Gangi Reddy, Sharath Chandra Etagi Suresh, Hao Bai, Wentao Yao, Mankeerat Sidhu, Karan Aggarwal, Prathamesh Sonawane, ChengXiang Zhai||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CharmBana:+Progressive+Responses+with+Real-Time+Internet+Search+for+Knowledge-Powered+Conversations)|0|
|[SIRUP: Search-based Book Recommendation Playground](https://doi.org/10.1145/3616855.3635692)|Ghazaleh Haratinezhad Torbati, Anna Tigunova, Gerhard Weikum||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SIRUP:+Search-based+Book+Recommendation+Playground)|0|
|[Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs](https://doi.org/10.1145/3616855.3635689)|Behnam Rahdari, Hao Ding, Ziwei Fan, Yifei Ma, Zhuotong Chen, Anoop Deoras, Branislav Kveton||The unique capabilities of Large Language Models (LLMs), such as the natural
language text generation ability, position them as strong candidates for
providing explanation for recommendations. However, despite the size of the
LLM, most existing models struggle to produce zero-shot explanations reliably.
To address this issue, we propose a framework called Logic-Scaffolding, that
combines the ideas of aspect-based explanation and chain-of-thought prompting
to generate explanations through intermediate reasoning steps. In this paper,
we share our experience in building the framework and present an interactive
demonstration for exploring our results.|大型语言模型(LLM)的独特功能，例如自然语言文本生成能力，将它们定位为为建议提供解释的强有力候选者。然而，尽管 LLM 的规模很大，大多数现有的模型都难以可靠地提供零射击的解释。为了解决这个问题，我们提出了一个名为逻辑支架的框架，它结合了基于方面的解释和思想链的提示，通过中间的推理步骤来生成解释。在本文中，我们分享了我们在构建框架方面的经验，并为探索我们的结果提供了一个交互式示范。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Logic-Scaffolding:+Personalized+Aspect-Instructed+Recommendation+Explanation+Generation+using+LLMs)|0|
|[Effective and Efficient Transformer Models for Sequential Recommendation](https://doi.org/10.1145/3616855.3635733)|Aleksandr V. Petrov||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Effective+and+Efficient+Transformer+Models+for+Sequential+Recommendation)|0|
|[CDRNP: Cross-Domain Recommendation to Cold-Start Users via Neural Process](https://doi.org/10.1145/3616855.3635794)|Xiaodong Li, Jiawei Sheng, Jiangxia Cao, Wenyuan Zhang, Quangang Li, Tingwen Liu||Cross-domain recommendation (CDR) has been proven as a promising way to tackle the user cold-start problem, which aims to make recommendations for users in the target domain by transferring the user preference derived from the source domain. Traditional CDR studies follow the embedding and mapping (EMCDR) paradigm, which transfers user representations from the source to target domain by learning a user-shared mapping function, neglecting the user-specific preference. Recent CDR studies attempt to learn user-specific mapping functions in meta-learning paradigm, which regards each user's CDR as an individual task, but neglects the preference correlations among users, limiting the beneficial information for user representations. Moreover, both of the paradigms neglect the explicit user-item interactions from both domains during the mapping process. To address the above issues, this paper proposes a novel CDR framework with neural process (NP), termed as CDRNP. Particularly, it develops the meta-learning paradigm to leverage user-specific preference, and further introduces a stochastic process by NP to capture the preference correlations among the overlapping and cold-start users, thus generating more powerful mapping functions by mapping the user-specific preference and common preference correlations to a predictive probability distribution. In addition, we also introduce a preference remainer to enhance the common preference from the overlapping users, and finally devises an adaptive conditional decoder with preference modulation to make prediction for cold-start users with items in the target domain. Experimental results demonstrate that CDRNP outperforms previous SOTA methods in three real-world CDR scenarios.|跨域推荐已被证明是一种解决用户冷启动问题的有效方法，其目的是通过传递源域中的用户偏好来为目标域中的用户提供推荐。传统的 CDR 研究遵循嵌入与映射(EMCDR)范式，通过学习用户共享映射函数，将用户表征从源域转移到目标域，忽略了用户特定的偏好。最近的 CDR 研究试图在元学习范式中学习用户特定的映射函数，它将每个用户的 CDR 视为一个单独的任务，但忽略了用户之间的偏好相关性，限制了用户表征的有益信息。此外，这两个范例在映射过程中都忽略了来自两个领域的显式用户-项目交互。为了解决上述问题，本文提出了一种新的带有神经过程的 CDR 框架，称为 CDRNP。特别是，它开发了元学习范式来利用用户特定的偏好，并进一步引入了 NP 的随机过程来捕捉重叠和冷启动用户之间的偏好相关性，从而通过将用户特定的偏好和共同的偏好相关性映射到预测概率分布来产生更强大的映射功能。此外，我们还引入了偏好余数来增强重叠用户的共同偏好，最后设计了一个具有偏好调制的自适应条件解码器来对目标域内的冷启动用户进行预测。实验结果表明，在三种实际的 CDR 场景下，CDRNP 方法的性能优于以往的 SOTA 方法。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CDRNP:+Cross-Domain+Recommendation+to+Cold-Start+Users+via+Neural+Process)|0|
|[Calibration-compatible Listwise Distillation of Privileged Features for CTR Prediction](https://doi.org/10.1145/3616855.3635810)|Xiaoqiang Gui, Yueyao Cheng, XiangRong Sheng, Yunfeng Zhao, Guoxian Yu, Shuguang Han, Yuning Jiang, Jian Xu, Bo Zheng||In machine learning systems, privileged features refer to the features that
are available during offline training but inaccessible for online serving.
Previous studies have recognized the importance of privileged features and
explored ways to tackle online-offline discrepancies. A typical practice is
privileged features distillation (PFD): train a teacher model using all
features (including privileged ones) and then distill the knowledge from the
teacher model using a student model (excluding the privileged features), which
is then employed for online serving. In practice, the pointwise cross-entropy
loss is often adopted for PFD. However, this loss is insufficient to distill
the ranking ability for CTR prediction. First, it does not consider the
non-i.i.d. characteristic of the data distribution, i.e., other items on the
same page significantly impact the click probability of the candidate item.
Second, it fails to consider the relative item order ranked by the teacher
model's predictions, which is essential to distill the ranking ability. To
address these issues, we first extend the pointwise-based PFD to the
listwise-based PFD. We then define the calibration-compatible property of
distillation loss and show that commonly used listwise losses do not satisfy
this property when employed as distillation loss, thus compromising the model's
calibration ability, which is another important measure for CTR prediction. To
tackle this dilemma, we propose Calibration-compatible LIstwise Distillation
(CLID), which employs carefully-designed listwise distillation loss to achieve
better ranking ability than the pointwise-based PFD while preserving the
model's calibration ability. We theoretically prove it is
calibration-compatible. Extensive experiments on public datasets and a
production dataset collected from the display advertising system of Alibaba
further demonstrate the effectiveness of CLID.|在机器学习系统中，特权特性是指在离线培训期间可用但在线服务无法访问的特性。以前的研究已经认识到特权功能的重要性，并探索了解决在线和离线差异的方法。一个典型的实践是特权特征提取(PFD) : 使用所有特征(包括特权特征)训练一个教师模型，然后使用学生模型(不包括特权特征)从教师模型中提取知识，然后用于在线服务。在实际应用中，PFD 通常采用逐点交叉熵损失。然而，这种损失不足以提取 CTR 预测的排名能力。首先，它没有考虑数据分布的非 ID 特征，也就是说，同一页面上的其他条目显著影响候选条目的点击概率。其次，没有考虑教师模型预测排名的相对项目顺序，这对提取排名能力至关重要。为了解决这些问题，我们首先将基于点的 PFD 扩展到基于列表的 PFD。然后定义了蒸馏损失的标定相容性，指出常用的列表损失作为蒸馏损失时不能满足这一性质，从而影响了模型的标定能力，这是 CTR 预测的另一个重要措施。为了解决这一难题，我们提出了与标定兼容的列表蒸馏(CLID)模型，该模型采用精心设计的列表蒸馏损耗来获得比基于点的 PFD 更好的排序能力，同时保留了模型的标定能力。我们从理论上证明了它是校准兼容的。从阿里巴巴的显示广告系统收集的公共数据集和生产数据集进行了广泛的实验，进一步证明了 CLID 的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Calibration-compatible+Listwise+Distillation+of+Privileged+Features+for+CTR+Prediction)|0|
|[Ranking with Long-Term Constraints](https://doi.org/10.1145/3616855.3635819)|Kianté Brantley, Zhichong Fang, Sarah Dean, Thorsten Joachims||The feedback that users provide through their choices (e.g., clicks, purchases) is one of the most common types of data readily available for training search and recommendation algorithms. However, myopically training systems based on choice data may only improve short-term engagement, but not the long-term sustainability of the platform and the long-term benefits to its users, content providers, and other stakeholders. In this paper, we thus develop a new framework in which decision makers (e.g., platform operators, regulators, users) can express long-term goals for the behavior of the platform (e.g., fairness, revenue distribution, legal requirements). These goals take the form of exposure or impact targets that go well beyond individual sessions, and we provide new control-based algorithms to achieve these goals. In particular, the controllers are designed to achieve the stated long-term goals with minimum impact on short-term engagement. Beyond the principled theoretical derivation of the controllers, we evaluate the algorithms on both synthetic and real-world data. While all controllers perform well, we find that they provide interesting trade-offs in efficiency, robustness, and the ability to plan ahead.|用户通过他们的选择(例如点击，购买)提供的反馈是最常见的数据类型之一，可用于培训搜索和推荐算法。然而，基于选择数据的近视培训系统只能改善短期参与，而不能改善平台的长期可持续性及其用户、内容提供者和其他利益相关者的长期利益。在本文中，我们提出了一个新的框架，在这个框架中决策者(例如，平台运营商，监管者，用户)可以表达平台行为的长期目标(例如，公平性，收入分配，法律要求)。这些目标采取暴露或影响目标的形式，远远超出了单个会话的范围，我们提供了新的基于控制的算法来实现这些目标。特别是，控制器的设计是为了实现既定的长期目标，对短期参与的影响最小。除了对控制器进行原理性的理论推导外，我们还对这些算法进行了合成数据和实际数据的评估。虽然所有控制器都表现良好，但我们发现它们在效率、健壮性和提前计划能力方面提供了有趣的权衡。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ranking+with+Long-Term+Constraints)|0|
|[Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights](https://doi.org/10.1145/3616855.3635805)|Junchen Fu, Fajie Yuan, Yu Song, Zheng Yuan, Mingyue Cheng, Shenghui Cheng, Jiaqi Zhang, Jie Wang, Yunzhu Pan||Adapters, a plug-in neural network module with some tunable parameters, have emerged as a parameter-efficient transfer learning technique for adapting pre-trained models to downstream tasks, especially for natural language processing (NLP) and computer vision (CV) fields. Meanwhile, learning recommendation models directly from raw item modality features -- e.g., texts of NLP and images of CV -- can enable effective and transferable recommender systems (called TransRec). In view of this, a natural question arises: can adapter-based learning techniques achieve parameter-efficient TransRec with good performance?   To this end, we perform empirical studies to address several key sub-questions. First, we ask whether the adapter-based TransRec performs comparably to TransRec based on standard full-parameter fine-tuning? does it hold for recommendation with different item modalities, e.g., textual RS and visual RS. If yes, we benchmark these existing adapters, which have been shown to be effective in NLP and CV tasks, in the item recommendation settings. Third, we carefully study several key factors for the adapter-based TransRec in terms of where and how to insert these adapters? Finally, we look at the effects of adapter-based TransRec by either scaling up its source training data or scaling down its target training data. Our paper provides key insights and practical guidance on unified & transferable recommendation -- a less studied recommendation scenario. We promise to release all code & datasets for future research.|适配器是一种具有可调参数的插入式神经网络模块，已经成为一种参数高效的传递学习技术，用于将预先训练好的模型适应下游任务，特别是在自然语言处理(NLP)和计算机视觉(CV)领域。同时，直接从原始项目模式特征(如 NLP 文本和简历图像)学习推荐模型，可以实现有效和可转移的推荐系统(称为 TransRec)。有鉴于此，一个自然而然的问题出现了: 基于适配器的学习技术能否以良好的性能实现参数有效的 TransRec？为此，我们进行了实证研究，以解决几个关键的子问题。首先，我们询问基于适配器的 TransRec 是否与基于标准全参数微调的 TransRec 性能相当？它是否适用于不同项目模式的推荐，例如，文本 RS 和视觉 RS。如果是，我们基准测试这些现有的适配器，已被证明是有效的自然语言处理和简历任务，在项目推荐设置。第三，我们仔细研究了基于适配器的 TransRec 在何处以及如何插入这些适配器方面的几个关键因素？最后，我们通过扩展源训练数据或缩小目标训练数据来研究基于适配器的 TransRec 的效果。我们的论文提供了关于统一和可转移推荐的关键见解和实践指导——一个研究较少的推荐场景。我们承诺为未来的研究发布所有的代码和数据集。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploring+Adapter-based+Transfer+Learning+for+Recommender+Systems:+Empirical+Studies+and+Practical+Insights)|0|
|[PEACE: Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation](https://doi.org/10.1145/3616855.3635781)|Chunjing Gan, Bo Huang, Binbin Hu, Jian Ma, Zhiqiang Zhang, Jun Zhou, Guannan Zhang, Wenliang Zhong||To help merchants/customers to provide/access a variety of services through miniapps, online service platforms have occupied a critical position in the effective content delivery, in which how to recommend items in the new domain launched by the service provider for customers has become more urgent. However, the non-negligible gap between the source and diversified target domains poses a considerable challenge to cross-domain recommendation systems, which often leads to performance bottlenecks in industrial settings. While entity graphs have the potential to serve as a bridge between domains, rudimentary utilization still fail to distill useful knowledge and even induce the negative transfer issue. To this end, we propose PEACE, a Prototype lEarning Augmented transferable framework for Cross-domain rEcommendation. For domain gap bridging, PEACE is built upon a multi-interest and entity-oriented pre-training architecture which could not only benefit the learning of generalized knowledge in a multi-granularity manner, but also help leverage more structural information in the entity graph. Then, we bring the prototype learning into the pre-training over source domains, so that representations of users and items are greatly improved by the contrastive prototype learning module and the prototype enhanced attention mechanism for adaptive knowledge utilization. To ease the pressure of online serving, PEACE is carefully deployed in a lightweight manner, and significant performance improvements are observed in both online and offline environments.|为协助商户/客户透过迷你应用程式提供/使用多元化的服务，网上服务平台在有效提供内容方面占有重要地位，因此如何向客户推荐服务供应商推出的新界别项目，已变得更为迫切。然而，来源域和多样化目标域之间不可忽视的差距对跨域推荐系统提出了相当大的挑战，这往往导致行业环境中的性能瓶颈。虽然实体图有可能作为领域之间的桥梁，但基本的利用仍然不能提取有用的知识，甚至引起负迁移问题。为此，我们提出了一个面向跨域推荐的原型学习增强可转移框架 PEACE。为了缩小领域间的差距，PEACE 建立在一个多兴趣和面向实体的预培训架构之上，它不仅有利于以多粒度方式学习广义知识，而且有助于利用实体图中更多的结构信息。然后，将原型学习引入到源域上的预训练中，通过对比原型学习模块和原型增强注意机制对用户和项目进行自适应知识利用，从而大大改善了用户和项目的表示。为了减轻在线服务的压力，PEACE 以一种轻量级的方式精心部署，并且在两种在线和离线环境中都观察到了显著的性能改善。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PEACE:+Prototype+lEarning+Augmented+transferable+framework+for+Cross-domain+rEcommendation)|0|
|[Motif-based Prompt Learning for Universal Cross-domain Recommendation](https://doi.org/10.1145/3616855.3635754)|Bowen Hao, Chaoqun Yang, Lei Guo, Junliang Yu, Hongzhi Yin||Cross-Domain Recommendation (CDR) stands as a pivotal technology addressing issues of data sparsity and cold start by transferring general knowledge from the source to the target domain. However, existing CDR models suffer limitations in adaptability across various scenarios due to their inherent complexity. To tackle this challenge, recent advancements introduce universal CDR models that leverage shared embeddings to capture general knowledge across domains and transfer it through "Multi-task Learning" or "Pre-train, Fine-tune" paradigms. However, these models often overlook the broader structural topology that spans domains and fail to align training objectives, potentially leading to negative transfer. To address these issues, we propose a motif-based prompt learning framework, MOP, which introduces motif-based shared embeddings to encapsulate generalized domain knowledge, catering to both intra-domain and inter-domain CDR tasks. Specifically, we devise three typical motifs: butterfly, triangle, and random walk, and encode them through a Motif-based Encoder to obtain motif-based shared embeddings. Moreover, we train MOP under the "Pre-training \& Prompt Tuning" paradigm. By unifying pre-training and recommendation tasks as a common motif-based similarity learning task and integrating adaptable prompt parameters to guide the model in downstream recommendation tasks, MOP excels in transferring domain knowledge effectively. Experimental results on four distinct CDR tasks demonstrate the effectiveness of MOP than the state-of-the-art models.|跨域推荐(CDR)是解决数据稀疏性和冷启动问题的关键技术，它通过将一般知识从数据源转移到目标域来实现。然而，现有的 CDR 模型由于其固有的复杂性，在不同场景的适应性方面存在局限性。为了应对这一挑战，最近的进展引入了通用的 CDR 模型，利用共享嵌入来获取跨领域的一般知识，并通过“多任务学习”或“预训练，微调”范例进行转移。然而，这些模型往往忽视了更广泛的结构拓扑，跨越领域和未能调整培训目标，潜在地导致负迁移。为了解决这些问题，我们提出了一个基于主题的快速学习框架 MOP，它引入了基于主题的共享嵌入来封装广义领域知识，同时满足领域内和领域间的 CDR 任务。具体来说，我们设计了三种典型的主题: 蝴蝶、三角形和随机游走，并通过基于主题的编码器对它们进行编码，以获得基于主题的共享嵌入。此外，我们训练 MOP 在“预先训练和即时调整”的范式。通过将预训练任务和推荐任务统一为基于主题的相似性学习任务，并结合自适应提示参数对下游推荐任务的模型进行指导，MOP 在领域知识的有效传递方面表现突出。在四个不同的 CDR 任务上的实验结果证明了 MOP 的有效性，而不是最先进的模型。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Motif-based+Prompt+Learning+for+Universal+Cross-domain+Recommendation)|0|
|[C²DR: Robust Cross-Domain Recommendation based on Causal Disentanglement](https://doi.org/10.1145/3616855.3635809)|Menglin Kong, Jia Wang, Yushan Pan, Haiyang Zhang, Muzhou Hou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=C²DR:+Robust+Cross-Domain+Recommendation+based+on+Causal+Disentanglement)|0|
|[Inverse Learning with Extremely Sparse Feedback for Recommendation](https://doi.org/10.1145/3616855.3635797)|Guanyu Lin, Chen Gao, Yu Zheng, Yinfeng Li, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, Zhiheng Li, Depeng Jin, Yong Li||Modern personalized recommendation services often rely on user feedback,
either explicit or implicit, to improve the quality of services. Explicit
feedback refers to behaviors like ratings, while implicit feedback refers to
behaviors like user clicks. However, in the scenario of full-screen video
viewing experiences like Tiktok and Reels, the click action is absent,
resulting in unclear feedback from users, hence introducing noises in modeling
training. Existing approaches on de-noising recommendation mainly focus on
positive instances while ignoring the noise in a large amount of sampled
negative feedback. In this paper, we propose a meta-learning method to annotate
the unlabeled data from loss and gradient perspectives, which considers the
noises in both positive and negative instances. Specifically, we first propose
an Inverse Dual Loss (IDL) to boost the true label learning and prevent the
false label learning. Then we further propose an Inverse Gradient (IG) method
to explore the correct updating gradient and adjust the updating based on
meta-learning. Finally, we conduct extensive experiments on both benchmark and
industrial datasets where our proposed method can significantly improve AUC by
9.25% against state-of-the-art methods. Further analysis verifies the proposed
inverse learning framework is model-agnostic and can improve a variety of
recommendation backbones. The source code, along with the best hyper-parameter
settings, is available at this link:
https://github.com/Guanyu-Lin/InverseLearning.|现代个性化推荐服务通常依赖于用户的反馈，无论是显性的还是隐性的，以提高服务质量。显式反馈指的是评分等行为，而隐式反馈指的是用户点击等行为。然而，在像 Tiktok 和 Reels 这样的全屏视频观看体验的场景中，没有点击动作，导致用户的反馈不清晰，因此在建模训练中引入了噪音。现有的去噪推荐方法主要集中在正向实例上，而忽略了大量采样负反馈中的噪声。本文提出了一种从损失和梯度角度对未标记数据进行标注的元学习方法，该方法同时考虑了正负两种情况下的噪声。具体地说，我们首先提出了一种逆双损耗(IDL)算法来提高真标记学习的能力，防止虚标记学习。然后进一步提出了一种基于元学习的逆梯度(IG)方法来探索正确的更新梯度，并对更新进行调整。最后，我们在基准和工业数据集上进行了广泛的实验，其中我们提出的方法与最先进的方法相比，AUC 可以显著提高9.25% 。进一步的分析验证了所提出的逆向学习框架是模型不可知的，可以改善各种推荐骨干。源代码，以及最好的超参数设置，可在以下连结找到:  https://github.com/guanyu-lin/inverselearning。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Inverse+Learning+with+Extremely+Sparse+Feedback+for+Recommendation)|0|
|[Pre-trained Recommender Systems: A Causal Debiasing Perspective](https://doi.org/10.1145/3616855.3635779)|Ziqian Lin, Hao Ding, Trong Nghia Hoang, Branislav Kveton, Anoop Deoras, Hao Wang||Recent studies on pre-trained vision/language models have demonstrated the
practical benefit of a new, promising solution-building paradigm in AI where
models can be pre-trained on broad data describing a generic task space and
then adapted successfully to solve a wide range of downstream tasks, even when
training data is severely limited (e.g., in zero- or few-shot learning
scenarios). Inspired by such progress, we investigate in this paper the
possibilities and challenges of adapting such a paradigm to the context of
recommender systems, which is less investigated from the perspective of
pre-trained model. In particular, we propose to develop a generic recommender
that captures universal interaction patterns by training on generic user-item
interaction data extracted from different domains, which can then be fast
adapted to improve few-shot learning performance in unseen new domains (with
limited data).
  However, unlike vision/language data which share strong conformity in the
semantic space, universal patterns underlying recommendation data collected
across different domains (e.g., different countries or different E-commerce
platforms) are often occluded by both in-domain and cross-domain biases
implicitly imposed by the cultural differences in their user and item bases, as
well as their uses of different e-commerce platforms. As shown in our
experiments, such heterogeneous biases in the data tend to hinder the
effectiveness of the pre-trained model. To address this challenge, we further
introduce and formalize a causal debiasing perspective, which is substantiated
via a hierarchical Bayesian deep learning model, named PreRec. Our empirical
studies on real-world data show that the proposed model could significantly
improve the recommendation performance in zero- and few-shot learning settings
under both cross-market and cross-platform scenarios.|最近关于预先训练的视觉/语言模型的研究已经证明了人工智能中新的，有希望的解决方案构建范式的实际益处，其中模型可以在描述通用任务空间的广泛数据上预先训练，然后成功地适应于解决广泛的下游任务，即使训练数据受到严重限制(例如，在零或少数镜头学习场景中)。受到这些进展的启发，本文研究了将这种模式应用到推荐系统中的可能性和挑战，而从预训练模型的角度对这种可能性和挑战的研究较少。具体而言，我们建议开发一个通用推荐器，通过对从不同领域提取的通用用户项交互数据进行训练来捕获通用交互模式，然后可以快速适应以改善在未见的新领域(具有有限的数据)中的少镜头学习性能。然而，与在语义空间中共享强烈一致性的视觉/语言数据不同，跨不同领域(例如，不同国家或不同电子商务平台)收集的推荐数据的普遍模式通常被隐含地由其用户和项目基础的文化差异所施加的领域内和跨领域偏见所遮蔽，以及他们对不同电子商务平台的使用。正如我们的实验所显示的，这种异质偏差的数据往往会阻碍预训练模型的有效性。为了应对这一挑战，我们进一步引入并形式化了一个因果消偏的观点，这是通过一个分层贝叶斯深度学习模型，命名为 PreRec。我们对实际数据的实证研究表明，在跨市场和跨平台的情况下，该模型可以显著提高零镜头和少镜头学习环境下的推荐性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pre-trained+Recommender+Systems:+A+Causal+Debiasing+Perspective)|0|
|[Interact with the Explanations: Causal Debiased Explainable Recommendation System](https://doi.org/10.1145/3616855.3635855)|Xu Liu, Tong Yu, Kaige Xie, Junda Wu, Shuai Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interact+with+the+Explanations:+Causal+Debiased+Explainable+Recommendation+System)|0|
|[Proxy-based Item Representation for Attribute and Context-aware Recommendation](https://doi.org/10.1145/3616855.3635824)|Jinseok Seol, Minseok Gang, Sanggoo Lee, Jaehui Park||Neural network approaches in recommender systems have shown remarkable
success by representing a large set of items as a learnable vector embedding
table. However, infrequent items may suffer from inadequate training
opportunities, making it difficult to learn meaningful representations. We
examine that in attribute and context-aware settings, the poorly learned
embeddings of infrequent items impair the recommendation accuracy. To address
such an issue, we propose a proxy-based item representation that allows each
item to be expressed as a weighted sum of learnable proxy embeddings. Here, the
proxy weight is determined by the attributes and context of each item and may
incorporate bias terms in case of frequent items to further reflect
collaborative signals. The proxy-based method calculates the item
representations compositionally, ensuring each representation resides inside a
well-trained simplex and, thus, acquires guaranteed quality. Additionally, that
the proxy embeddings are shared across all items allows the infrequent items to
borrow training signals of frequent items in a unified model structure and
end-to-end manner. Our proposed method is a plug-and-play model that can
replace the item encoding layer of any neural network-based recommendation
model, while consistently improving the recommendation performance with much
smaller parameter usage. Experiments conducted on real-world recommendation
benchmark datasets demonstrate that our proposed model outperforms
state-of-the-art models in terms of recommendation accuracy by up to 17% while
using only 10% of the parameters.|推荐系统中的神经网络方法通过将大量的项目表示为一个可学习的向量嵌入表，取得了显著的成功。然而，不经常学习的项目可能会受到培训机会不足的影响，从而难以学习有意义的表征。我们研究了在属性和上下文感知的设置中，不常见项目的嵌入会影响推荐的准确性。为了解决这个问题，我们提出了一种基于代理的项表示方法，该方法允许每个项表示为可学习代理嵌入的加权和。在这里，代理权重是由每个项目的属性和上下文决定的，并且在频繁项目的情况下可以加入偏倚项，以进一步反映协作信号。基于代理的方法组合计算项表示，确保每个表示驻留在一个训练有素的单纯形内，从而获得有保证的质量。此外，代理嵌入在所有项目之间共享，允许频繁项目以统一的模型结构和端到端的方式借用频繁项目的训练信号。我们提出的方法是即插即用模型，可以取代任何基于神经网络的推荐模型的项目编码层，同时以更小的参数使用率持续改善推荐性能。在真实世界的推荐基准数据集上进行的实验表明，我们提出的模型在推荐准确率方面比最先进的模型高出17% ，而只使用了10% 的参数。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Proxy-based+Item+Representation+for+Attribute+and+Context-aware+Recommendation)|0|
|[LEAD: Liberal Feature-based Distillation for Dense Retrieval](https://doi.org/10.1145/3616855.3635774)|Hao Sun, Xiao Liu, Yeyun Gong, Anlei Dong, Jingwen Lu, Yan Zhang, Linjun Yang, Rangan Majumder, Nan Duan||Knowledge distillation is often used to transfer knowledge from a strong teacher model to a relatively weak student model. Traditional knowledge distillation methods include response-based methods and feature-based methods. Response-based methods are used the most widely but suffer from lower upper limit of model performance, while feature-based methods have constraints on the vocabularies and tokenizers. In this paper, we propose a tokenizer-free method liberal feature-based distillation (LEAD). LEAD aligns the distribution between teacher model and student model, which is effective, extendable, portable and has no requirements on vocabularies, tokenizer, or model architecture. Extensive experiments show the effectiveness of LEAD on several widely-used benchmarks, including MS MARCO Passage, TREC Passage 19, TREC Passage 20, MS MARCO Document, TREC Document 19 and TREC Document 20.|知识提取经常被用来将知识从一个强教师模型转移到一个相对弱的学生模型。传统的知识提取方法包括基于响应的方法和基于特征的方法。基于响应的方法应用最广泛，但模型性能的上限较低，而基于特征的方法对词汇和标记有限制。本文提出了一种基于特征的自由精馏算法(LEAD)。LEAD 调整了教师模型和学生模型之间的分布，它是有效的、可扩展的、可移植的，并且对词汇表、标记器或模型架构没有要求。广泛的实验显示了 LEAD 在几个广泛使用的基准上的有效性，包括 MS MARCO Passage，TREC Passage 19，TREC Passage 20，MS MARCO Document，TREC Document 19和 TREC Document 20。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LEAD:+Liberal+Feature-based+Distillation+for+Dense+Retrieval)|0|
|[Not All Negatives Are Worth Attending to: Meta-Bootstrapping Negative Sampling Framework for Link Prediction](https://doi.org/10.1145/3616855.3635840)|Yakun Wang, Binbin Hu, Shuo Yang, Meiqi Zhu, Zhiqiang Zhang, Qiyang Zhang, Jun Zhou, Guo Ye, Huimei He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Not+All+Negatives+Are+Worth+Attending+to:+Meta-Bootstrapping+Negative+Sampling+Framework+for+Link+Prediction)|0|
|[Diff-MSR: A Diffusion Model Enhanced Paradigm for Cold-Start Multi-Scenario Recommendation](https://doi.org/10.1145/3616855.3635807)|Yuhao Wang, Ziru Liu, Yichao Wang, Xiangyu Zhao, Bo Chen, Huifeng Guo, Ruiming Tang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Diff-MSR:+A+Diffusion+Model+Enhanced+Paradigm+for+Cold-Start+Multi-Scenario+Recommendation)|0|
|[On the Effectiveness of Unlearning in Session-Based Recommendation](https://doi.org/10.1145/3616855.3635823)|Xin Xin, Liu Yang, Ziqi Zhao, Pengjie Ren, Zhumin Chen, Jun Ma, Zhaochun Ren||Session-based recommendation predicts users' future interests from previous
interactions in a session. Despite the memorizing of historical samples, the
request of unlearning, i.e., to remove the effect of certain training samples,
also occurs for reasons such as user privacy or model fidelity. However,
existing studies on unlearning are not tailored for the session-based
recommendation. On the one hand, these approaches cannot achieve satisfying
unlearning effects due to the collaborative correlations and sequential
connections between the unlearning item and the remaining items in the session.
On the other hand, seldom work has conducted the research to verify the
unlearning effectiveness in the session-based recommendation scenario. In this
paper, we propose SRU, a session-based recommendation unlearning framework,
which enables high unlearning efficiency, accurate recommendation performance,
and improved unlearning effectiveness in session-based recommendation.
Specifically, we first partition the training sessions into separate sub-models
according to the similarity across the sessions, then we utilize an
attention-based aggregation layer to fuse the hidden states according to the
correlations between the session and the centroid of the data in the sub-model.
To improve the unlearning effectiveness, we further propose three extra data
deletion strategies, including collaborative extra deletion (CED), neighbor
extra deletion (NED), and random extra deletion (RED). Besides, we propose an
evaluation metric that measures whether the unlearning sample can be inferred
after the data deletion to verify the unlearning effectiveness. We implement
SRU with three representative session-based recommendation models and conduct
experiments on three benchmark datasets. Experimental results demonstrate the
effectiveness of our methods.|基于会话的推荐可以根据会话中以前的交互预测用户的未来兴趣。除了对历史样本的记忆，忘记的要求，即去除某些训练样本的影响，也出于用户隐私或模型保真等原因。然而，现有的关于忘却的研究并不适合基于会话的建议。一方面，这些方法不能达到令人满意的忘却效果，因为在忘却项目和会话中剩余项目之间存在协作关联和顺序关联。另一方面，在基于会话的推荐场景中，很少有研究验证学习效果。本文提出了一种基于会话的推荐去学习框架 SRU，该框架具有较高的去学习效率、较准确的推荐性能和较高的去学习效率。具体来说，我们首先根据训练会话间的相似性将训练会话划分为不同的子模型，然后利用基于注意力的聚合层根据会话与子模型中数据质心的相关性对隐藏状态进行融合。为了提高学习效率，我们进一步提出了三种额外的数据删除策略，包括协作额外删除(CED)、邻居额外删除(NED)和随机额外删除(RED)。此外，我们提出一个评估指标，测量在删除数据后是否可以推断出忘却样本，以验证忘却的有效性。我们使用三个具有代表性的基于会话的推荐模型实现 SRU，并在三个基准数据集上进行实验。实验结果证明了该方法的有效性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=On+the+Effectiveness+of+Unlearning+in+Session-Based+Recommendation)|0|
|[IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling](https://doi.org/10.1145/3616855.3635699)|Nolwenn Bernard, Ivica Kostric, Krisztian Balog||While interest in conversational recommender systems has been on the rise,
operational systems suitable for serving as research platforms for
comprehensive studies are currently lacking. This paper introduces an enhanced
version of the IAI MovieBot conversational movie recommender system, aiming to
evolve it into a robust and adaptable platform for conducting user-facing
experiments. The key highlights of this enhancement include the addition of
trainable neural components for natural language understanding and dialogue
policy, transparent and explainable modeling of user preferences, along with
improvements in the user interface and research infrastructure.|虽然对会话推荐系统的兴趣一直在上升，但目前还缺乏适合作为综合研究的研究平台的操作系统。本文介绍了 IAI MovieBot 会话电影推荐系统的一个增强版本，旨在将其发展成一个健壮的、可适应的平台，用于进行面向用户的实验。这一增强的主要亮点包括增加了可训练的神经组件，用于自然语言理解和对话政策，对用户偏好进行透明和可解释的建模，以及改进用户界面和研究基础设施。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IAI+MovieBot+2.0:+An+Enhanced+Research+Platform+with+Trainable+Neural+Components+and+Transparent+User+Modeling)|0|
|[Domain Level Interpretability: Interpreting Black-box Model with Domain-specific Embedding](https://doi.org/10.1145/3616855.3635688)|YaLin Zhang, Caizhi Tang, Lu Yu, Jun Zhou, Longfei Li, Qing Cui, Fangfang Fan, Linbo Jiang, Xiaosong Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Domain+Level+Interpretability:+Interpreting+Black-box+Model+with+Domain-specific+Embedding)|0|
|[Unbiased Learning to Rank: On Recent Advances and Practical Applications](https://doi.org/10.1145/3616855.3636451)|Shashank Gupta, Philipp Hager, Jin Huang, Ali Vardasbi, Harrie Oosterhuis||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unbiased+Learning+to+Rank:+On+Recent+Advances+and+Practical+Applications)|0|
|[Leveraging User Simulation to Develop and Evaluate Conversational Information Access Agents](https://doi.org/10.1145/3616855.3635730)|Nolwenn Bernard||We observe a change in the way users access information, that is, the rise of
conversational information access (CIA) agents. However, the automatic
evaluation of these agents remains an open challenge. Moreover, the training of
CIA agents is cumbersome as it mostly relies on conversational corpora, expert
knowledge, and reinforcement learning. User simulation has been identified as a
promising solution to tackle automatic evaluation and has been previously used
in reinforcement learning. In this research, we investigate how user simulation
can be leveraged in the context of CIA. We organize the work in three parts. We
begin with the identification of requirements for user simulators for training
and evaluating CIA agents and compare existing types of simulator regarding
these. Then, we plan to combine these different types of simulators into a new
hybrid simulator. Finally, we aim to extend simulators to handle more complex
information seeking scenarios.|我们观察到用户访问信息的方式发生了变化，即会话信息访问(CIA)代理的兴起。然而，这些代理的自动评估仍然是一个开放的挑战。此外，中情局特工的培训非常繁琐，因为它主要依赖于会话语料库、专业知识和强化学习。用户模拟已被确定为解决自动评估的一个有前途的解决方案，并且以前在强化学习中使用过。在这项研究中，我们调查如何用户模拟可以在中央情报局的背景下利用。我们把工作分为三部分。我们首先确定用户模拟器的培训和评估 CIA 代理的需求，并比较现有的模拟器类型。然后，我们计划将这些不同类型的模拟器组合成一个新的混合模拟器。最后，我们的目标是扩展模拟器以处理更复杂的信息搜索场景。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+User+Simulation+to+Develop+and+Evaluate+Conversational+Information+Access+Agents)|0|
|[Delphic Costs and Benefits in Web Search: A Utilitarian and Historical Analysis](https://doi.org/10.1145/3616855.3638208)|Andrei Z. Broder||We present a new framework to conceptualize and operationalize the total user experience of search, by studying the entirety of a search journey from an utilitarian point of view. Web search engines are widely perceived as "free". But search requires time and effort: in reality there are many intermingled non-monetary costs (e.g. time costs, cognitive costs, interactivity costs) and the benefits may be marred by various impairments, such as misunderstanding and misinformation. This characterization of costs and benefits appears to be inherent to the human search for information within the pursuit of some larger task: most of the costs and impairments can be identified in interactions with any web search engine, interactions with public libraries, and even in interactions with ancient oracles. To emphasize this innate connection, we call these costs and benefits Delphic, in contrast to explicitly financial costs and benefits. Our main thesis is that the users' satisfaction with a search engine mostly depends on their experience of Delphic cost and benefits, in other words on their utility. The consumer utility is correlated with classic measures of search engine quality, such as ranking, precision, recall, etc., but is not completely determined by them. To argue our thesis, we catalog the Delphic costs and benefits and show how the development of search engines over the last quarter century, from classic Information Retrieval roots to the integration of Large Language Models, was driven to a great extent by the quest of decreasing Delphic costs and increasing Delphic benefits. We hope that the Delphic costs framework will engender new ideas and new research for evaluating and improving the web experience for everyone.|我们提出了一个新的框架来概念化和操作的总体用户体验的搜索，通过研究整个搜索旅程从功利的角度来看。网络搜索引擎被广泛认为是“免费的”。但是搜索需要时间和精力: 在现实中有许多混合的非货币成本(例如时间成本、认知成本、互动成本) ，好处可能被各种损害所破坏，例如误解和错误信息。这种成本和收益的角色塑造似乎是人类在追求某种更大的任务时所固有的信息搜索: 大多数成本和损害可以通过与任何网络搜索引擎的互动、与公共图书馆的互动、甚至与古代神谕的互动来识别。为了强调这种内在的联系，我们把这些成本和收益称为德尔菲，与明确的财务成本和收益形成对比。我们的主要论点是，用户对搜索引擎的满意度主要取决于他们对德尔菲成本和收益的体验，换句话说，取决于他们的实用性。消费者效用与搜索引擎质量的经典指标相关，如排名、精确度、召回等，但并不完全由它们决定。为了证明我们的论点，我们列出了德尔菲的成本和收益，并展示了在过去25年里搜索引擎的发展，从传统的信息检索到大型语言模型的整合，在很大程度上是由降低德尔菲成本和增加德尔菲收益的追求所驱动的。我们希望德尔菲成本框架将产生新的想法和新的研究，以评估和改善每个人的网络体验。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Delphic+Costs+and+Benefits+in+Web+Search:+A+Utilitarian+and+Historical+Analysis)|0|
|[The Journey to A Knowledgeable Assistant with Retrieval-Augmented Generation (RAG)](https://doi.org/10.1145/3616855.3638207)|Xin Luna Dong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Journey+to+A+Knowledgeable+Assistant+with+Retrieval-Augmented+Generation+(RAG))|0|
|[LabelCraft: Empowering Short Video Recommendations with Automated Label Crafting](https://doi.org/10.1145/3616855.3635816)|Yimeng Bai, Yang Zhang, Jing Lu, Jianxin Chang, Xiaoxue Zang, Yanan Niu, Yang Song, Fuli Feng||Short video recommendations often face limitations due to the quality of user feedback, which may not accurately depict user interests. To tackle this challenge, a new task has emerged: generating more dependable labels from original feedback. Existing label generation methods rely on manual rules, demanding substantial human effort and potentially misaligning with the desired objectives of the platform. To transcend these constraints, we introduce LabelCraft, a novel automated label generation method explicitly optimizing pivotal operational metrics for platform success. By formulating label generation as a higher-level optimization problem above recommender model optimization, LabelCraft introduces a trainable labeling model for automatic label mechanism modeling. Through meta-learning techniques, LabelCraft effectively addresses the bi-level optimization hurdle posed by the recommender and labeling models, enabling the automatic acquisition of intricate label generation mechanisms.Extensive experiments on real-world datasets corroborate LabelCraft's excellence across varied operational metrics, encompassing usage time, user engagement, and retention. Codes are available at https://github.com/baiyimeng/LabelCraft.|由于用户反馈的质量问题，短视频推荐往往面临着限制，这可能不能准确地描述用户的兴趣。为了应对这一挑战，一项新的任务出现了: 从原始反馈中生成更可靠的标签。现有的标签生成方法依赖于手工规则，需要大量的人工努力，并且可能与平台的期望目标不一致。为了超越这些约束，我们引入了 LabelCraft，一种新的自动标签生成方法，它显式地优化了平台成功的关键操作指标。通过将标签生成作为推荐模型优化之上的一个更高层次的最佳化问题，LabelCraft 为自动标签机制建模引入了一个可训练的标签模型。通过元学习技术，LabelCraft 有效地解决了推荐和标签模型造成的双层优化障碍，使得复杂的标签生成机制的自动获取成为可能。在真实世界数据集上的大量实验证实了 LabelCraft 在各种操作指标上的卓越性，包括使用时间、用户参与度和保持性。密码可在 https://github.com/baiyimeng/labelcraft 索取。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LabelCraft:+Empowering+Short+Video+Recommendations+with+Automated+Label+Crafting)|0|
|[Towards Mitigating Dimensional Collapse of Representations in Collaborative Filtering](https://doi.org/10.1145/3616855.3635832)|Huiyuan Chen, Vivian Lai, Hongye Jin, Zhimeng Jiang, Mahashweta Das, Xia Hu||Contrastive Learning (CL) has shown promising performance in collaborative
filtering. The key idea is to generate augmentation-invariant embeddings by
maximizing the Mutual Information between different augmented views of the same
instance. However, we empirically observe that existing CL models suffer from
the dimensional collapse issue, where user/item embeddings only span a
low-dimension subspace of the entire feature space. This suppresses other
dimensional information and weakens the distinguishability of embeddings. Here
we propose a non-contrastive learning objective, named nCL, which explicitly
mitigates dimensional collapse of representations in collaborative filtering.
Our nCL aims to achieve geometric properties of Alignment and
Compactness on the embedding space. In particular, the alignment tries
to push together representations of positive-related user-item pairs, while
compactness tends to find the optimal coding length of user/item embeddings,
subject to a given distortion. More importantly, our nCL does not require data
augmentation nor negative sampling during training, making it scalable to large
datasets. Experimental results demonstrate the superiority of our nCL.|对比学习(CL)在协同过滤方面表现出色。其核心思想是通过最大化同一实例中不同增强视图之间的互信息来产生增强不变嵌入。然而，我们经验地观察到现有的 CL 模型遭受维度折叠问题，其中用户/项目嵌入只跨越整个特征空间的一个低维子空间。这抑制了其他维度信息，削弱了嵌入的可区分性。在这里，我们提出了一个非对比学习目标，命名为 nCL，它明确地减轻了协同过滤表示的维度崩溃。我们的 nCL 旨在实现嵌入空间上的对齐性和紧性的几何性质。特别是，对齐试图将正相关的用户项对的表示推到一起，而紧凑性倾向于找到用户/项嵌入的最佳编码长度，受到给定的失真。更重要的是，我们的 nCL 在训练期间不需要数据增强或负采样，使其可扩展到大型数据集。实验结果证明了该方法的优越性。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Mitigating+Dimensional+Collapse+of+Representations+in+Collaborative+Filtering)|0|
|[CL4DIV: A Contrastive Learning Framework for Search Result Diversification](https://doi.org/10.1145/3616855.3635851)|Zhirui Deng, Zhicheng Dou, Yutao Zhu, JiRong Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CL4DIV:+A+Contrastive+Learning+Framework+for+Search+Result+Diversification)|0|
|[From Second to First: Mixed Censored Multi-Task Learning for Winning Price Prediction](https://doi.org/10.1145/3616855.3635838)|Jiani Huang, Zhenzhe Zheng, Yanrong Kang, Zixiao Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=From+Second+to+First:+Mixed+Censored+Multi-Task+Learning+for+Winning+Price+Prediction)|0|
|[DiffKG: Knowledge Graph Diffusion Model for Recommendation](https://doi.org/10.1145/3616855.3635850)|Yangqin Jiang, Yuhao Yang, Lianghao Xia, Chao Huang||Knowledge Graphs (KGs) have emerged as invaluable resources for enriching
recommendation systems by providing a wealth of factual information and
capturing semantic relationships among items. Leveraging KGs can significantly
enhance recommendation performance. However, not all relations within a KG are
equally relevant or beneficial for the target recommendation task. In fact,
certain item-entity connections may introduce noise or lack informative value,
thus potentially misleading our understanding of user preferences. To bridge
this research gap, we propose a novel knowledge graph diffusion model for
recommendation, referred to as DiffKG. Our framework integrates a generative
diffusion model with a data augmentation paradigm, enabling robust knowledge
graph representation learning. This integration facilitates a better alignment
between knowledge-aware item semantics and collaborative relation modeling.
Moreover, we introduce a collaborative knowledge graph convolution mechanism
that incorporates collaborative signals reflecting user-item interaction
patterns, guiding the knowledge graph diffusion process. We conduct extensive
experiments on three publicly available datasets, consistently demonstrating
the superiority of our DiffKG compared to various competitive baselines. We
provide the source code repository of our proposed DiffKG model at the
following link: https://github.com/HKUDS/DiffKG.|知识图(KGs)已经成为丰富推荐系统的宝贵资源，它提供了大量的事实信息，捕获了项目之间的语义关系。利用幼稚园可显著提升推荐表现。然而，并非所有幼儿园内部的关系对于目标推荐任务都同样相关或有益。事实上，某些项目-实体连接可能会引入噪音或缺乏信息价值，从而可能误导我们对用户偏好的理解。为了弥补这一研究差距，我们提出了一种新的知识图扩散推荐模型，称为迪夫 KG。我们的框架集成了一个生成扩散模型和一个数据增强范例，支持健壮的知识图表示学习。这种集成促进了知识感知项语义和协作关系建模之间的更好结合。此外，本文还引入了一种协同知识图卷积机制，该机制融合了反映用户-项目交互模式的协同信号，引导知识图的扩散过程。我们在三个公开可用的数据集上进行了广泛的实验，一致地证明了我们的 DiffKG 相对于各种竞争基线的优越性。我们在以下连结提供有关「区分幼稚园」模式的原始码储存库:  https://github.com/hkuds/DiffKG。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DiffKG:+Knowledge+Graph+Diffusion+Model+for+Recommendation)|0|
|[Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation](https://doi.org/10.1145/3616855.3635822)|Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum||Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.|基于知识图的会话问题回答模型通常在黄金问题回答对的基准上进行训练和测试。这意味着训练仅限于在各自的数据集中看到的表面形式，并且评估是针对一小组被拒绝的问题。通过我们提出的框架 REIGN，我们采取了几个步骤来补救这种受限制的学习设置。首先，我们系统地生成训练问题的重新编排，以增加模型对表面形式变化的鲁棒性。鉴于这些问题的不完整性，这是一个特别具有挑战性的问题。其次，我们使用深度强化学习，通过只提供那些有助于提高回答质量的重新编排来引导 convQA 模型获得更高的性能。第三，我们证明了在一个基准上训练主要模型组件并将它们应用到另一个基准上的可行性。最后，为了严格评估训练过的模型的鲁棒性，我们使用并发布了大量不同的重新编排，这些重新编排是通过提示 GPT 测试基准测试集(导致大小增加20倍)而产生的。我们的研究结果表明，使用强大的训练通过重新制定，严格质量保证模型，明显优于那些标准的训练，从黄金质量保证对。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Robust+Training+for+Conversational+Question+Answering+Models+with+Reinforced+Reformulation+Generation)|0|
|[MONET: Modality-Embracing Graph Convolutional Network and Target-Aware Attention for Multimedia Recommendation](https://doi.org/10.1145/3616855.3635817)|Yungi Kim, Taeri Kim, WonYong Shin, SangWook Kim||In this paper, we focus on multimedia recommender systems using graph
convolutional networks (GCNs) where the multimodal features as well as
user-item interactions are employed together. Our study aims to exploit
multimodal features more effectively in order to accurately capture users'
preferences for items. To this end, we point out following two limitations of
existing GCN-based multimedia recommender systems: (L1) although multimodal
features of interacted items by a user can reveal her preferences on items,
existing methods utilize GCN designed to focus only on capturing collaborative
signals, resulting in insufficient reflection of the multimodal features in the
final user/item embeddings; (L2) although a user decides whether to prefer the
target item by considering its multimodal features, existing methods represent
her as only a single embedding regardless of the target item's multimodal
features and then utilize her embedding to predict her preference for the
target item. To address the above issues, we propose a novel multimedia
recommender system, named MONET, composed of following two core ideas:
modality-embracing GCN (MeGCN) and target-aware attention. Through extensive
experiments using four real-world datasets, we demonstrate i) the significant
superiority of MONET over seven state-of-the-art competitors (up to 30.32%
higher accuracy in terms of recall@20, compared to the best competitor) and ii)
the effectiveness of the two core ideas in MONET. All MONET codes are available
at https://github.com/Kimyungi/MONET.|本文主要研究基于图卷积网络(GCNs)的多媒体推荐系统。我们的研究旨在更有效地利用多模态特征，以便准确地捕捉用户对项目的偏好。为此，我们指出了现有的基于 GCN 的多媒体推荐系统的两个局限性: (L1)尽管用户交互项目的多模态特征可以揭示她对项目的偏好，但是现有的方法利用 GCN 设计只关注于捕获协作信号，导致在最终用户/项目嵌入中对多模态特征的反映不足; (L2)尽管用户决定是否通过考虑其多模态特征来选择目标项目，但是现有的方法表示她只是一个单一的嵌入，而不管目标项目的多模态特征如何，然后利用她的嵌入来预测她对目标。为了解决上述问题，我们提出了一个新的多媒体推荐系统，命名为 MONET，它由以下两个核心思想组成: 包含模式的广域网(MeGCN)和目标感知注意。通过使用四个真实世界数据集的广泛实验，我们证明了 i) MONET 相对于七个最先进的竞争对手的显着优势(与最好的竞争对手相比，在召回@20方面高达30.32% 的准确性)和 ii) MONET 中两个核心思想的有效性。所有 MONET 代码均可在 https://github.com/kimyungi/MONET 下载。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MONET:+Modality-Embracing+Graph+Convolutional+Network+and+Target-Aware+Attention+for+Multimedia+Recommendation)|0|
|[Text-Video Retrieval via Multi-Modal Hypergraph Networks](https://doi.org/10.1145/3616855.3635757)|Qian Li, Lixin Su, Jiashu Zhao, Long Xia, Hengyi Cai, Suqi Cheng, Hengzhu Tang, Junfeng Wang, Dawei Yin||Text-video retrieval is a challenging task that aims to identify relevant
videos given textual queries. Compared to conventional textual retrieval, the
main obstacle for text-video retrieval is the semantic gap between the textual
nature of queries and the visual richness of video content. Previous works
primarily focus on aligning the query and the video by finely aggregating
word-frame matching signals. Inspired by the human cognitive process of
modularly judging the relevance between text and video, the judgment needs
high-order matching signal due to the consecutive and complex nature of video
contents. In this paper, we propose chunk-level text-video matching, where the
query chunks are extracted to describe a specific retrieval unit, and the video
chunks are segmented into distinct clips from videos. We formulate the
chunk-level matching as n-ary correlations modeling between words of the query
and frames of the video and introduce a multi-modal hypergraph for n-ary
correlation modeling. By representing textual units and video frames as nodes
and using hyperedges to depict their relationships, a multi-modal hypergraph is
constructed. In this way, the query and the video can be aligned in a
high-order semantic space. In addition, to enhance the model's generalization
ability, the extracted features are fed into a variational inference component
for computation, obtaining the variational representation under the Gaussian
distribution. The incorporation of hypergraphs and variational inference allows
our model to capture complex, n-ary interactions among textual and visual
contents. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance on the text-video retrieval task.|文本视频检索是一项具有挑战性的任务，其目的是识别给定文本查询的相关视频。与传统的文本检索相比，文本-视频检索的主要障碍是查询的文本性质与视频内容的视觉丰富性之间的语义差距。以前的工作主要集中在对齐查询和视频通过精细聚合字帧匹配信号。由于视频内容的连续性和复杂性，受人类对文本与视频相关性进行模块化判断的认知过程的启发，需要高阶匹配信号来进行判断。本文提出了块级文本-视频匹配算法，该算法提取查询块来描述特定的检索单元，并将视频块从视频中分割成不同的片段。将块级匹配表述为查询词与视频帧之间的 n 元相关建模，并引入多模态超图进行 n 元相关建模。通过将文本单元和视频帧表示为节点，利用超边界描述它们之间的关系，构造了一个多模态超图。通过这种方式，查询和视频可以在高阶语义空间中对齐。此外，为了提高模型的泛化能力，提取的特征被输入到一个变分推理组件中进行计算，从而获得正态分布下的变分表示。超图和变分推理的结合使我们的模型能够捕获文本和视觉内容之间复杂的 n 元交互。实验结果表明，该方法在文本视频检索任务中取得了较好的性能。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Text-Video+Retrieval+via+Multi-Modal+Hypergraph+Networks)|0|
|[MultiFS: Automated Multi-Scenario Feature Selection in Deep Recommender Systems](https://doi.org/10.1145/3616855.3635859)|Dugang Liu, Chaohua Yang, Xing Tang, Yejing Wang, Fuyuan Lyu, Weihong Luo, Xiuqiang He, Zhong Ming, Xiangyu Zhao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MultiFS:+Automated+Multi-Scenario+Feature+Selection+in+Deep+Recommender+Systems)|0|
|[ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models](https://doi.org/10.1145/3616855.3635845)|Qijiong Liu, Nuo Chen, Tetsuya Sakai, XiaoMing Wu||Personalized content-based recommender systems have become indispensable tools for users to navigate through the vast amount of content available on platforms like daily news websites and book recommendation services. However, existing recommenders face significant challenges in understanding the content of items. Large language models (LLMs), which possess deep semantic comprehension and extensive knowledge from pretraining, have proven to be effective in various natural language processing tasks. In this study, we explore the potential of leveraging both open- and closed-source LLMs to enhance content-based recommendation. With open-source LLMs, we utilize their deep layers as content encoders, enriching the representation of content at the embedding level. For closed-source LLMs, we employ prompting techniques to enrich the training data at the token level. Through comprehensive experiments, we demonstrate the high effectiveness of both types of LLMs and show the synergistic relationship between them. Notably, we observed a significant relative improvement of up to 19.32% compared to existing state-of-the-art recommendation models. These findings highlight the immense potential of both open- and closed-source of LLMs in enhancing content-based recommendation systems. We will make our code and LLM-generated data available for other researchers to reproduce our results.|基于内容的个性化推荐系统已经成为用户浏览日常新闻网站和图书推荐服务等平台上大量内容不可或缺的工具。但是，现有的推荐程序在理解项目内容方面面临重大挑战。大语言模型(LLM)具有深刻的语义理解能力和广泛的预训知识，已被证明能够有效地处理各种自然语言处理任务。在这项研究中，我们探索了利用开源和闭源 LLM 来增强基于内容的推荐的潜力。使用开源 LLM，我们利用它们的深层作为内容编码器，丰富了内容在嵌入级别的表示。对于闭源 LLM，我们使用提示技术在令牌级别上丰富训练数据。通过综合实验，我们证明了这两类 LLM 的高效性，并显示了它们之间的协同关系。值得注意的是，与现有最先进的推荐模型相比，我们观察到了高达19.32% 的显著相对改善。这些发现突出了开放和封闭的 LLM 来源在加强基于内容的推荐系统方面的巨大潜力。我们将使我们的代码和 LLM 生成的数据可用于其他研究人员重现我们的结果。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ONCE:+Boosting+Content-based+Recommendation+with+Both+Open-+and+Closed-source+Large+Language+Models)|0|
|[Knowledge Graph Context-Enhanced Diversified Recommendation](https://doi.org/10.1145/3616855.3635803)|Xiaolong Liu, Liangwei Yang, Zhiwei Liu, Mingdai Yang, Chen Wang, Hao Peng, Philip S. Yu||The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified Embedding Learning (DEL) module, meticulously designed to formulate user representations that possess an innate awareness of diversity. In tandem with this, we introduce a novel technique named Conditional Alignment and Uniformity (CAU). It adeptly encodes KG item embeddings while preserving contextual integrity. Collectively, our contributions signify a substantial stride towards augmenting the panorama of recommendation diversity within the realm of KG-informed RecSys paradigms.|推荐系统(RecSys)领域已经被广泛研究，通过利用用户的历史交互来提高准确性。尽管如此，这种对准确性的持续追求常常导致多样性的减少，最终导致公认的“回声室”现象。多样化 RecSys 已经成为一种对策，它将多样性与准确性放在同等重要的位置，并引起了学术界和业内人士的关注。本研究探讨复杂的知识图表(KG)背景下多元化的 RecSys 领域。这些幼儿园充当有关实体和项目的相互关联信息的储存库，通过纳入有见地的背景信息，为扩大建议的多样性提供了一个有利的途径。我们的贡献包括引入一个创新的度量标准，实体覆盖率和关系覆盖率，它有效地量化了 KG 领域内的多样性。此外，我们介绍了多样化嵌入学习(DEL)模块，精心设计的用户表示，具有天生的多样性意识。与此同时，我们介绍了一种新的技术，称为条件对齐和一致性(CAU)。该算法在保持上下文完整性的前提下，对 KG 项嵌入进行编码。总的来说，我们的贡献意味着在 KG 知情的 RecSys 范式领域内，在增强推荐多样性的全景方面取得了实质性的进展。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Knowledge+Graph+Context-Enhanced+Diversified+Recommendation)|0|
|[SSLRec: A Self-Supervised Learning Framework for Recommendation](https://doi.org/10.1145/3616855.3635814)|Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, Chao Huang||Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec framework features a modular architecture that allows users to easily evaluate state-of-the-art models and a complete set of data augmentation and self-supervised toolkits to help create SSL recommendation models with specific needs. Furthermore, SSLRec simplifies the process of training and evaluating different recommendation models with consistent and fair settings. Our SSLRec platform covers a comprehensive set of state-of-the-art SSL-enhanced recommendation models across different scenarios, enabling researchers to evaluate these cutting-edge models and drive further innovation in the field. Our implemented SSLRec framework is available at the source code repository https://github.com/HKUDS/SSLRec.|自监督学习(SSL)作为一种解决推荐系统中数据稀疏和噪声问题的方法，近年来引起了人们的极大兴趣。尽管有越来越多的 SSL 算法被设计用来在各种推荐场景中提供最先进的性能(例如，图形协同过滤、顺序推荐、社交推荐、 KG 增强推荐) ，但是仍然缺乏统一的框架来整合不同领域的推荐算法。这样一个框架可以作为自监督推荐算法的基石，统一现有方法的验证，并推动新方法的设计。为了弥补这一差距，我们引入了 SSLRec，这是一个新颖的基准平台，它为评估各种 SSL 增强的推荐程序提供了一个标准化、灵活和全面的框架。SSLRec 框架采用模块化架构，允许用户方便地评估最先进的模型，并提供一套完整的数据增强和自我监督工具包，以帮助创建具有特定需求的 SSL 推荐模型。此外，SSLRec 通过一致和公平的设置简化了不同推荐模型的培训和评估过程。我们的 SSLRec 平台涵盖了不同场景下一整套最先进的 SSL 增强推荐模型，使研究人员能够评估这些尖端模型，并推动该领域的进一步创新。我们已实施的 SSlrec 架构可在原始码储存库 https://github.com/hkuds/SSLRec 找到。|[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SSLRec:+A+Self-Supervised+Learning+Framework+for+Recommendation)|0|
|[Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation](https://doi.org/10.1145/3616855.3635780)|Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, Hui Xiong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dynamic+Sparse+Learning:+A+Novel+Paradigm+for+Efficient+Recommendation)|0|
|[Towards Better Chinese Spelling Check for Search Engines: A New Dataset and Strong Baseline](https://doi.org/10.1145/3616855.3635847)|Yue Wang, Zilong Zheng, Zecheng Tang, Juntao Li, Zhihui Liu, Kunlong Chen, Jinxiong Chang, Qishen Zhang, Zhongyi Liu, Min Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Better+Chinese+Spelling+Check+for+Search+Engines:+A+New+Dataset+and+Strong+Baseline)|0|
|[Neural Kalman Filtering for Robust Temporal Recommendation](https://doi.org/10.1145/3616855.3635837)|Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Neural+Kalman+Filtering+for+Robust+Temporal+Recommendation)|0|
|[Unified Pretraining for Recommendation via Task Hypergraphs](https://doi.org/10.1145/3616855.3635811)|Mingdai Yang, Zhiwei Liu, Liangwei Yang, Xiaolong Liu, Chen Wang, Hao Peng, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Pretraining+for+Recommendation+via+Task+Hypergraphs)|0|
|[COTER: Conditional Optimal Transport meets Table Retrieval](https://doi.org/10.1145/3616855.3635796)|Xun Yao, Zhixin Zhang, Xinrong Hu, Jie (Jack) Yang, Yi Guo, Daniel (Dianliang) Zhu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=COTER:+Conditional+Optimal+Transport+meets+Table+Retrieval)|0|
|[IncMSR: An Incremental Learning Approach for Multi-Scenario Recommendation](https://doi.org/10.1145/3616855.3635828)|Kexin Zhang, Yichao Wang, Xiu Li, Ruiming Tang, Rui Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IncMSR:+An+Incremental+Learning+Approach+for+Multi-Scenario+Recommendation)|0|
|[Defense Against Model Extraction Attacks on Recommender Systems](https://doi.org/10.1145/3616855.3635751)|Sixiao Zhang, Hongzhi Yin, Hongxu Chen, Cheng Long||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Defense+Against+Model+Extraction+Attacks+on+Recommender+Systems)|0|
|[GEMRec: Towards Generative Model Recommendation](https://doi.org/10.1145/3616855.3635700)|Yuanhe Guo, Haoming Liu, Hongyi Wen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GEMRec:+Towards+Generative+Model+Recommendation)|0|
|[Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters](https://doi.org/10.1145/3616855.3635690)|Yukang Xie, Chengyu Wang, Junbing Yan, Jiyong Zhou, Feiqi Deng, Jun Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Making+Small+Language+Models+Better+Multi-task+Learners+with+Mixture-of-Task-Adapters)|0|
|[Grounded and Transparent Response Generation for Conversational Information-Seeking Systems](https://doi.org/10.1145/3616855.3635727)|Weronika Lajewska||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Grounded+and+Transparent+Response+Generation+for+Conversational+Information-Seeking+Systems)|0|
|[Augmenting Keyword-based Search in Mobile Applications Using LLMs](https://doi.org/10.1145/3616855.3635745)|Harikrishnan C, Giridhar Sreenivasa Murthy, Kumar Rangarajan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Augmenting+Keyword-based+Search+in+Mobile+Applications+Using+LLMs)|0|
|[Recent Advances in Refinement Recommendations](https://doi.org/10.1145/3616855.3635747)|Akshay Jagatap, Sachin Farfade||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Recent+Advances+in+Refinement+Recommendations)|0|
|[Scaling Up LLM Reviews for Google Ads Content Moderation](https://doi.org/10.1145/3616855.3635736)|Wei Qiao, Tushar Dogra, Otilia Stretcu, YuHan Lyu, Tiantian Fang, Dongjin Kwon, ChunTa Lu, Enming Luo, Yuan Wang, ChihChun Chia, Ariel Fuxman, Fangzhou Wang, Ranjay Krishna, Mehmet Tek||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Up+LLM+Reviews+for+Google+Ads+Content+Moderation)|0|
|[Customer Understanding for Recommender Systems](https://doi.org/10.1145/3616855.3635742)|Md. Mostafizur Rahman, Yu Hirate||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Customer+Understanding+for+Recommender+Systems)|0|
|["Maya"- A Conversational Shopping Assistant for Fashion at Myntra](https://doi.org/10.1145/3616855.3635740)|Akhil Raj, Hrishikesh Ganu, Saikat Kumar Das, R. Sandeep, Satyajeet Singh, Sreekanth Vempati||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q="Maya"-+A+Conversational+Shopping+Assistant+for+Fashion+at+Myntra)|0|
|[Fresh Content Recommendation at Scale: A Multi-funnel Solution and the Potential of LLMs](https://doi.org/10.1145/3616855.3635749)|Jianling Wang, Haokai Lu, Minmin Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fresh+Content+Recommendation+at+Scale:+A+Multi-funnel+Solution+and+the+Potential+of+LLMs)|0|
|[Lessons Learnt from Building Friend Recommendation Systems](https://doi.org/10.1145/3616855.3635750)|Jun Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Lessons+Learnt+from+Building+Friend+Recommendation+Systems)|0|
|[Leveraging Multimodal Features and Item-level User Feedback for Bundle Construction](https://doi.org/10.1145/3616855.3635854)|Yunshan Ma, Xiaohao Liu, Yinwei Wei, Zhulin Tao, Xiang Wang, TatSeng Chua||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Leveraging+Multimodal+Features+and+Item-level+User+Feedback+for+Bundle+Construction)|0|
|[Cost-Effective Active Learning for Bid Exploration in Online Advertising](https://doi.org/10.1145/3616855.3635839)|Zixiao Wang, Zhenzhe Zheng, Yanrong Kang, Jiani Huang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Cost-Effective+Active+Learning+for+Bid+Exploration+in+Online+Advertising)|0|
|[LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection](https://doi.org/10.1145/3616855.3635843)|Zijian Cai, Zhaoxuan Tan, Zhenyu Lei, Zifeng Zhu, Hongrui Wang, Qinghua Zheng, Minnan Luo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=LMBot:+Distilling+Graph+Knowledge+into+Language+Model+for+Graph-less+Deployment+in+Twitter+Bot+Detection)|0|
|[Long-Term Value of Exploration: Measurements, Findings and Algorithms](https://doi.org/10.1145/3616855.3635833)|Yi Su, Xiangyu Wang, Elaine Ya Le, Liang Liu, Yuening Li, Haokai Lu, Benjamin Lipshitz, Sriraj Badam, Lukasz Heldt, Shuchao Bi, Ed H. Chi, Cristos Goodrow, SuLin Wu, Lexi Baugher, Minmin Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Long-Term+Value+of+Exploration:+Measurements,+Findings+and+Algorithms)|0|
|[Unified Visual Preference Learning for User Intent Understanding](https://doi.org/10.1145/3616855.3635858)|Yihua Wen, Si Chen, Yu Tian, Wanxian Guan, Pengjie Wang, Hongbo Deng, Jian Xu, Bo Zheng, Zihao Li, Lixin Zou, Chenliang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unified+Visual+Preference+Learning+for+User+Intent+Understanding)|0|
|[Framework for Bias Detection in Machine Learning Models: A Fairness Approach](https://doi.org/10.1145/3616855.3635731)|Alveiro Alonso Rosado Gomez, Maritza Liliana CalderónBenavides||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Framework+for+Bias+Detection+in+Machine+Learning+Models:+A+Fairness+Approach)|0|
|[IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification](https://doi.org/10.1145/3616855.3635849)|Abdullah Alsuhaibani, Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=IDoFew:+Intermediate+Training+Using+Dual-Clustering+in+Language+Models+for+Few+Labels+Text+Classification)|0|
|[MAD: Multi-Scale Anomaly Detection in Link Streams](https://doi.org/10.1145/3616855.3635834)|Esteban Bautista, Laurent Brisson, Cécile Bothorel, Grégory Smits||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MAD:+Multi-Scale+Anomaly+Detection+in+Link+Streams)|0|
|[Customized and Robust Deep Neural Network Watermarking](https://doi.org/10.1145/3616855.3635812)|TzuYun Chien, ChihYa Shen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Customized+and+Robust+Deep+Neural+Network+Watermarking)|0|
|[Incomplete Graph Learning via Attribute-Structure Decoupled Variational Auto-Encoder](https://doi.org/10.1145/3616855.3635769)|Xinke Jiang, Zidi Qin, Jiarong Xu, Xiang Ao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Incomplete+Graph+Learning+via+Attribute-Structure+Decoupled+Variational+Auto-Encoder)|0|
|[Source Free Graph Unsupervised Domain Adaptation](https://doi.org/10.1145/3616855.3635802)|Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Source+Free+Graph+Unsupervised+Domain+Adaptation)|0|
|[PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization](https://doi.org/10.1145/3616855.3635783)|Ziqi Yuan, Haoyi Zhou, Tianyu Chen, Jianxin Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=PhoGAD:+Graph-based+Anomaly+Behavior+Detection+with+Persistent+Homology+Optimization)|0|
|[The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation](https://doi.org/10.1145/3616855.3635768)|Yuchang Zhu, Jintang Li, Liang Chen, Zibin Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=The+Devil+is+in+the+Data:+Learning+Fair+Graph+Neural+Networks+via+Partial+Knowledge+Distillation)|0|
|[Dance with Labels: Dual-Heterogeneous Label Graph Interaction for Multi-intent Spoken Language Understanding](https://doi.org/10.1145/3616855.3635782)|Zhihong Zhu, Xuxin Cheng, Hongxiang Li, Yaowei Li, Yuexian Zou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Dance+with+Labels:+Dual-Heterogeneous+Label+Graph+Interaction+for+Multi-intent+Spoken+Language+Understanding)|0|
|[Wildfire: A Twitter Social Sensing Platform for Layperson](https://doi.org/10.1145/3616855.3635704)|Zeyu Zhang, Zhengyuan Zhu, Haiqi Zhang, Foram Patel, Josue Caraballo, Patrick Hennecke, Chengkai Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Wildfire:+A+Twitter+Social+Sensing+Platform+for+Layperson)|0|
|[Bridging Text Data and Graph Data: Towards Semantics and Structure-aware Knowledge Discovery](https://doi.org/10.1145/3616855.3636450)|Bowen Jin, Yu Zhang, Sha Li, Jiawei Han||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Bridging+Text+Data+and+Graph+Data:+Towards+Semantics+and+Structure-aware+Knowledge+Discovery)|0|
|[Practical Bandits: An Industry Perspective](https://doi.org/10.1145/3616855.3636449)|Bram van den Akker, Olivier Jeunen, Ying Li, Ben London, Zahra Nazari, Devesh Parekh||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Practical+Bandits:+An+Industry+Perspective)|0|
|[Automated Tailoring of Large Language Models for Industry-Specific Downstream Tasks](https://doi.org/10.1145/3616855.3635743)|Shreya Saxena, Siva Prasad, Muneeswaran I, Advaith Shankar, Varun V, Saisubramaniam Gopalakrishnan, Vishal Vaddina||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Tailoring+of+Large+Language+Models+for+Industry-Specific+Downstream+Tasks)|0|
|[Unlocking Human Curiosity](https://doi.org/10.1145/3616855.3637631)|Elizabeth Reid||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unlocking+Human+Curiosity)|0|
|[Unveiling AI-Driven Collective Action for a Worker-Centric Future](https://doi.org/10.1145/3616855.3637633)|Saiph Savage||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Unveiling+AI-Driven+Collective+Action+for+a+Worker-Centric+Future)|0|
|[What I Learned from Spending a Dozen Years in the Dark Web](https://doi.org/10.1145/3616855.3637632)|Nicolas Christin||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=What+I+Learned+from+Spending+a+Dozen+Years+in+the+Dark+Web)|0|
|[Professional Network Matters: Connections Empower Person-Job Fit](https://doi.org/10.1145/3616855.3635852)|Hao Chen, Lun Du, Yuxuan Lu, Qiang Fu, Xu Chen, Shi Han, Yanbin Kang, Guangming Lu, Zi Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Professional+Network+Matters:+Connections+Empower+Person-Job+Fit)|0|
|[Empathetic Response Generation with Relation-aware Commonsense Knowledge](https://doi.org/10.1145/3616855.3635836)|Changyu Chen, Yanran Li, Chen Wei, Jianwei Cui, Bin Wang, Rui Yan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Empathetic+Response+Generation+with+Relation-aware+Commonsense+Knowledge)|0|
|[Exploiting Duality in Open Information Extraction with Predicate Prompt](https://doi.org/10.1145/3616855.3635799)|Zhen Chen, Jingping Liu, Deqing Yang, Yanghua Xiao, Huimin Xu, Zongyu Wang, Rui Xie, Yunsen Xian||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Exploiting+Duality+in+Open+Information+Extraction+with+Predicate+Prompt)|0|
|[Overlapping and Robust Edge-Colored Clustering in Hypergraphs](https://doi.org/10.1145/3616855.3635792)|Alex Crane, Brian Lavallee, Blair D. Sullivan, Nate Veldt||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Overlapping+and+Robust+Edge-Colored+Clustering+in+Hypergraphs)|0|
|[TemporalMed: Advancing Medical Dialogues with Time-Aware Responses in Large Language Models](https://doi.org/10.1145/3616855.3635860)|Yuyan Chen, Jin Zhao, Zhihao Wen, Zhixu Li, Yanghua Xiao||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TemporalMed:+Advancing+Medical+Dialogues+with+Time-Aware+Responses+in+Large+Language+Models)|0|
|[CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking](https://doi.org/10.1145/3616855.3635795)|Shohreh Deldari, Dimitris Spathis, Mohammad Malekzadeh, Fahim Kawsar, Flora D. Salim, Akhil Mathur||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CroSSL:+Cross-modal+Self-Supervised+Learning+for+Time-series+through+Latent+Masking)|0|
|[Guardian: Guarding against Gradient Leakage with Provable Defense for Federated Learning](https://doi.org/10.1145/3616855.3635758)|Mingyuan Fan, Yang Liu, Cen Chen, Chengyu Wang, Minghui Qiu, Wenmeng Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Guardian:+Guarding+against+Gradient+Leakage+with+Provable+Defense+for+Federated+Learning)|0|
|[TTC-QuAli: A Text-Table-Chart Dataset for Multimodal Quantity Alignment](https://doi.org/10.1145/3616855.3635777)|Haoyu Dong, Haochen Wang, Anda Zhou, Yue Hu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=TTC-QuAli:+A+Text-Table-Chart+Dataset+for+Multimodal+Quantity+Alignment)|0|
|[DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting](https://doi.org/10.1145/3616855.3635788)|Tianyu Fu, Chiyue Wei, Yu Wang, Rex Ying||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=DeSCo:+Towards+Generalizable+and+Scalable+Deep+Subgraph+Counting)|0|
|[CausalMMM: Learning Causal Structure for Marketing Mix Modeling](https://doi.org/10.1145/3616855.3635766)|Chang Gong, Di Yao, Lei Zhang, Sheng Chen, Wenbin Li, Yueyang Su, Jingping Bi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CausalMMM:+Learning+Causal+Structure+for+Marketing+Mix+Modeling)|0|
|[SCAD: Subspace Clustering based Adversarial Detector](https://doi.org/10.1145/3616855.3635835)|Xinrong Hu, Wushuan Chen, Jie Yang, Yi Guo, Xun Yao, Bangchao Wang, Junping Liu, Ce Xu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=SCAD:+Subspace+Clustering+based+Adversarial+Detector)|0|
|[Capturing Temporal Node Evolution via Self-supervised Learning: A New Perspective on Dynamic Graph Learning](https://doi.org/10.1145/3616855.3635765)|Lingwen Liu, Guangqi Wen, Peng Cao, Jinzhu Yang, Weiping Li, Osmar R. Zaïane||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Capturing+Temporal+Node+Evolution+via+Self-supervised+Learning:+A+New+Perspective+on+Dynamic+Graph+Learning)|0|
|[Generative Models for Complex Logical Reasoning over Knowledge Graphs](https://doi.org/10.1145/3616855.3635804)|Yu Liu, Yanan Cao, Shi Wang, Qingyue Wang, Guanqun Bi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Generative+Models+for+Complex+Logical+Reasoning+over+Knowledge+Graphs)|0|
|[A Linguistic Grounding-Infused Contrastive Learning Approach for Health Mention Classification on Social Media](https://doi.org/10.1145/3616855.3635763)|Usman Naseem, Jinman Kim, Matloob Khushi, Adam G. Dunn||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Linguistic+Grounding-Infused+Contrastive+Learning+Approach+for+Health+Mention+Classification+on+Social+Media)|0|
|[GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction](https://doi.org/10.1145/3616855.3635767)|Amit Roy, Juan Shu, Jia Li, Carl Yang, Olivier Elshocht, Jeroen Smeets, Pan Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAD-NR:+Graph+Anomaly+Detection+via+Neighborhood+Reconstruction)|0|
|[Ad-load Balancing via Off-policy Learning in a Content Marketplace](https://doi.org/10.1145/3616855.3635846)|Hitesh Sagtani, Madan Gopal Jhawar, Rishabh Mehrotra, Olivier Jeunen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ad-load+Balancing+via+Off-policy+Learning+in+a+Content+Marketplace)|0|
|[ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees](https://doi.org/10.1145/3616855.3635761)|Sina Sajadmanesh, Daniel GaticaPerez||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=ProGAP:+Progressive+Graph+Neural+Networks+with+Differential+Privacy+Guarantees)|0|
|[Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling](https://doi.org/10.1145/3616855.3635825)|Marija Sakota, Maxime Peyrard, Robert West||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Fly-Swat+or+Cannon?+Cost-Effective+Language+Model+Choice+via+Meta-Modeling)|0|
|[Causality Guided Disentanglement for Cross-Platform Hate Speech Detection](https://doi.org/10.1145/3616855.3635771)|Paras Sheth, Raha Moraffah, Tharindu S. Kumarage, Aman Chadha, Huan Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Causality+Guided+Disentanglement+for+Cross-Platform+Hate+Speech+Detection)|0|
|[Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study](https://doi.org/10.1145/3616855.3635752)|Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Table+Meets+LLM:+Can+Large+Language+Models+Understand+Structured+Table+Data?+A+Benchmark+and+Empirical+Study)|0|
|[Rethinking and Simplifying Bootstrapped Graph Latents](https://doi.org/10.1145/3616855.3635842)|Wangbin Sun, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Rethinking+and+Simplifying+Bootstrapped+Graph+Latents)|0|
|[Temporal Blind Spots in Large Language Models](https://doi.org/10.1145/3616855.3635818)|Jonas Wallat, Adam Jatowt, Avishek Anand||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Blind+Spots+in+Large+Language+Models)|0|
|[Efficient, Direct, and Restricted Black-Box Graph Evasion Attacks to Any-Layer Graph Neural Networks via Influence Function](https://doi.org/10.1145/3616855.3635826)|Binghui Wang, Minhua Lin, Tianxiang Zhou, Pan Zhou, Ang Li, Meng Pang, Hai Helen Li, Yiran Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Efficient,+Direct,+and+Restricted+Black-Box+Graph+Evasion+Attacks+to+Any-Layer+Graph+Neural+Networks+via+Influence+Function)|0|
|[CityCAN: Causal Attention Network for Citywide Spatio-Temporal Forecasting](https://doi.org/10.1145/3616855.3635764)|Chengxin Wang, Yuxuan Liang, Gary Tan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CityCAN:+Causal+Attention+Network+for+Citywide+Spatio-Temporal+Forecasting)|0|
|[Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels](https://doi.org/10.1145/3616855.3635793)|Fali Wang, Tianxiang Zhao, Suhang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Distribution+Consistency+based+Self-Training+for+Graph+Neural+Networks+with+Sparse+Labels)|0|
|[FairIF: Boosting Fairness in Deep Learning via Influence Functions with Validation Set Sensitive Attributes](https://doi.org/10.1145/3616855.3635844)|Haonan Wang, Ziwei Wu, Jingrui He||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=FairIF:+Boosting+Fairness+in+Deep+Learning+via+Influence+Functions+with+Validation+Set+Sensitive+Attributes)|0|
|[NeuralReconciler for Hierarchical Time Series Forecasting](https://doi.org/10.1145/3616855.3635806)|Shiyu Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=NeuralReconciler+for+Hierarchical+Time+Series+Forecasting)|0|
|[Follow the LIBRA: Guiding Fair Policy for Unified Impression Allocation via Adversarial Rewarding](https://doi.org/10.1145/3616855.3635756)|Xiaoyu Wang, Yonghui Guo, Bin Tan, Tao Yang, Dongbo Huang, Lan Xu, Hao Zhou, Xiangyang Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Follow+the+LIBRA:+Guiding+Fair+Policy+for+Unified+Impression+Allocation+via+Adversarial+Rewarding)|0|
|[Continuous-time Autoencoders for Regular and Irregular Time Series Imputation](https://doi.org/10.1145/3616855.3635831)|Hyowon Wi, Yehjin Shin, Noseong Park||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Continuous-time+Autoencoders+for+Regular+and+Irregular+Time+Series+Imputation)|0|
|[Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding](https://doi.org/10.1145/3616855.3635753)|Hongshen Xu, Lu Chen, Zihan Zhao, Da Ma, Ruisheng Cao, Zichen Zhu, Kai Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Hierarchical+Multimodal+Pre-training+for+Visually+Rich+Webpage+Understanding)|0|
|[Towards Alignment-Uniformity Aware Representation in Graph Contrastive Learning](https://doi.org/10.1145/3616855.3635789)|Rong Yan, Peng Bao, Xiao Zhang, Zhongyi Liu, Hui Liu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Alignment-Uniformity+Aware+Representation+in+Graph+Contrastive+Learning)|0|
|[GAP: A Grammar and Position-Aware Framework for Efficient Recognition of Multi-Line Mathematical Formulas](https://doi.org/10.1145/3616855.3635776)|Zhe Yang, Qi Liu, Kai Zhang, Shiwei Tong, Enhong Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=GAP:+A+Grammar+and+Position-Aware+Framework+for+Efficient+Recognition+of+Multi-Line+Mathematical+Formulas)|0|
|[Maximizing Malicious Influence in Node Injection Attack](https://doi.org/10.1145/3616855.3635790)|Xiao Zhang, Peng Bao, Shirui Pan||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Maximizing+Malicious+Influence+in+Node+Injection+Attack)|0|
|[Interpretable Imitation Learning with Dynamic Causal Relations](https://doi.org/10.1145/3616855.3635827)|Tianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong Chen, Yanchi Liu, Wei Cheng, Haifeng Chen||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Interpretable+Imitation+Learning+with+Dynamic+Causal+Relations)|0|
|[RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis](https://doi.org/10.1145/3616855.3635775)|Xusheng Zhao, Hao Peng, Qiong Dai, Xu Bai, Huailiang Peng, Yanbing Liu, Qinglang Guo, Philip S. Yu||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=RDGCN:+Reinforced+Dependency+Graph+Convolutional+Network+for+Aspect-based+Sentiment+Analysis)|0|
|[CreST: A Credible Spatiotemporal Learning Framework for Uncertainty-aware Traffic Forecasting](https://doi.org/10.1145/3616855.3635759)|Zhengyang Zhou, Jiahao Shi, Hongbo Zhang, Qiongyu Chen, Xu Wang, Hongyang Chen, Yang Wang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=CreST:+A+Credible+Spatiotemporal+Learning+Framework+for+Uncertainty-aware+Traffic+Forecasting)|0|
|[Pitfalls in Link Prediction with Graph Neural Networks: Understanding the Impact of Target-link Inclusion & Better Practices](https://doi.org/10.1145/3616855.3635786)|Jing Zhu, Yuhang Zhou, Vassilis N. Ioannidis, Shengyi Qian, Wei Ai, Xiang Song, Danai Koutra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Pitfalls+in+Link+Prediction+with+Graph+Neural+Networks:+Understanding+the+Impact+of+Target-link+Inclusion+&+Better+Practices)|0|
|[MultiSPANS: A Multi-range Spatial-Temporal Transformer Network for Traffic Forecast via Structural Entropy Optimization](https://doi.org/10.1145/3616855.3635820)|Dongcheng Zou, Senzhang Wang, Xuefeng Li, Hao Peng, Yuandong Wang, Chunyang Liu, Kehua Sheng, Bo Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=MultiSPANS:+A+Multi-range+Spatial-Temporal+Transformer+Network+for+Traffic+Forecast+via+Structural+Entropy+Optimization)|0|
|[WordGraph: A Python Package for Reconstructing Interactive Causal Graphical Models from Text Data](https://doi.org/10.1145/3616855.3635698)|Amine Ferdjaoui, Séverine Affeldt, Mohamed Nadif||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=WordGraph:+A+Python+Package+for+Reconstructing+Interactive+Causal+Graphical+Models+from+Text+Data)|0|
|[EvidenceQuest: An Interactive Evidence Discovery System for Explainable Artificial Intelligence](https://doi.org/10.1145/3616855.3635697)|Ambreen Hanif, Amin Beheshti, Xuyun Zhang, Steven Wood, Boualem Benatallah, EuJin Foo||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=EvidenceQuest:+An+Interactive+Evidence+Discovery+System+for+Explainable+Artificial+Intelligence)|0|
|[Ginkgo-P: General Illustrations of Knowledge Graphs for Openness as a Platform](https://doi.org/10.1145/3616855.3635701)|Blaine Hill, Lihui Liu, Hanghang Tong||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Ginkgo-P:+General+Illustrations+of+Knowledge+Graphs+for+Openness+as+a+Platform)|0|
|[Real-time E-bike Route Planning with Battery Range Prediction](https://doi.org/10.1145/3616855.3635696)|Zhao Li, Guoqi Ren, Yongchun Gu, Siwei Zhou, Xuanwu Liu, Jiaming Huang, Ming Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Real-time+E-bike+Route+Planning+with+Battery+Range+Prediction)|0|
|[An Interpretable Brain Graph Contrastive Learning Framework for Brain Disorder Analysis](https://doi.org/10.1145/3616855.3635695)|Xuexiong Luo, Guangwei Dong, Jia Wu, Amin Beheshti, Jian Yang, Shan Xue||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=An+Interpretable+Brain+Graph+Contrastive+Learning+Framework+for+Brain+Disorder+Analysis)|0|
|[Future Timelines: Extraction and Visualization of Future-related Content From News Articles](https://doi.org/10.1145/3616855.3635693)|Juwal Regev, Adam Jatowt, Michael Färber||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Future+Timelines:+Extraction+and+Visualization+of+Future-related+Content+From+News+Articles)|0|
|[Temporal Graph Analysis with TGX](https://doi.org/10.1145/3616855.3635694)|Razieh Shirzadkhani, Shenyang Huang, Elahe Kooshafar, Reihaneh Rabbany, Farimah Poursafaei||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Temporal+Graph+Analysis+with+TGX)|0|
|[A Scalable Open-Source System for Segmenting Urban Areas with Road Networks](https://doi.org/10.1145/3616855.3635703)|Ming Zhang, Yanyan Li, Jianguo Duan, Jizhou Huang, Jingbo Zhou||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=A+Scalable+Open-Source+System+for+Segmenting+Urban+Areas+with+Road+Networks)|0|
|[Some Useful Things to Know When Combining IR and NLP: The Easy, the Hard and the Ugly](https://doi.org/10.1145/3616855.3636452)|Omar Alonso, Kenneth Church||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Some+Useful+Things+to+Know+When+Combining+IR+and+NLP:+The+Easy,+the+Hard+and+the+Ugly)|0|
|[Introduction to Responsible AI](https://doi.org/10.1145/3616855.3636455)|Ricardo BaezaYates||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Introduction+to+Responsible+AI)|0|
|[Towards Trustworthy Large Language Models](https://doi.org/10.1145/3616855.3636454)|Sanmi Koyejo, Bo Li||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Towards+Trustworthy+Large+Language+Models)|0|
|[Strategic ML: How to Learn With Data That 'Behaves'](https://doi.org/10.1145/3616855.3636453)|Nir Rosenfeld||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Strategic+ML:+How+to+Learn+With+Data+That+'Behaves')|0|
|[Learning Opinion Dynamics from Data](https://doi.org/10.1145/3616855.3635729)|Jacopo Lenti||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Learning+Opinion+Dynamics+from+Data)|0|
|[Gaussian Graphical Model-Based Clustering of Time Series Data](https://doi.org/10.1145/3616855.3635728)|Kohei Obata||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Gaussian+Graphical+Model-Based+Clustering+of+Time+Series+Data)|0|
|[Using Causal Inference to Solve Uncertainty Issues in Dataset Shift](https://doi.org/10.1145/3616855.3635732)|Song Shuang, Muhammad Syafiq Bin Mohd Pozi||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Using+Causal+Inference+to+Solve+Uncertainty+Issues+in+Dataset+Shift)|0|
|[Multi-Granular Text Classification with Minimal Supervision](https://doi.org/10.1145/3616855.3635735)|Yunyi Zhang||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Multi-Granular+Text+Classification+with+Minimal+Supervision)|0|
|[Scaling Use-case Based Shopping using LLMs](https://doi.org/10.1145/3616855.3635748)|Sachin Farfade, Sachin Vernekar, Vineet Chaoji, Rajdeep Mukherjee||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Scaling+Use-case+Based+Shopping+using+LLMs)|0|
|[HealAI: A Healthcare LLM for Effective Medical Documentation](https://doi.org/10.1145/3616855.3635739)|Sagar Goyal, Eti Rastogi, Sree Prasanna Rajagopal, Dong Yuan, Fen Zhao, Jai Chintagunta, Gautam Naik, Jeff Ward||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=HealAI:+A+Healthcare+LLM+for+Effective+Medical+Documentation)|0|
|[Mitigating Factual Inconsistency and Hallucination in Large Language Models](https://doi.org/10.1145/3616855.3635744)|Muneeswaran I, Advaith Shankar, Varun V, Saisubramaniam Gopalakrishnan, Vishal Vaddina||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Mitigating+Factual+Inconsistency+and+Hallucination+in+Large+Language+Models)|0|
|[Foundation Models for Aerial Robotics](https://doi.org/10.1145/3616855.3638206)|Ashish Kapoor||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Foundation+Models+for+Aerial+Robotics)|0|
|[Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers](https://doi.org/10.1145/3616855.3635737)|Sohini Roychowdhury||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Journey+of+Hallucination-minimized+Generative+AI+Solutions+for+Financial+Decision+Makers)|0|
|[Accelerating Pharmacovigilance using Large Language Models](https://doi.org/10.1145/3616855.3635741)|Mukkamala Venkata Sai Prakash, Ganesh Parab, Meghana Veeramalla, Siddartha Reddy, Varun V, Saisubramaniam Gopalakrishnan, Vishal Pagidipally, Vishal Vaddina||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Accelerating+Pharmacovigilance+using+Large+Language+Models)|0|
|[Automated Topic Generation for the Mexican Platform for Access to Government Public Information During the Period 2003-2020](https://doi.org/10.1145/3616855.3636502)|Hermelando CruzPérez, Alejandro MolinaVillegas||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automated+Topic+Generation+for+the+Mexican+Platform+for+Access+to+Government+Public+Information+During+the+Period+2003-2020)|0|
|[Profiling Urban Mobility Patterns with High Spatial and Temporal Resolution: A Deep Dive into Cellphone Geo-position Data](https://doi.org/10.1145/3616855.3636504)|José Ignacio Huertas, Luisa Fernanda Chaparro Sierra||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Profiling+Urban+Mobility+Patterns+with+High+Spatial+and+Temporal+Resolution:+A+Deep+Dive+into+Cellphone+Geo-position+Data)|0|
|[Integrating Knowledge Graph Data with Large Language Models for Explainable Inference](https://doi.org/10.1145/3616855.3636507)|Carlos Efrain Quintero Narvaez, Raúl Monroy||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Integrating+Knowledge+Graph+Data+with+Large+Language+Models+for+Explainable+Inference)|0|
|[Genomic-World Fungi Data: Synteny Part](https://doi.org/10.1145/3616855.3636505)|Pedro EscobarTurriza, Luis MuñozMiranda, Alejandro PereiraSantana||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Genomic-World+Fungi+Data:+Synteny+Part)|0|
|[Preserving Heritage: Developing a Translation Tool for Indigenous Dialects](https://doi.org/10.1145/3616855.3637828)|Melissa Robles, Cristian A. Martínez, Juan C. Prieto, Sara Palacios, Rubén Manrique||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Preserving+Heritage:+Developing+a+Translation+Tool+for+Indigenous+Dialects)|0|
|[Automatic Extraction of Patterns in Digital News Articles of Femicides occurred in Mexico by Text Mining Techniques](https://doi.org/10.1145/3616855.3636503)|Jonathan ZárateCartas, Alejandro MolinaVillegas||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Automatic+Extraction+of+Patterns+in+Digital+News+Articles+of+Femicides+occurred+in+Mexico+by+Text+Mining+Techniques)|0|
|[Integrity 2024: Integrity in Social Networks and Media](https://doi.org/10.1145/3616855.3635721)|Lluís Garcia Pueyo, Symeon Papadopoulos, Prathyusha Senthil Kumar, Aristides Gionis, Panayiotis Tsaparas, Vasilis Verroios, Giuseppe Manco, Anton Andryeyev, Stefano Cresci, Timos Sellis, Anthony McCosker||||[code](https://paperswithcode.com/search?q_meta=&q_type=&q=Integrity+2024:+Integrity+in+Social+Networks+and+Media)|0|
